%!TEX root=main

\chapter{Descriptive estimators}

\subsection{The arithmetic mean: a first estimator of central tendency}

One of the crucial misunderstandings in pre-statistical experimentation was
that the amount of information you had---or, similarly, the precision of your
measurement---increased linearly with the number of data points you took. So
if I took 100 data points and you took 25, I would have 4 times more
information than you.

A simple and very important instance of this is estimating the expected value
of a distribution. People were doing this before they knew they were doing it,
and you probably have too: the arithmetic mean of a bunch of numbers is an
estimator of the expected value of the distribution they were drawn from.

I'm suggesting that a good estimator for $\mathbb{E}[X]$ is $(1/N)\sum_i X_i$,
where the $X_i$ are iid random variables distributed just like $X$. I think
the most consistent notation would be to name this estimator
$\hat{\mathbb{E}}_X$, to emphasize that it is a random variable that is
estimating the expected value of $X$. It's typical and more compact however,
to write it as $\overline{X}$, in analogy with the arithmetic mean of numbers
$\overline{x}$. Regardless of what notation you use, the important this to
remember is that recall that $\mathbb{E}$ is a function that takes random
variables like $X$ and returns numbers, so $\expect{X}$ is a fixed number,
while $\hat{\mathbb{E}}_X$ (a.k.a. $\overline{X}$) is an estimator, that is, a
random variable.

\subsubsection{Consistency of arithmetic mean as estimator of the expected value}

Like we talked about in the last chapter, a basic requirement for an estimator
should be that it is \emph{consistent}, that it approaches the true value as
the amount of data increases. It feels very intuitive that the arithmetic mean
should approach the expected value, but but is not a trivial result. Jacob
Bernoulli took 20 years to formulate a suitably rigorous proof of a special
case of this result. He thought so highly of it that he called this result the
``Golden Theorem''. We call the more general result the \emph{law of large
numbers}:
\begin{equation}
\hat{\mathbb{E}}_X \to \expect{X}
\end{equation}
where the arrow means ``converges probabilistically'', like we saw in the last
chapter.

The proof is not very difficult, but it's not very illuminating for us either,
so I put it in an Appendix.

\subsubsection{The arithmetic mean is an unbiased estimator of the expected value}

Like we said last chapter, knowing that an estimator is \emph{consistent} is nice,
but we'd like to know it's not systematically wrong in one direction or the other.
It's pretty easy to see that the arithmetic mean is a consistent estimator for the
expected value:
\begin{equation}
\expect{\overline{X}} = \expect{\frac{1}{N} \sum_i X_i} = \frac{1}{N} \sum_i \expect{X_i} = \expect{X}.
\end{equation}
I showed that the expected value $\expect{\overline{X}}$ of a random variable
$\overline{X}$---which happens to be an estimator for the expected value
$\expect{X}$ of the random variable $X$---is exactly equal to the expected
value $\expect{X}$, the value that was being estimated. That means that $\overline{X}$
is an unbiased estimator for $\expect{X}$.

Note that the arithmetic mean is also automatically a consistent estimator of
the expected value. Even if you draw just one data point, the expected value
of the arithmetic mean is the true expected value of the target distribution.

\subsubsection{Efficiency of the arithmetic mean}

So I showed you, with some crazy symbology, something you felt was true for a
long time. So what? Well, this formalism set us up to answer harder questions.
Here's one: what's the \emph{variance} of $\overline{X}$? This will help
answer the question: what's a typical deviation of the arithmetic mean from
the true expected value of a distribution? Now that we're thinking of the
arithmetic mean as a random variable, we have the power!
\begin{align}\label{eq:sem}
\var{\overline{X}} &= \var{\frac{1}{N} \sum_i X_i} \nonumber\\
  &= \frac{1}{N^2} \sum_i \var{X_i} \quad\text{(variances of independent rv's add)} \nonumber\\
  &= \frac{\var{X}}{N}.
\end{align}
Variance is a little confusing, being the square of the normal deviation, so
normally we take it's square root and call it the standard deviation. The
standard deviation of the arithmetic mean---as an estimator for $X$---is the
standard deviation of $X$ divided by $\sqrt{n}$.\footnote{Take careful note
that the words ``standard deviation'' here mean ``square root of the variance'',
so, in this case, standard deviation is a function of a random variable, \emph{not}
a function of numbers.}

This is a fundamental and critical result in statistics: if I take 100 data
points and you took 25, then my arithmetic mean's standard deviation has a
term like $\sqrt{100} = 10$ in the denominator, and yours has $\sqrt{25} = 5$.
In other words, even thought I took four times the data, my estimate is only
twice as precise as yours. We get diminishing returns from every data point
we take.

You may see a resemblance between the results above and the familiar equation
$s/\sqrt{n}$ for the ``standard error'' or ``standard error of the
mean''.\footnote{I haven't defined $s$ because we're going to get to that in
the next section.} The reason the ``standard error'' is different from the
``standard deviation'' by a factor of $\sqrt{n}$ is exactly because of what we
just went through: $s$ is some estimate of the standard deviation of the true
population---that is, of the variation inherent in the thing being
sampled---and $s/\sqrt{n}$ is the variation inherent in the arithmetic mean.

I titled this section with the word \emph{efficiency}, because asking about
the variance of an estimator is another good way, along with consistency and
unbiasedness, to see if the estimator is ``good''. One estimator is more
\emph{efficient} than another if it has smaller variance for the same number
of data points. In other words, you might have two estimators that are both
asymptotically correct and unbiased, but one of them systematically delivers
estimates nearer to the true value. More technically, an estimator is called
\emph{minimum variance} or just plain \emph{efficient} if it is the estimator
(or one of the estimators) that has the smallest possible
variance.\footnote{This theoretical limit is the Cram\'{e}r-Rao bound: the
variance of an estimator for a parameter is at least one over the Fisher
information of that parameter.}

\subsection{Variance: a first estimator of spread}

You may have been disappointed by $\eqref{eq:sem}$: when you asked me about
the variance $\var{\overline{X}}$ in my estimate of the expected value, I told you that it had to
do with the true variance of the population $\var{X}$. That's normally not
information we have access to: we usually also need to estimate the population
variance.

What's a reasonable estimator the true variance $\var{X}$? Well, just as the
true variance is the square of the values in the distribution from their true
expected value, we might guess that an estimate for the variance is the
arithmetic mean of the squares of the deviations of the data points from their
arithmetic mean:
\begin{equation}\label{eq:estimator-vx1}
\hat{\mathbb{V}}_X \defeq \frac{1}{N} \sum_i \left( X_i - \overline{X} \right)^2.
\end{equation}

Before diving into that equation, note that both $X_i$ and $\overline{X}$ are
random variables, and note that they are not independent: the value of
$\overline{X}$ certainly depends on each of the $X_i$. So first let's imagine
a simpler case, where we're in a universe where happen to know the expected
value of the distribution we're trying to determine the variance of. To make
the equations simpler to read, I'll use the standard notation $\mu \defeq
\expect{X}$.\footnote{Do remember, however, that $\overline{X}$ is a random
variable---a function of the data---while $\expect{X}$ is a function of the
distribution, and therefore just a single matter-of-fact number.} In this
case, having known expected value (``kEV''), our estimator will be a little
simpler:
\begin{equation}
\hat{\mathbb{V}}_{X,\mathrm{kEV}} = \frac{1}{N} \sum_i ( X_i - \mu )^2
\end{equation}

\subsubsection{Bias in estimators of variance}

The known-expected-value estimator is unbiased:
\begin{align*}
\expect{\hat{\mathbb{V}}_{X,\mathrm{kEV}}}
  &= \expect{\frac{1}{N} \sum_i ( X_i - \mu )^2} \\
  &= \frac{1}{N} \sum_i \expect{X_i^2 - 2\mu X_i + \mu^2} \\
  &= \expect{X^2} - 2 \mu \expect{X} + \mu^2 \quad\text{(since $X_i$ are identic. distrib.)} \\
  &= \expect{X^2} - \mu^2 \\
  &= \var{X}. \quad\text{(since $\var{X} = \expect{X^2} - \expect{X}^2$)}
\end{align*}
So if we happen to know the true expected value $\mu$, then we can compute
variance in the naive way, and it's exactly correct.

Now we can use this result to pull a little mathematical trick. Even if we
don't know $\mu$ and $\var{X}$, we do know they exist, so we can manipulate
\eqref{eq:estimator-vx1} to make it easier to work with. Specifically, I'm
going to write ``standardized''\footnote{You're probably used to seeing
standardized \emph{normal} variables written this way, but note that I haven't
assumed that the $X_i$ are normally distributed. This logic holds for any
random variable that has a well-defined expected value and variance.}
random variables:
\begin{equation}
Z_i = \frac{X_i - \mu}{\sqrt{\var{X}}} \implies X_i = \sqrt{\var{X}} Z_i + \mu
\end{equation}
It should be easy to see that $Z_i$ has expected value 0 and variance $\expect{Z_i^2} = 1$,
and $\overline{Z}$ has expected value
0 and variance $\expect{\overline{Z}^2} = 1/n$. Now I'll rewrite \eqref{eq:estimator-vx1}
so it has $Z_i$ instead of $X_i$:
\begin{align*}
\hat{\mathbb{V}}_X
  &= \frac{1}{n} \sum_i \left( X_i - \overline{X} \right)^2 \\
  &= \frac{1}{n} \sum_i \left( \left[\sqrt{\var{X}} Z_i + \mu\right] - \left[ \sqrt{\var{X}} \overline{Z} + \mu \right] \right)^2 \\
  &= \frac{\var{X}}{n} \sum_i \left( Z_i - \overline{Z} \right)^2
\end{align*}
Analogous to how we showed that $\var{X} = \expect{X^2} - \expect{X}^2$, some
algebra shows that
\begin{equation}
\sum_i \left(Z_i - \overline{Z}\right)^2 = \sum_i Z_i^2 - n \overline{Z}^2.
\end{equation}
Thus, the expected value of this estimator is
\begin{align*}
\expect{\hat{\mathbb{V}}_X}
  &= \expect{\frac{\var{X}}{n} \sum_i \left( Z_i - \overline{Z} \right)^2} \\
  &= \frac{\var{X}}{n} \expect{\sum_i Z_i^2 - n \overline{Z}^2} \\
  &= \frac{\var{X}}{n} \left( \sum_i \expect{Z_i^2} - n \expect{\overline{Z}^2} \right)\\
  &= \frac{\var{X}}{n} (n - 1) \quad\text{(using the little identities)} \\
  &= \frac{n-1}{n} \var{X}.
\end{align*}
Note that the expected value of our estimator is not equal to the thing we're
trying to estimate, so this estimator is biased! It systematically \emph{underestimate}
the true variance. Interestingly, like when we tried to cook up
an estimator for the upper limit of a uniform distribution, we ended up with
something that was off by a multiplicative factor. The solution is to make a new
estimator that has the inverse, cancelling factor out front:
\begin{equation}
\hat{\mathbb{V}}_{X,\mathrm{unbiased}}
  = \frac{n}{n-1} \times \frac{1}{n} \sum_i \left(X_i - \overline{X}\right)^2
  = \frac{1}{n-1} \sum_i \left(X_i - \overline{X}\right)^2.
\end{equation}
Using $n-1$ instead of $n$ in the denominator is known as Bessel's
correction.\footnote{Gauss was using the correction before Bessel discovered
it, as early as 1823. I hope the late date, 1823, impresses upon you how
subtle this reasoning must be. The ancient Greeks were using the arithmetic
mean, but an unbiased estimator for standard deviation took thousands of
years.}

This equation may look familiar as the ``sample variance'', typically written
\begin{equation}
s^2 = \frac{1}{n-1} \sum_i \left(x_i - \overline{x}\right)^2.
\end{equation}
I always wondered why, in Stats 101, I was told that the mean had $n$ in the
denominator but variance had $n-1$.\footnote{The traditional answer says that it's
about ``degrees of freedom'': you're ``using up'' one ``degree of freedom''
when computing $\overline{x}$, so you only have $n-1$ to compute $s^2$.
``Degrees of freedom'' is an ill-defined concept that I don't think it useful.
Explaining one mystery in terms of another mystery is not good pedagogy!}
This is why! The point is to make an unbiased estimator.

Here's the intuitive explanation. Remember that we previously derived that, if
we know $\mu$, then the equation $(1/n) \sum_i (x_i-\mu)^2$ works just fine:
we get an unbiased estimator for $\var{X}$. The trouble, then, is simultaneously
estimating $\expect{X}$ and $\var{X}$. Our estimate $\overline{x}$---although unbiased,
so not systematically above or below the true $\expect{X}$---will always be in the
middle of the our data $x_i$. Therefore, the distance between each $x_i$ and our
estimated $\overline{x}$ will be systematically smaller than the distance between
each $x_i$ and the true $\expect{X}$. By how much? Well, that's asking about how
much larger, on average, $(x_i - \overline{x})^2$ is smaller than $(x_i - \mu)^2$.
We did just that calculation: the typical values for $(x_i - \overline{x})^2$ are
part of our estimate $\hat{\mathbb{V}}_X$, while the typical values for
$(x_i - \mu)^2$ come from the true variance $\var{X}$. The difference between those
values is the bias, which we can correct for with that factor of $n/(n-1)$.

Comfortingly, as $n$ grows, the difference between using $n$ and $n-1$ in the
denominator becomes unimportant. If you're a big data person, then there's
really no need to worry about Bessel's correction. The point of all this was
to show the logic of looking for unbiased estimators, and to show you that
there's a good example of a bias-corrected estimator right under all our
noses.\footnote{Distressingly, because the square root is a concave function,
the square root of the unbiased estimator of variance is not an unbiased
estimator of the standard deviation. However, the difference is so small that
I don't think anyone really worries about it.}

\subsubsection{Consistency of variance estimators}

Gosh, consistency again? Well, you have to ask: does the unbiased variance
estimator that we learned in Stats 101 approach the true value with infinite
data?

It turns out this is not a trivial question. The rough idea is that the
variance estimator can be written as $(1/n) \sum_i X_i^2 - \overline{X}^2$.
The first part, a sum of many iid variables, converges toward a normal
distribution (according to the \emph{cental limit theorem}, which we'll get to
later). We previously showed that $\overline{X}$ converges to $\expect{X}$.
Then you need some tools called \emph{Slutsky's theorem} to showed that the
square of $\overline{X}$ converges to the square of $\overline{X}$ and that
the difference between two random variables converges to the difference of the
things they converge to.

Again, this isn't something that should concern the practical, data-minded
scientist, but it is interesting to know that, when you peek under the hood,
there are some deep chasms.

\subsection{Maximum likelihood estimate of the mean of a normal distribution}

Why is this a good idea? Early thinkers in statistics---who thought
about it before it had a name---were curious about the same thing. The
arithmetic mean was computationally simple (i.e., you could do it will
quill and parchment, OK, pen and paper) and it seemed to ``work'', but
why?

It turns out that the arithmetic mean has some nice properties for, you
guessed it, the normal distribution. In fact, we'll show that it is a
certain kind of ``best way'' to guess the true, population mean from our
sample mean. (It's the maximum likelihood estimator.)

\subsection{Median and mode: other estimators of central tendency}

In typical speech, we say something like, ``The average family has two
children'', by which we mean that a typical family has two children.
This is confusing because we also use the word ``average'' to refer to
the arithmetic mean. These two things are similar only in special cases,
including---you guessed it---the normal distribution.

The arithmetic mean of number of children in families is somewhere
between 2 and 3 (let's say 2.5), but it's clear that no ``average
family'' has 2.5 children. Similarly, it's confusing that the mean
salary in the US is whatever, even thought that is above what whatever
percent of people get paid.

Typical can mean ``common'', so we might say, in a sense, that an
``average'' family has two children. Of the number of children you can
have (zero, one, two, etc.), two is the most common, so that's
``average''. Technically, the most common value is the \emph{mode}. No
one really talks about the mode except in stats textbooks.

A more interesting number is the middle point. We might say that an
average American makes whatever because half of people make more and
half of people make less. This is the \emph{median} (from Latin
\emph{medius}, meaning ``middle'').

It even feels natural to combine the median and mode ideas (have our
median cake and eat it a la mode?). We want a sense of what are
``middling'' numbers, and we want a sense of what are ``common''
numbers. So you might ask what range is covered by the most middle half
of numbers.

Children are often measured against a growth chart, which shows
\emph{percentiles}: if you are in the 25th percentile, you are taller
than 50\% of people. (The median is just another name for the 50th
percentile.) The range between the 25th and 75th percentiles, which
covers the middle half of people, is the \emph{interquartile range},
because the 25th, 50th, and 75th percentiles are also called the
\emph{quartiles}, because they divide the numbers into four quarters
(bottom quarter, bottom-middle, top-middle, and top).

\subsection{Mean, median, and mode are usually
different}\label{mean-median-and-mode-are-usually-different}

Roll a dice many times. What are the mean, median, and mode? Mean is
easy: each number is equally likely to come up, so we just take the mean
of 1, 2, 3, 4, 5, 6, which is 3.5. The median is a little tricky here:
it's between 3 and 4, and when we hit this situation we normally define
the median as the arithmetic mean of the two middle values. So the
median is also 3.5 here, but only because of some convention. The mode
is also confusing, since all numbers are equally likely, which means
that they are all equally the mode.

For the normal distribution, the mean, median, and mode are all the
same. So it's only in this very potentially unusual case that our
intuition is correct that that the ``average'' value, in the sense of
something common (i.e., the mode), is the same as the ``average'' value,
in the sense of something middling (i.e., the median), is the same as
the arithmetic mean.

Historical box for Quetelet: the idea of an ``average person'' is
directly traceable to a particular proto-statistician, who is remarkable
for having believed that almost \emph{everything} was normally
distributed. At the time, they only had very rudimentary methods for
determining if something was normally distributed (basically, eyeballing
it), and there were very appealing aesthetic/philosophical reasons to
believe that almost everything was normally distributed, so he believed
that too.