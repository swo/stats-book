%!TEX root=main

\chapter{Confidence intervals}

\section{Definition}

So far we have worked with \emph{point estimates}, that is, guesses for
population parameters that are just a single number. This presents a problem,
because statistics and probability are all about uncertainty, and we know that
the point estimate we make for, say, $B$ in the German tank problem will likely
never be exactly correct, even if the estimator is consistent, unbiased, and
efficient.

In the frequentist framework, the approach for expressing uncertainty is using
\emph{confidence intervals}. A confidence interval for a parameter $\theta$ with estimator $\hat{\theta}$
at \emph{level} $1-\alpha$ is a pair of estimators $\hat{\theta}_-$ and $\hat{\theta}_+$
defined such that, for any parameter value $\theta$, it holds that:
\begin{equation}
  \prob{\hat{\theta}_- \leq \theta \leq \hat{\theta}_+} \geq 1 - \alpha.
\end{equation}
A typical value for $\alpha$ is 5\%, or $0.05$, which yields 95\% confidence intervals.

In other words, in the frequentist approach, we develop two statistics ---functions
of the observed data--- such that their corresponding estimators will, with some
probability, enclose the true value, for every possible true value. Strictly speaking,
you must select a method for constructing confidence intervals such that, for any
parameter $\theta$ I choose, the proportion of infinitely many repeated trials will
produce realizations of the confidence interval that enclose $\theta$.

Note that $\alpha$ is the probability of an error, that is, of the confidence intervals not
enclosing the true value. Thus, a higher confidence percentage, which translates to a lower 
$\alpha$, means a lower probability of the
confidence intervals being incorrect, and thus wider confidence intervals.

\section{Meaning and interpretation}

Confidence intervals are a very new concept. They were first introduced in the statistical literature in 1937 and become commonplace in scientific work only in the later 20th century.
It is perhaps no surprise, given its youth, that the concept of the confidence interval
is very confusing.

The most common misconception about confidence intervals is that they represent
\emph{confidence}. (One might argue that ``confidence'' was a poor word to use to name this
thing!) In this misconception, we can have 95\% confidence that the true parameter $\theta$
lies inside the confidence interval constructed after the experimental data has been gathered.
Strictly speaking, this is incorrect. In a frequentist framework, the true
value is either inside the realized confidence interval, or it is not; our ignorance of
the true value has nothing to do with probability. In fact, the interval that encloses
the true value with some ``confidence'' is a Bayesian concept, the \emph{credible interval}.
``Confidence'' is part of the Bayesian definition of probability; it is foreign to the
frequentist construction.

To a practicing scientist, this distinction might appear entirely semantic. Regardless of
what they specifically mean, it is useful to have some quantification of uncertainty.

\section{Constructing intervals}

To show how confidence intervals are constructed, consider the German
tank problem again. Our unbiased point estimate is $\hat{B} = \tfrac{n+1}{n} \max_i X_i$.
We will design a \emph{symmetric} confidence interval, which means that:
\begin{equation*}
    \prob{\hat{B}_- \leq B} = \prob{\hat{B}_+ \geq B} = 1 - \frac{\alpha}{2}
\end{equation*}
This confidence interval is symmetric because the probability that the confidence interval
will not include $B$ because it is too low is equal to the probability that it will not
include $B$ because it is too high. For a 95\% confidence interval, the probability of the
confidence interval being wrong on either side is $2.5\%$, for a 5\% total probability of
error.

To construct $\hat{B}_-$ and $\hat{B}_+$, we will find the range of values in which the point
estimator $\hat{B}$ is expected fall, given $B$. We found that the cdf for the biased
estimator $\max_i X_i$ was $(x/B)^n$, so the cdf for the
unbiased point estimator:
\begin{equation*}
    \prob{\hat{B} \leq x} = \frac{n+1}{n} \left(\frac{x}{B}\right)^n
\end{equation*}
First, we use this cdf to find the value of $x$ such that $\hat{B}$ will fall below it
with probability $1-\frac{\alpha}{2}$:
\begin{equation*}
    \prob{\hat{B} \leq x} = 1 - \frac{\alpha}{2} \implies x = \left( \frac{\alpha}{2} \frac{n}{n+1}\right)^{1/n} B
\end{equation*}
Next, we ``invert'' this relationship:
\begin{equation*}
    1 - \frac{\alpha}{2}
    = \prob{\hat{B} \leq \left( \frac{\alpha}{2} \frac{n}{n+1}\right)^{1/n} B}
    = \prob{\left( \frac{\alpha}{2} \frac{n}{n+1}\right)^{-1/n} \hat{B} \leq B}
\end{equation*}
And thus, we have found our lower confidence interval:
\begin{equation*}
    \hat{B}_- = \left( \frac{\alpha}{2} \frac{n}{n+1}\right)^{-1/n} \hat{B}
\end{equation*}
A similar exercise shows that:
\begin{equation*}
    \hat{B}_+ = \left[ \left(1- \frac{\alpha}{2}\right) \frac{n}{n+1}\right]^{-1/n} \hat{B}
\end{equation*}

As an example, for $n=5$, $\max x_i = 1$, and $\alpha = 5\%$, we have $\hat{B} = 1.2$, $\hat{B}_- = 1.04$ and $\hat{B}_+ = 2.17$. For $n=100$ and the same observed maximum $1$,
we have $\hat{B} = 1.01$,
$\hat{B}_- = 1.0004$, and $\hat{B}_+ = 1.04$. In both these cases, we do not say what $B$
actually is, so we cannot say whether or not the confidence intervals contain the true value
or not. We only know that, regardless of what $B$ is, there is a 95\% probability that the
resulting data will produce, according to our definitions, a value of $\hat{B}_-$ and
$\hat{B}_+$, that contain $B$.

We could have been much lazier with our confidence intervals. For example, I could have
defined $\hat{B}_- = 0$. Because we know \textit{a priori} that $B>0$, this lower end
of the confidence interval will always be correct. In fact, it means that the confidence
intervals will be contain $B$ with a probability greater than $1-\alpha$. This is an
undesirable feature. Confidence intervals with the desirable property that they have a
error probability of exactly $\alpha$ and no more are termed \emph{valid} confidence intervals.

\section{Bootstrapping}

\section{Jackknife}

\section{Profile method}