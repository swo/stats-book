%!TEX root=main

\chapter{Parameters and estimators}

\section{Orientation and definitions}

\subsection{Descriptive and inferential statistics}

Up to this point, random variables and their values have occurred in a
deductive kind of way: you know what the distribution of the random variable
is, and then you ask what its expected value or variance is. Statistics is
about going in the opposite, inferential direction. Given that I have some data---that is,
realizations of the values of some random variables---what can I say about the
distributions that those values were drawn from? Indeed, the word \emph{statistic}
means a function of the data (or the value of that function) that depends only
on the data.

This characterization of statistics embraces a common typology of statistics into
\emph{descriptive} statistics and \emph{inferential} statistics. Inferential
statistics, which includes hypothesis testing, certainly has some more
philosophical and technical hang-ups, but it's fair, at this point, to say
that both descriptive and inferential statistics hinge on how we can use the
data we have to say something about the populations that we drew that data
from. Indeed, the reason the field of study is called ``descriptive statistics''
is because it studies statistics---functions of the data---that are descriptive
of the distributions from which the data were drawn.

The jargon for this deductive/inductive difference is that mathematical
probability considers \emph{statistical parameters}, which are functions of
the distributions that give numbers, and \emph{estimators}, which are
functions of the data that approximate something about the true distribution.
The expected value $\mathbb{E}$ and variance $\mathbb{V}$ that we learned
about earlier are statistical parameters: they take a random variable (or, loosely
speaking, a ``theoretical distribution'') and
return a number. You can also think of
statistical parameters are an ``index'' of the distribution. We say ``a normal
distribution with mean $\mu$ and variance $\sigma^2$'' because, having said
those two things, we've perfectly specified what distribution we're talking
about.

This chapter is about the second thing, estimators, which, as their name
implies, are functions of the data that estimate something about the true
distribution.

\subsection{Samples and populations}

Up to this point, I've used the word ``distribution'' in the mathematical
probability sense: it's the cdf $F_X$ or the pmf/pdf $f_X$. After this point,
it will become crucial to draw a distinction between these mathematical
probability objects on the one hand and, on the other hand, the mass of data
that we get from an experiment, which might also be called the ``distribution''
of the data.

The distinction is usually made in the jargon of the social sciences. You draw
some \emph{samples} from a target \emph{population}. If you're interested in
the height of male versus female humans, then your target populations will be,
say, \emph{all} American (or whatever) males and \emph{all} American female.
In some alternate universe, you can somehow just measure the height of all
American males and female. After this, the analysis doesn't require any
probability or statistics: you just look at the complete, real data and draw
whatever conclusions you wanted to. In contrast, in this universe, you need to
take a sample of each of those populations. You use the sample data to
estimate the properties of the population.

I take this language another step further, because it's typical to make
further assumptions about the target population. In reality, the distribution
of the true population is going to be some messy curve. If you squint, it will
probably look normally distributed. But if you reallydid measure all 300 million
Americans, you would certainly find \emph{some} deviation from the normal
distribution.\footnote{This fact, that taking more data makes it hard to fit
traditional statistical models, will come up later.}

However, it's typical to assume that the target population is normally
distributed and call that normal distribution the ``population''. In that
sense, a lot of statistical methods are trying to infer the properties of some
ideal, mathematical distribution that \emph{approximates} the literal target
population. This distinction is usually not made, so when someone says
``population'' they mean ``normal distribution with mean and standard
deviation whatever''. It's also not unusual to hear about the ``true''
population, by which again is meant the idealized $\mathcal{N}(\mu,
\sigma^2)$, which might be confusing because the literal target population
will almost certainly \emph{not} be normally distributed, since the normal
distribution is just a mathematical abstraction. So just keep in mind that
most statistics only distinguishes between the \emph{sample}---the data you
have, which are a subset of the literal target population---and the ``true
population''---which is some idealized approximation of the literal target
population.

\subsection{Variation and error}

In the ``hard'' experimental sciences, this language about ``population'' may
sound weird. If I'm trying to measure the speed of light, I'll repeat my
experiment many times and get many different values. In the jargon, those
values are the sample. What, then, is the population? We think there is a
single, true speed of light. In this case, the ``population'' refers, somewhat
tautologically, to the distribution of the values that would come from an
infinite number of repetitions of our experiment. In other words, the sample
defines the population, not the other way around.

In the social sciences example, the shape of the distribution of measured
values says something about the variation of the values in the target
population: we get a range of measured values because people are different
heights. In the physics example, the shape of the distribution has a lot to do
with the precision in our experiment: we get a range of measured values
because of all sorts of error that arie the in the process of experimentation.

In biology, the situation is somewhere in between: we think there is true
variation in, say, the physiology of the cells used in an experiment. We
might wish there weren't: it would be easier to do some experiments if all
cells started in exactly the same state and responded exactly the same way to
an experimental condition. The meaning of the range of values we get is then
something of a philosophical one. To what degree do we treat the range of
values as due to error, because we couldn't make all the cells the same, and
to what degree is it an honest report about the heterogeneity of behavior in
some ``target population'' of cells? The answer depends on your research
question.

\subsection{The logic of statistics}

Regardless of whether the variation we observe is due to ``real'' variation in
the target population or to mere error, the whole logic of statistics is to
collect multiple data points and use them to infer a single truth.

This was a radical notion when it was first proposed. How could multiple,
trashy bits of data somehow make something great? It's like saying that you
can ask a thousand untrained people to make a sketch of an enigmatic, smiling
woman and somehow expect the combination of those thousand sketches to turn
into the Mona Lisa. The assumed correct answer, before statistics, was to pick
the best value. If multiple scientists reported different values, you would
examine their experimental methods, see how good they were with their hands,
and pick the value from the scientist with the best method.

This idea of ``combination of observations'' is today so ingrained that it's
hard to imagine \emph{not} using a combination of observations to infer a
truth. I think a useful bridge is the idea of meta-analysis. For example, does
a certain educational intervention improve students' learning? Until the
1970s, the assumed correct answer was to examine all the interventions, see
which one worked well, and mostly ignore the ones that were imperfect
according to the measure established by that one study (or small set of
studies). In the 1970s, meta-analysis started comparing multiple studies---
which detractors said was like comparing apples and oranges---to ask about
whether an entire class of education intervention works. The 1970s was not
that long ago.


\section{Properties of estimators}

Let's get to it! What are these estimators? What do they do? Rather than give
some dry definitions, I'll use a motivating example, from which the
definitions will arise (I hope) naturally.

\subsection{A motivating example: the German tank problem}

During World War II, it was important to the Allies to know how many tanks
Germany was producing. One way to find this out was with spies, aerial
reconnaissance, etc. The other way---you guessed it!---was with statistics.

The German tanks had sequential serial numbers for various parts of the tank.
This means, if you had a list of all the serial numbers, you could figure out
easily figure out how many tanks there were, since they would go from 1 up to
the number of tanks. Of course, if you had all the serial numbers, you
probably got them from the tanks, and you could probably just count the tanks.

Consider this simplified problem: say you know the serial numbers go from 1 up
to $N$, the actual number of tanks produced, and say you capture a few tanks
with serial numbers $x_1, x_2,$ etc. How many tanks do you think there
are?

It turns out that the math will be simpler for the continuous case, in which
we draw numbers from some continuous, uniform distribution between $A=0$ and
$B>A$. The probability distribution function for a random variable $X$ that
follows this distribution is:
\begin{equation}\label{eq:uniform_pdf}
f_X(x) = \begin{cases}
  1/B &\text{if $0 \leq x \leq B$} \\
  0 &\text{otherwise}
\end{cases}
\end{equation}
The idea is, if you don't know $B$, we'll draw many data points from this
distribution and use them to make guesses about the value of $B$.\footnote{The math
of the German tank problem, with integers, is very similar, except that a
certain sum over integers is hard and the corresponding integral over
continuous numbers is easy.}

The idea is to generate an estimator, a function of our data that will
approximate the true value $B$. It's typical to write estimators as the
true value with a ``hat'', so we're interested in making a function $\hat{B}$.

\subsection{Consistent estimators}

Let me start with what will seem like a very inane example.\footnote{Perhaps
this sounds silly, and it is, but I've more than once heard respected and
well-meaning scientists say that the answer to statistical questions is ``3'',
just 3.}  Say I draw $n$ data points, and then I ignore all of them and just
pick 3:
\begin{equation}
\hat{B}_\mathrm{inane}(\{x_i\}) \defeq 3,
\end{equation}
where $\{x_i\}$ means that $x_i$ for $i = 1, \ldots, n$.
Now, this technically is an estimator: it's a function of the
data alone, only it just so happens to not use any of that data. It's also,
and not just technically, not a good estimator at all. The main problem is
that, no matter how much data I accumulate, it's not guaranteed to be right.
(In fact, it's guaranteed to be right only if $B=3.000\ldots$ just by sheer luck.)

A very valuable improvement presents itself: if I drew $n$ data points, why not
just take the maximum of those data points? I'll call this $\hat{B}_1$ since
it's our first reasonable guess:
\begin{equation}\label{eq:hatb1}
\hat{B}_1\left( \{x_i : i \in 1, \ldots, n\} \right) \defeq \max_i x_i
\end{equation}
Note that $\hat{B}_1$ is a function only of our data, the sampled numbers $x_i$.

This new estimator has the crucial property of being \emph{consistent}, which
means that, as you collect more and more data, the value of the estimator
approaches the true value of the thing being estimated:
\begin{equation}\label{eq:consistent-estimator}
\text{$\hat{B}$ is a consistent estimator for $B$}
  \implies \lim_{n\to\infty} \hat{B}(\{x_i : i \in 1,\ldots,n\}) = B
\end{equation}

\subsection{Estimators are random variables}

I pulled two fast tricks in \eqref{eq:consistent-estimator} that I should explain.

First, I said that an estimator is a function of the data. That's true, from a
certain point of view: you can think of an estimator that takes numbers and
gives back another number. Analogously, you can think of an estimator as a
random variable that takes on values in the same way that the function-of-
numbers estimator takes numbers as input. In other words, I could write \eqref{eq:hatb1}
as
\begin{equation}
\hat{B}_1\left( \{X_i\} \right) \defeq \max_i X_i.
\end{equation}
That's right: I just changed the lowercase $x_i$ to uppercase $X_i$. So what?

The point is that, so long as we think of $\hat{B}_1$ as a function that just
takes numbers, we can't do anything about probability with it. However, it is
easy to translate a function-of-numbers into a function-of-random-variables.
Now that $\hat{B}_1$ is itself a random variable, we can ask interesting questions
about its behavior without examining the outcome of any particular experiment.
Later in this section, you'll see why looking at the expected value and variance
of an estimator is interesting.

Second, I used the word ``limit'' in \eqref{eq:consistent-estimator}.
If you're a math wizzkid you'll have pounced that. To say that
a \emph{sequence of numbers} has a limit means that, for any threshold difference
$\varepsilon$ you give me, I can give you an integer $N$ such that for every value
in the sequence after the $N$-th one is within $\varepsilon$ of the true value.
Clearly this definition is not sufficient in probability, where you could never
be guaranteed, even after a trillion data points, of anything.
In this example, there's always some
chance that all your data points happen to fall really short of $B$.

Instead, in
probability, we say that a sequence of random variables $\{Y_i\}$ \emph{converges}
toward a number $a$
\begin{equation}
\lim_{n\to\infty} \prob{|Y_n - a| > \varepsilon} = 0,
\end{equation}
in other words, if the probability that $Y_n$ takes on a value more than $\varepsilon$
away from $a$ vanishes. In our example, $a$ is $B$,
and the $Y_n$ are the random-variable estimators
for each $n$: $\max_i \{X_i : i \in 1,\ldots, n\}$.

It's fairly intuitive that $\hat{B}_1$ converges to $B$. To prove it, you give me
some small $\varepsilon > 0$.
The probability of a point falling more than $\varepsilon$ below $B$
is $1 - \varepsilon/B$. The probability of all $n$ points falling $\varepsilon$ from $B$
is $(1 - \varepsilon/B)^n$,
which goes to zero as $n$ increases, no matter how small you make $\varepsilon$.

\subsection{Unbiased estimators}

Great! We have a consistent estimator $\hat{B}_1$. If I collect a zillion data
points, I'll get really close to $B$.

This seems like poor consolation. I'm rarely drawing a zillion data points.
The Allies certainly weren't capturing a zillion German tanks.
Consider the opposite case, if I draw only one data point. Clearly the maximum
of the drawn values, just one that point, is a pretty bad estimate for the
upper limit. If you draw 3 points, then the maximum is a little better, but
you can still feel very sure that estimating population maximum $B$ using the
sample maximum is \emph{biased}: it is always going to underestimate the
population maximum. This is true even if I take a zillion data points: I'm
never ever going to overestimate $B$ using $\hat{B}_1$.

Formally, a \emph{biased} estimator is one whose expected value is not the
thing it's trying to estimate. Conversely, an \emph{unbiased} estimator is
one whose expected value is the thing it's trying to estimate:
\begin{equation}
\text{$\hat{B}$ is an unbiased estimator for $B$} \implies \expect{\hat{B}} = B.
\end{equation}

How do we turn a biased estimator into an unbiased one? Maybe, if we compute
$\expect{\hat{B}}$, we can just look and find some way. This will require the
probability density function for $\max_i X_i$. Similar to the calculation above,
it's actually not hard to write the probability of a single data point being
below $x$, that is between zero and $x$: it's just $x/B$. The probability of all
$n$ data points falling between zero and $x$---which is the same as saying that
the maximum is $x$---is $(x/B)^n$. I'll write a new random variable $M = \max_i X_i$
so that
\begin{equation}
F_M(x) = (x/B)^n.
\end{equation}
Recall that the probability density function is the derivative of the cumulative
distribution function:
\begin{equation}
f_M(x) = \frac{d}{dx} F_M(x) = \frac{n}{B} \left( \frac{x}{B} \right)^{n-1}.
\end{equation}
The expected value is the integral over this probability distribution function:
\begin{equation}
\expect{M} = \int_0^B x f_M(x) \,dx = \frac{n}{n+1} B.
\end{equation}

Let's examine this result: if $n=1$, then $\expect{M} = \tfrac{1}{2}B$. In
other words, if you draw one point, you expect it to be right in the middle of
the range $[0, B]$. Seems reasonable. For $n=2$, $\expect{M} = \tfrac{2}{3}B$.
In other words, you expect your data points to fall at something like
$\tfrac{1}{3}$ and $\tfrac{2}{3}$, so you expect a maximum of $\tfrac{2}{3}$.
Clearly, as $n \to \infty$, $\expect{M} \to B$, like we had shown previously.

I can just flip the numerical factor out front to get
an unbiased estimator:
\begin{equation}
\hat{B}_\mathrm{unbiased} \defeq \frac{n+1}{n} \max_i X_i.
\end{equation}
I did this so that:
\begin{equation}
\expect{\hat{B}_\text{unbiased}} = \expect{\frac{n+1}{n} \max_i X_i} = \frac{n+1}{n} \expect{\max_i X_i} = B.
\end{equation}
So now we've assured that our estimates for $B$ are ``centered'' around
$B$.\footnote{I put ``cenetered'' in quotes because the expected value can be
a weird way to quantify ``middleness''. We'll get to that soon.} Sometimes we
will overestimate $B$, but at least we won't, in a sense, be systematically
underestimating it.

\subsection{Best linear unbiased estimators (BLUEs)}

You might go even further, and demand that an estimator, aside from being
cenetered around $B$, also have a minimal spread around $B$. In a very
confusing turn of phrase, the estimator that, in a class of estimator, has
minimum variance is the \emph{best} estimator. You're most likely to encounter
this terminology in the specific case of linear regression models, for which
it turns out that ordinary least squares gives the \emph{best linear} unbiased
estimator, where ``best'' we just defined and ``linear'' means that the
estimator is a linear combination of the data.

It's worth noting that our $\hat{B}_\text{unbiased}$ is a linear combination
of the $X_i$: it has coefficient zero for all the $X_i$ except for the one
that takes on the biggest value, which we give coefficient 1. Beyond that,
though, I don't think it's worth digging into the variance of
$\hat{B}_\text{unbiased}$. I do think it's worth understanding that different
estimators can vary in their precision.

\subsection{Maximum likelihood estimators}

In this simple case, it was pretty easy to cook up a good estimator. There was
a very short chain of logic between ``want maximum of true distribution'' to
``look at maximum of observed values''. It's usually not this easy. To revisit
the question of regression, if I have a bunch of data points, what's the a
good way to make a slope out of them? It's not \textit{a priori} obvious.

There's a convenient methodology for making sensible estimators, called
\emph{maximum likelihood} estimators. \emph{Likelihood} is a confusing word:
in common speech, ``likelihood'' and ``probability'' are synonyms. In
statistics jargon, they mean similar things, with slightly different emphases.
A ``probability'' function typically asks about the probability of different
outcomes given some fixed parameters. A ``likelihood'' functions asks about
the probability (or probability density) of some fixed outcome while varying
the parameters. Imagine a probability density function $f_X$ for some data
value $x$ and some parameter $\theta$. We say that $f_X(x; \theta)$ is the
probability function and $L(\theta; x) \defeq f_X(x; \theta)$ is the
likelihood function. We say ``maximum likelihood'' rather than ``maximum
probability'' as a way of emphasizing that the maximization will be done for
fixed data points over varying parameters.

For our example, the maximum likelihood (ML) approach says: as an estimator, pick
the value such that, if it were the true value, would maximize the probability
density of the observed data. That is:
\begin{equation}
\hat{B}_\text{ML} \defeq \max_b \prod_i f_X(x_i; b)
\end{equation}
We can use \eqref{eq:uniform_pdf} to compute the probability:
\begin{align*}
\hat{B}_\text{ML} = \max_b \begin{cases}
0 &\text{if any $x_i > b$} \\
\prod_i (x_i/b) &\text{otherwise}
\end{cases}
\end{align*}
The first case means that we can overshoot at all, and the second says that
the likelihood decreases if we decrease $b$. The solution is that
$\hat{B}_\text{ML} = \max_i X_i$, which was our first good guess.

As an exercise, redo these calculations for the German tank problem, where you
draw integers between 1 and an unknown $N$. Start by assuming that $N$ is much
larger than the number of integers you draw so that the chance of drawing the
same integer twice is really small and you can treat the separate draws as
independent. The expected value will be tough to calculate, so just do it for
drawing one point. Then, do everything with a computer.

