%!TEX root=main

\chapter{Parameters and estimators}

\section{Orientation and definitions}

\subsection{Descriptive and inferential statistics}

Up to this point, random variables and their values have occurred in a
deductive kind of way: you know what the distribution of the random variable
is, and then you ask what its expected value or variance is. Statistics is
about going in the opposite, inferential direction. Given that I have some data---that is,
realizations of the values of some random variables---what can I say about the
distributions that those values were drawn from? Indeed, the word \emph{statistic}
means a function of the data (or the value of that function) that depends only
on the data.

This characterization of statistics embraces a common typology of statistics into
\emph{descriptive} statistics and \emph{inferential} statistics. Inferential
statistics, which includes hypothesis testing, certainly has some more
philosophical and technical hang-ups, but it's fair, at this point, to say
that both descriptive and inferential statistics hinge on how we can use the
data we have to say something about the populations that we drew that data
from. Indeed, the reason the field of study is called ``descriptive statistics''
is because it studies statistics---functions of the data---that are descriptive
of the distributions from which the data were drawn.

The jargon for this deductive/inductive difference is that mathematical
probability considers \emph{statistical parameters}, which are functions of
the distributions that give numbers, and \emph{estimators}, which are
functions of the data that approximate something about the true distribution.
The expected value $\mathbb{E}$ and variance $\mathbb{V}$ that we learned
about earlier are statistical parameters: they take a random variable (or, loosely
speaking, a ``theoretical distribution'') and
return a number. You can also think of
statistical parameters are an ``index'' of the distribution. We say ``a normal
distribution with mean $\mu$ and variance $\sigma^2$'' because, having said
those two things, we've perfectly specified what distribution we're talking
about.

This chapter is about the second thing, estimators, which, as their name
implies, are functions of the data that estimate something about the true
distribution.

\subsection{Samples and populations}

Up to this point, I've used the word ``distribution'' in the mathematical
probability sense: it's the cdf $F_X$ or the pmf/pdf $f_X$. After this point,
it will become crucial to draw a distinction between these mathematical
probability objects on the one hand and, on the other hand, the mass of data
that we get from an experiment, which might also be called the ``distribution''
of the data.

The distinction is usually made in the jargon of the social sciences. You draw
some \emph{samples} from a target \emph{population}. If you're interested in
the height of male versus female humans, then your target populations will be,
say, \emph{all} American (or whatever) males and \emph{all} American female.
In some alternate universe, you can somehow just measure the height of all
American males and female. After this, the analysis doesn't require any
probability or statistics: you just look at the complete, real data and draw
whatever conclusions you wanted to. In contrast, in this universe, you need to
take a sample of each of those populations. You use the sample data to
estimate the properties of the population.

I take this language another step further, because it's typical to make
further assumptions about the target population. In reality, the distribution
of the true population is going to be some messy curve. If you squint, it will
probably look normally distributed. But if you reallydid measure all 300 million
Americans, you would certainly find \emph{some} deviation from the normal
distribution.\footnote{This fact, that taking more data makes it hard to fit
traditional statistical models, will come up later.}

However, it's typical to assume that the target population is normally
distributed and call that normal distribution the ``population''. In that
sense, a lot of statistical methods are trying to infer the properties of some
ideal, mathematical distribution that \emph{approximates} the literal target
population. This distinction is usually not made, so when someone says
``population'' they mean ``normal distribution with mean and standard
deviation whatever''. It's also not unusual to hear about the ``true''
population, by which again is meant the idealized $\mathcal{N}(\mu,
\sigma^2)$, which might be confusing because the literal target population
will almost certainly \emph{not} be normally distributed, since the normal
distribution is just a mathematical abstraction. So just keep in mind that
most statistics only distinguishes between the \emph{sample}---the data you
have, which are a subset of the literal target population---and the ``true
population''---which is some idealized approximation of the literal target
population.

\subsection{Variation and error}

In the ``hard'' experimental sciences, this language about ``population'' may
sound weird. If I'm trying to measure the speed of light, I'll repeat my
experiment many times and get many different values. In the jargon, those
values are the sample. What, then, is the population? We think there is a
single, true speed of light. In this case, the ``population'' refers, somewhat
tautologically, to the distribution of the values that would come from an
infinite number of repetitions of our experiment. In other words, the sample
defines the population, not the other way around.

In the social sciences example, the shape of the distribution of measured
values says something about the variation of the values in the target
population: we get a range of measured values because people are different
heights. In the physics example, the shape of the distribution has a lot to do
with the precision in our experiment: we get a range of measured values
because of all sorts of error that arie the in the process of experimentation.

In biology, the situation is somewhere in between: we think there is true
variation in, say, the physiology of the cells used in an experiment. We
might wish there weren't: it would be easier to do some experiments if all
cells started in exactly the same state and responded exactly the same way to
an experimental condition. The meaning of the range of values we get is then
something of a philosophical one. To what degree do we treat the range of
values as due to error, because we couldn't make all the cells the same, and
to what degree is it an honest report about the heterogeneity of behavior in
some ``target population'' of cells? The answer depends on your research
question.

\subsection{The logic of statistics}

Regardless of whether the variation we observe is due to ``real'' variation in
the target population or to mere error, the whole logic of statistics is to
collect multiple data points and use them to infer a single truth.

This was a radical notion when it was first proposed. How could multiple,
trashy bits of data somehow make something great? It's like saying that you
can ask a thousand untrained people to make a sketch of an enigmatic, smiling
woman and somehow expect the combination of those thousand sketches to turn
into the Mona Lisa. The assumed correct answer, before statistics, was to pick
the best value. If multiple scientists reported different values, you would
examine their experimental methods, see how good they were with their hands,
and pick the value from the scientist with the best method.

This idea of ``combination of observations'' is today so ingrained that it's
hard to imagine \emph{not} using a combination of observations to infer a
truth. I think a useful bridge is the idea of meta-analysis. For example, does
a certain educational intervention improve students' learning? Until the
1970s, the assumed correct answer was to examine all the interventions, see
which one worked well, and mostly ignore the ones that were imperfect
according to the measure established by that one study (or small set of
studies). In the 1970s, meta-analysis started comparing multiple studies---
which detractors said was like comparing apples and oranges---to ask about
whether an entire class of education intervention works. The 1970s was not
that long ago.


\section{Properties of estimators}

Let's get to it! What are these estimators? What do they do? Rather than give
some dry definitions, I'll use a motivating example, from which the
definitions will arise (I hope) naturally.

\subsection{A motivating example: the maximum of a uniform distribution}

Imagine you're doing an experiment where you get numbers that are uniformly
distributed between some $A$ and $B$. For simplicity, let's set $A=0$ so that
the probability distribution function for a random variable $X$ that follows
this distribution is:
\begin{equation}\label{eq:uniform_pdf}
f_X(x) = \begin{cases}
  1/B &\text{if $0 \leq x \leq B$} \\
  0 &\text{otherwise}
\end{cases}
\end{equation}
Imagine that you don't know $B$, and you're trying to figure it out, and the
way you've decided to do that it draw many iid variables $X$. (I know this sounds
like a silly experiment.)

The idea is to generate an estimator, a function of our data that will
approximate the true value $B$. It's typical to write estimators as the
true value with a ``hat'', so we're interested in making a function $\hat{B}$.

Let me start with what will seem like a very inane example.\footnote{Perhaps
this sounds silly, and it is, but I've more than once heard respected and
well-meaning scientists say that the answer to statistical questions is ``3'',
just 3.}  Say I draw $n$ data points, and then I ignore all of them and just
pick 3:
\begin{equation}
\hat{B}_\mathrm{inane}(\{x_i : i \in 1, \ldots, n \}) \defeq 3.
\end{equation}
Now, this is technically an estimator: it's a function of the
data alone, only it just so happens to not use any of that data. It's also,
and not just technically, not a good estimator at all. The main problem is
that, no matter how much data I accumulate, it's not guaranteed to be right.
(In fact, it's guaranteed to be right only if $B=3.000\ldots$ just by sheer luck.)

\subsection{Consistent estimators}

A very valuable improvement presents itself: if I drew $n$ data points, why not
just take the maximum of those data points? I'll call this $\hat{B}_1$ since
it's our first reasonable guess:
\begin{equation}
\hat{B}_1\left( \{x_i : i \in 1, \ldots, n\} \right) \defeq \max_i x_i
\end{equation}
Note that $\hat{B}_1$ is a function only of our data, the sampled numbers $x_i$.

This new estimator has the crucial property of being \emph{consistent}, which
means that, as you collect more and more data, the value of the estimator
approaches the true value of the thing being estimated:
\begin{equation}
\text{$\hat{B}$ is a consistent estimator for $B$}
  \implies \lim_{n\to\infty} \hat{B}(\{x_i : i \in 1,\ldots,n\}) = B
\end{equation}

If you're a math wizzkid you'll have pounced on my use of ``limit''. To say that
a sequence of numbers has a limit means that, for any threshold difference
$\varepsilon$ you give me, I can give you an integer $N$ such that for every value
in the sequence after the $N$-th one is within $\varepsilon$ of the true value.
Clearly this definition is not sufficient in probability, where you could never
be guaranteed, even after a trillion data points, of anything. In this example, there's always some
chance, I won't say how large, that all your data points are some finite distance away from $B$. So instead, in
probability, we say that a sequence of random variables $\{Y_i\}$ \emph{converges}
toward a random variable $Y$ if
\begin{equation}
\lim_{n\to\infty} \prob{|Y_n - Y| > \varepsilon} = 0,
\end{equation}
in other words, if the probability that $Y_n$ takes on a value more than $\varepsilon$
away from $Y$ vanishes. In our example, $Y$ is a boring random variable that always
takes on the value $B$, and the $Y_n$ are the estimators (imagined as random variables)
for each $n$: $\max_i \{X_i : i \in 1,\ldots, n\}$. I'll use the notation for limits
loosely.

It's fairly intuitive that $\hat{B}_1$ converges to $B$. To prove it, you give me
some $\varepsilon$, and I say that the probability of a point falling within $\varepsilon$
of $B$ is $\varepsilon / B$. The probability of a point falling outside that range
is $1 - \varepsilon/B$. The probability of $n$ points falling outside is $(1 - \varepsilon/B)^n$,
which goes to zero as $n$ increases.

\subsection{Unbiased estimators}

Great! We have a consistent estimator $\hat{B}_1$. If I collect a zillion data
points, I'll get really close to $B$.

This seems like poor consolation. I'm rarely drawing a zillion data points.
Consider the opposite case, if I draw only one data point. Clearly the maximum
of the drawn values, just one that point, is a pretty bad estimate for the
upper limit. If you draw 3 points, then the maximum is a little better, but
you can still feel very sure that estimating population maximum $B$ using the
sample maximum is \emph{biased}: it is always going to underestimate the
population maximum. This is true even if I take a zillion data points: I'm
never ever going to overestimate $B$ using $\hat{B}_1$.

Formally, a \emph{biased} estimator is one whose expected value is not the
thing it's trying to estimate. Conversely, an \emph{unbiased} estimator is
one whose expected value is the thing it's trying to estimate:
\begin{equation}
\text{$\hat{B}$ is an unbiased estimator for $B$} \implies \expect{\hat{B}} = B.
\end{equation}

How do we turn a biased estimator into an unbiased one? Maybe, if we compute
$\expect{\hat{B}}$, we can just look and find some way. This will require the
probability density function for $\max_i x_i$. Similar to the calculation above,
it's actually not hard to write the probability of a single data point being
below $x$, that is between zero and $x$: it's just $x/B$. The probability of all
$n$ data points falling between zero and $x$---which is the same as saying that
the maximum is $x$---is $(x/B)^n$. I'll write a new random variable $M = \max_i X_i$
so that
\begin{equation}
F_M(x) = (x/B)^n.
\end{equation}
Recall that the probability density function is the derivative of the cumulative
distribution function:
\begin{equation}
f_M(x) = \frac{d}{dx} F_M(x) = \frac{n}{B} \left( \frac{x}{B} \right)^{n-1}.
\end{equation}
The expected value is the integral over this probability distribution function:
\begin{equation}
\expect{M} = \int_0^B x f_M(x) \,dx = \frac{n}{n+1} B.
\end{equation}

Let's examine this result: if $n=1$, then $\expect{M} = \tfrac{1}{2}B$. In
other words, if you draw one point, you expect it to be right in the middle of
the range $[0, B]$. Seems reasonable. For $n=2$, $\expect{M} = \tfrac{2}{3}B$.
In other words, you expect your data points to fall at something like
$\tfrac{1}{3}$ and $\tfrac{2}{3}$, so you expect a maximum of $\tfrac{2}{3}$.
Clearly, as $n \to \infty$, $\expect{M} \to B$, like we had shown previously.

This result suggests that I can just flip the numerical factor out front to get
an unbiased estimator:
\begin{equation}
\hat{B}_\mathrm{unbiased} \defeq \frac{n+1}{n} \max_i X_i.
\end{equation}
I did this so that:
\begin{equation}
\expect{\hat{B}_\text{unbiased}} = \expect{\frac{n+1}{n} \max_i X_i} = \frac{n+1}{n} \frac{n}{n+1} B = B.
\end{equation}
So now we've assured that our estimates for $B$ are ``centered'' around
$B$.\footnote{I put ``cenetered'' in quotes because the expected value can be
a weird way to quantify ``middleness''. We'll get to that soon.} Sometimes we
will overestimate $B$, but at least we won't, in a sense, be systematically
underestimating it.

\subsection{Best linear unbiased estimators (BLUEs)}

You might go even further, and demand that an estimator, aside from being
cenetered around $B$, also have a minimal spread around $B$. In a very
confusing turn of phrase, the estimator that, in a class of estimator, has
minimum variance is the \emph{best} estimator. You're most likely to encounter
this terminology in the specific case of linear regression models, for which
it turns out that ordinary least squares gives the \emph{best linear} unbiased
estimator, where ``best'' we just defined and ``linear'' means that the
estimator is a linear combination of the data.

It's worth noting that our $\hat{B}_\text{unbiased}$ is a linear combination
of the $X_i$: it has coefficient zero for all the $X_i$ except for the one
that takes on the biggest value, which we give coefficient 1. Beyond that,
though, I don't think it's worth digging into the variance of
$\hat{B}_\text{unbiased}$. I do think it's worth understanding that different
estimators can vary in their precision.

\subsection{Maximum likelihood estimators}

In this simple case, it was pretty easy to cook up a good estimator. There was
a very short chain of logic between ``want maximum of true distribution'' to
``look at maximum of observed values''. It's usually not this easy. To revisit
the question of regression, if I have a bunch of data points, what's the a
good way to make a slope out of them? It's not \textit{a priori} obvious.

There's a convenient methodology for making sensible estimators, called
\emph{maximum likelihood} estimators. \emph{Likelihood} is a confusing word:
in common speech, ``likelihood'' and ``probability'' are synonyms. In
statistics jargon, they mean similar things, with slightly different emphases.
A ``probability'' function typically asks about the probability of different
outcomes given some fixed parameters. A ``likelihood'' functions asks about
the probability (or probability density) of some fixed outcome while varying
the parameters. Imagine a probability density function $f_X$ for some data
value $x$ and some parameter $\theta$. We say that $f_X(x; \theta)$ is the
probability function and $L(\theta; x) \defeq f_X(x; \theta)$ is the
likelihood function. We say ``maximum likelihood'' rather than ``maximum
probability'' as a way of emphasizing that the maximization will be done for
fixed data points over varying parameters.

For our example, the maximum likelihood approach says: as an estimator, pick
the value such that, if it were the true value, would maximize the probability
density of the observed data. That is:
\begin{equation}
\hat{B}_\text{ML} \defeq \max_b \prod_i f_X(x_i; b)
\end{equation}
We can use \eqref{eq:uniform_pdf} to compute the probability:
\begin{align*}
\hat{B}_\text{ML} = \max_b \begin{cases}
0 &\text{if any $x_i > b$} \\
\prod_i (x_i/b) &\text{otherwise}
\end{cases}
\end{align*}
The first case means that we can overshoot at all, and the second says that
the likelihood decreases if we decrease $b$. The solution is that
$\hat{B}_\text{ML} = \max_i X_i$, which was our first good guess.

\textbf{German tank problem} as an exercise.

\section{Variance}\label{variance}

One estimator is used so often that it's often not even explained as an
estimator, or even called an estimator, which is very confusing. Imagine
if someone said that the definition of a maximum of a set of points was
\(\tfrac{n+1}{n}\) times the biggest value. Crazy, right? If you agree,
then you're well prepared.

The definition of sample variance is the average square deviation from
the mean: \[
s^2 \equiv \frac{1}{n} \sum_i (x_i - \overline{x})^2,
\] where \(\overline{x}\) is just the normal arithmetic mean. Because
we're used to estimators, it's a nice question to ask, is this a
``good'' estimator of the true variance \(\sigma^2\)?

\textbf{Some math} will show that
\(\mathbb{E}[s^2] = \tfrac{n-1}{n} \sigma^2\), that is, that the sample
variance is a biased (underestimating) estimator for the true variance.
Happily, it requires merely multiplying by \(\tfrac{n}{n-1}\) (called
``Bessel's correction''), so that the unbiased estimator is: \[
\hat{\sigma^2} = \frac{1}{n-1} \sum_i (x_i - \overline{x})^2.
\]

This estimator is so commonly used that it is often simply referred to
as ``sample variance'', leaving students to wonder why the variance is
one thing for ``true distributions'' and another thing for samples. To
be clear, the sum of square deviations divided by \(n-1\) is an unbiased
\emph{estimator} for the population variance.

Unfortunately, the words ``sample variance'' are used to refer to the
actual sample variance (sum of square deviations divided by \(n\)) as
well as to this estimator. There's often no way to tell. I'm sorry. You
should probably guess that people mean the \(n-1\) denominator
\emph{estimator}, and you should probably guess that most people won't
know the difference.\footnote{Comfortingly, as \(n\) grows, the
  difference between the sample variance and the ``sample variance''
  becomes negligible.}

\section{The mean}


\section{Summary statistics}\label{summary-statistics}

It is so critical to look
at the distribution of your data \emph{before} selecting summary
statistics.

\subsection{The (arithmetic) mean}\label{the-arithmetic-mean}

Everyone knows the average: take your numbers, sum them, and divide by
the number of numbers.

Why is this a good idea? Early thinkers in statistics---who thought
about it before it had a name---were curious about the same thing. The
arithmetic mean was computationally simple (i.e., you could do it will
quill and parchment, OK, pen and paper) and it seemed to ``work'', but
why?

It turns out that the arithmetic mean has some nice properties for, you
guessed it, the normal distribution. In fact, we'll show that it is a
certain kind of ``best way'' to guess the true, population mean from our
sample mean. (It's the maximum likelihood estimator.)

\subsection{The ``average''}

In typical speech, we say something like, ``The average family has two
children'', by which we mean that a typical family has two children.
This is confusing because we also use the word ``average'' to refer to
the arithmetic mean. These two things are similar only in special cases,
including (you guessed it!) the normal distribution.

The arithmetic mean of number of children in families is somewhere
between 2 and 3 (let's say 2.5), but it's clear that no ``average
family'' has 2.5 children. Similarly, it's confusing that the mean
salary in the US is whatever, even thought that is above what whatever
percent of people get paid.

\subsection{Mode, median, and
percentiles}\label{mode-median-and-percentiles}

Typical can mean ``common'', so we might say, in a sense, that an
``average'' family has two children. Of the number of children you can
have (zero, one, two, etc.), two is the most common, so that's
``average''. Technically, the most common value is the \emph{mode}. No
one really talks about the mode except in stats textbooks.

A more interesting number is the middle point. We might say that an
average American makes whatever because half of people make more and
half of people make less. This is the \emph{median} (from Latin
\emph{medius}, meaning ``middle'').

It even feels natural to combine the median and mode ideas (have our
median cake and eat it a la mode?). We want a sense of what are
``middling'' numbers, and we want a sense of what are ``common''
numbers. So you might ask what range is covered by the most middle half
of numbers.

Children are often measured against a growth chart, which shows
\emph{percentiles}: if you are in the 25th percentile, you are taller
than 50\% of people. (The median is just another name for the 50th
percentile.) The range between the 25th and 75th percentiles, which
covers the middle half of people, is the \emph{interquartile range},
because the 25th, 50th, and 75th percentiles are also called the
\emph{quartiles}, because they divide the numbers into four quarters
(bottom quarter, bottom-middle, top-middle, and top).

\subsection{Mean, median, and mode are usually
different}\label{mean-median-and-mode-are-usually-different}

Roll a dice many times. What are the mean, median, and mode? Mean is
easy: each number is equally likely to come up, so we just take the mean
of 1, 2, 3, 4, 5, 6, which is 3.5. The median is a little tricky here:
it's between 3 and 4, and when we hit this situation we normally define
the median as the arithmetic mean of the two middle values. So the
median is also 3.5 here, but only because of some convention. The mode
is also confusing, since all numbers are equally likely, which means
that they are all equally the mode.

For the normal distribution, the mean, median, and mode are all the
same. So it's only in this very potentially unusual case that our
intuition is correct that that the ``average'' value, in the sense of
something common (i.e., the mode), is the same as the ``average'' value,
in the sense of something middling (i.e., the median), is the same as
the arithmetic mean.

Historical box for Quetelet: the idea of an ``average person'' is
directly traceable to a particular proto-statistician, who is remarkable
for having believed that almost \emph{everything} was normally
distributed. At the time, they only had very rudimentary methods for
determining if something was normally distributed (basically, eyeballing
it), and there were very appealing aesthetic/philosophical reasons to
believe that almost everything was normally distributed, so he believed
that too.
