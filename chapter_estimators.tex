%!TEX root=main

\chapter{Estimators}

\section{A statistic is a function of data}

In common speech, we use the terms like expected value, variance, covariance, and
correlation in reference to data, that is, to sets of fixed numbers, rather than
to mathematical objects like random variables. In this chapter, we will bridge the
gap between the probability theory we have considered so far and the practice of
statistics.

The term ``statistics'' has two meanings. As a singular noun, a \emph{statistic},
is a function that maps data, that is, an ordered list of numbers, to a real
number. We use $t$ as a stand-in to mean ``any old statistic'':
\begin{equation*}
    t : \mathbb{R}^n \to \mathbb{R}
\end{equation*}
To study the properties of a statistic, we will define a random variable that is
analogous to that statistic. A random variable that is analogous to a statistic is
called an \emph{estimator}, often written $T$.

For example, the arithmetic mean maps a list of numbers $x_i$ to a single number, and
so is a statistic, which I will write as $t$, such that $t \defeq (1/n) \sum_i x_i$.
There is an analogous estimator $T \defeq t(X) = (1/n) \sum_i X_i$. As I emphasized in the
previous chapter, although the notation in these two equations is almost identical,
just replacing some lowercase letters with uppercase ones, it is critical to remember
that adding random variables is not the same thing as adding numbers. The function
$t$ maps numbers to a number, so $t(X)$ is, strictly speaking, nonsense. But in the
same way that we use the plus sign when adding numbers, like in $x + y$, and for
``adding'' random variables, like in $X + Y$, we use $t$ to refer to an operation on
numbers as well as to the analogous operation on random variables.

The word ``statistics'' ending in \textit{s} could either be the plural of statistic,
that is, many functions of the data, or it can refer to the field of study of
estimators. Thus it is syntactically and semantically correct to say ``statistics
\textit{is} fun''.

Up to this point, we have discussed random variables in a deductive way:
given the distribution of the random variable, you ask
what its expected value or variance is. Estimators are the starting point of
statistics (the field of study), which goes the opposite way: given some data
points, what can we say about the distribution of the random variables that gave
rise to that data?

\section{Parameters, populations, and samples}

Some people refer to statistics (the functions) as \emph{sample statistics} to
emphasize that they are functions of a sample of data that was drawn from some
larger \emph{population}, which has some fixed an unknowable \emph{parameters}
that describe it. For example, if you draw many data points $x_i$ from a distribution
and compute the mean of the drawn data points, you do not expect that the
\emph{sample} mean that you compute will be exactly equal to the true
\emph{population} mean.

In mathematical terms, we say that a random variable $X$ has some expected value
$\mathbb{E}[X]$ that is fixed. A random variable is a function, and the expected
value is another function that links the random variable with a single number.
There is nothing ``random'' about this so far. The analogy to the random sample
are iid random variables $X_i$, so the sample mean is analogous to the estimator
$t(X)$.

Ideally, the estimator $t(X)$ will actually reflect the parameter $\mathbb{E}[X]$
that we wanted to learn about. The rest of this section will discuss what makes
a ``good'' estimator.

\section{A useful example: the ``German tank'' problem}

To examine the properties of estimators and what makes an estimator a ``good'' one,
I will use an example that is mathematically tractable but just unfamiliar enough
to make you think.

During World War II, it was important to the Allies to know how many tanks
Germany was producing. The traditional approach was to use spies and aerial
reconnaissance. The new approach was to use statistics on serial numbers,
which turned out to be much more accurate.

A serial number is a unique number written on a manufactured part. Serial numbers
are usually assigned in sequential order, so that older parts have lower
serial numbers and newer parts have higher numbers. German tanks had serial
numbers. When
the Allies captured German tanks, they took note of those numbers, which gave
them a clue about how may thanks there were. For example, if you captured three
tanks and found serial numbers 1, 3, and 5, you know there are at least 5 tanks
total, and there probably aren't more than 10 or so. If you find serial numbers
100, 300, and 500, then you know there are at least 500 tanks, and there are
probably more like 1,000.

The Allies had a fairly complex problem, because they wanted to estimate the
rate of production, and they had many serial numbers on many different parts
of the tank, some of which were not
exactly sequential. Let's instead consider an abstracted, simplified version of
this problem. Say you drew $n$ numbers from \emph{uniform distribution} ranging
from $A$ to some unknown upper limit $B$. You want to estimate $B$, which is
analogous to the number of German tanks out there.

Mathematically, we are interested in iid random variables $X$ that follow the
uniform distribution:
\begin{gather*}
    f_X(x) = \frac{1}{B-A} \text{ for } A \leq x \leq B \\
    F_X(x) = \frac{x-A}{B-A} \text{ for } A \leq x \leq B
\end{gather*}
For simplicity, let's say that $A=0$ so that $f_X(x) = 1/B$ and $f_X(x) = x/B$
for $0 \leq x \leq B$.

Our challenge is to create a statistic, a function of the data, that estimates
the parameter $B$ and then examine the mathematical properties of the corresponding estimator. It's typical to denote estimators with a
``hat'', so we will write our estimators like $\hat{B}$. It's also typical to denote
statistics with lower-case letters reminiscent of the true value, so we'll
use names like $b$. To be clear, $b$ is a function of numbers, although we also
write $\hat{B} = b(X)$, where now ``$b$'' means some manipulation of a set of iid
random variables to create a new random variable.

\subsection{Consistent estimators}

Let me start with what will seem like a very inane choice of statistic.
Maybe my favorite number is 3, so I will say that, regardless of what data I
collect, I will just always guess that $B$ is 3:
\begin{equation*}
    b(x_1, \ldots, x_n) \defeq 3.
\end{equation*}
The corresponding estimator $\hat{B}$ is very simple. It takes on the
value 3 with 100\% probability:
\begin{equation*}
    f_{\hat{B}}(x) = \begin{cases}
        1 &\text{if $x = 3$} \\
        0 &\text{otherwise}
    \end{cases}
\end{equation*}

This is clearly a bad estimator. No matter how much data I accumulate, my
estimate doesn't improve. I will only be ``right'' if $B$, by some miracle,
happens to be exactly $3$. By contrast, my expectation would be that, in
the limit of collecting a lot of data, the statistic $b$ would be almost
guaranteed to be very close to $B$. This requirement is called \emph{consistency}.

Mathematically, I want the probability that $\hat{B}$ takes on a value far from $B$ to get close to zero as I collect more and more data. This requires defining what a ``limit'' is for a series of random variables. For a sequence of numbers, a limit means that, for any \textit{a priori}, fixed threshold $\varepsilon > 0$, there is some integer $n$ such that every value in the sequence after the $n$-th one is within $\varepsilon$ of the true value:
\begin{equation*}
    \lim_{n\to\infty} a_n = L \text{ means that for any $\varepsilon > 0$ there exists some $n$ such that } |a_i - L| < \varepsilon \text{ for all $i \geq n$}.
\end{equation*}
For example, given the geometric series $1, \tfrac{1}{2}, \tfrac{1}{4}, \ldots$,
if you pick the threshold $\varepsilon = \tfrac{1}{100}$, I can pick $n=7$, since the terms in the series are equal to $\tfrac{1}{2^n}$, and $\tfrac{1}{2^7} = \tfrac{1}{128}$. All subsequent terms in the series will be even closer to zero than that term.

In probability, if you give me $\varepsilon$, in most cases I cannot pick an
$n$ such that the estimator maps all values outside that threshold to zero probability.
Casually speaking, even after a trillion data points, there is no guarantee that
a series of random data points won't stray from their limit.
Instead, in probability, we say that a sequence of random variables $X_n$
\emph{converges} toward a number $a$ if
\begin{equation}
\lim_{n \to \infty} \prob{|X_n - a| > \varepsilon} = 0,
\end{equation}
that is, if the probability that $X_n$
takes on a value more than $\varepsilon$ away from $a$ approaches zero as $n$
increases. An estimator is \emph{consistent} if the series of random variables
corresponding to collecting more and more data converge toward the true value.

In our example, $\hat{B}_1$ corresponds to the estimator when we only collect
1 data point, $\hat{B}_2$ when we collect 2, and so on. In the dumb example
where $\hat{B}$ only takes on the value 3, the $\hat{B}_n$ converge to 3, but not
to $B$, except in the unlikely case that $B$ just happens to be 3.

For our second guess, let's use a more reasonable statistic. Say that $b$
is the maximum of whatever data points we collected:
\begin{equation*}
    b(x_1, \ldots, x_n) = \max_i x_i.
\end{equation*}
I know I can write out an analogous equation to define the estimator:
\begin{equation*}
    \hat{B} = \max_i X_i.
\end{equation*}
But what does it mean to take the maximum of two random variables? Recall that,
we computed the distribution of a sum of two random variables $Z = X + Y$ as
$f_Z(z) = \int f_X(x) f_Y(z-x) \,dx$. For a maximum, it is easier to express the
new variable's distribution using its cdf. To say $Z = \max[X, Y]$ means that,
if $X$ and $Y$ are independent,
\begin{align*}
    F_Z(z) &= \prob{Z \leq z} \\
    &= \prob{(X \leq x) \cap (Y \leq y)} \\
    &= \prob{X \leq x} \, \prob{Y \leq y} \\
    &= F_X(x) F_Y(y)
\end{align*}
So if $F_X(x) = x/B$, then $F_{\hat{B}_n} = (x/B)^n$. Note that $\hat{B}_n$ can
only take on values between $0$ and $B$, so if it diverges from $B$, it will do
so by falling short:
\begin{align*}
    \prob{|\hat{B}_n - B| > \varepsilon}
    &= \prob{\hat{B}_n \leq B - \varepsilon} \\
    &= F_{\hat{B}_n}(B - \varepsilon) \\
    &= \left(\frac{B - \varepsilon}{B}\right)^n \\
    &= \left(1 - \frac{\varepsilon}{B}\right)^n.
\end{align*}
For $\varepsilon > 0$, this limit of this number, as $n \to \infty$, is zero,
and thus $\hat{B}_n$ converges to $B$, so $\hat{B}$ is a consistent estimator.

\subsection{Unbiased estimators}

It is a nice thing that our estimator $\hat{B}$ is consistent: as we get more
and more data, the statistic $b$ will end up closer and closer to the true value
$B$. It is disappointing, though, that $b$ is always an underestimate. If the
data points $x_i$ are randomly distributed between $0$ and $B$, and we take
$b = \max_i x_i$, it is necessarily the case that $b < B$. Because $b$ is not
``centered around'' $B$, we say that $\hat{B}$ is a \emph{biased} estimator.

Consider the extreme case where $n=1$: we draw only one data point $x$, which is
somewhere between $0$ and $B$. Intuitively, I might expect $x$ to be about
halfway between $0$ and $B$, that is, that $x \approx \tfrac{1}{2}B$, so that
$b$ underestimates $B$ by a factor of $\tfrac{1}{2}$. And if
$n=2$, then I might expect those points, roughly speaking, to land somewhere
like $\tfrac{1}{3}B$ and $\tfrac{2}{3}B$, so that $b$ underestimates $B$ by
a factor of about $\tfrac{1}{3}$. These factors appear to shrink as $n$ increases,
which makes sense, since $\hat{B}$ is a consistent estimator. But can we do
something to figure out, mathematically, what that factor should be?

We do this by examining the expected value of the estimator:
\begin{equation*}
    \expect{\hat{B}} = \int_0^B x \, f_{\hat{B}}(x) \, dx.
\end{equation*}
We said above that $F_{\hat{B}} = (x/B)^n$, from which it follows that
\begin{equation*}
    f_{\hat{B}}(x) = \frac{d}{dx} F_{\hat{B}} = \frac{n}{B^n} x^{n-1}.
\end{equation*}
Plugging this definition of $f_{\hat{B}}$ into the integral gives
\begin{equation*}
    \expect{\hat{B}} = \int_0^B \frac{n}{B^n} x^n \, dx = \frac{n}{n+1} B.
\end{equation*}
This result accords exactly with our intuition above: for $n=1$, the expected
value of $\hat{B}$ is $\tfrac{1}{2}B$. For $n=2$, it is $\tfrac{2}{3}B$, and
so forth. Based on this finding, we can define a new estimator:
\begin{equation}
\hat{B}_\mathrm{unbiased} \defeq \frac{n+1}{n} \max_i X_i.
\end{equation}
This ensures that our estimates for $B$ are ``centered'' around $B$, because
$\expect{\hat{B}_\mathrm{unbiased}} = B$. More generally, we say that an estimator
is \emph{unbiased} if its expected value is equal to the parameter it is estimating.

Note the subtlety in what we did: without actually knowing what $B$ is, we found
a way to adjust the way that we estimate $B$ to ensure that we come up with a more
accurate estimate. I was mystified in introductory statistics that, while we computed
variance as $\tfrac{1}{n} \sum_i (x_i - \mu)^2$, we computed \emph{sample variance}
as $\tfrac{1}{n-1} \sum_i (x_i - \mu)^2$. The standard explanation has something to
do with ``degrees of freedom'', which I never understood. There is, I think, a
more straightforward explanation, which you can now understand, which is that the
$n-1$ is there in the sample variance, instead of $n$, because it corrects for a
bias.

\subsection{Efficient estimators}

Consistency ensures that, in the limit of infinite data, our estimate approaches
the true value. How can we be sure that we are getting the best estimate for our
data, given the fact that we can't collect infinite data? This question relates
to the concept of \emph{efficiency}. One unbiased estimator $\hat{X}_1$ is 
more \emph{efficient} than another
unbiased estimator $\hat{X}_2$ if it has lower variance:
\begin{equation*}
    \var{\hat{X}_1} < \var{\hat{X}_2}.
\end{equation*}
As we will see, confidence intervals are related to the variance of an estimator,
so a more efficient estimator means that we can get more narrow confidence intervals
for the same amount of data, which is clearly a desirable thing. I don't want to
appear more ignorant than I have to be, just because I picked a poor estimator!

It turns out that there is a theoretical lower limit to the variance of estimators
called the \emph{Cram\'{e}r-Rao bound}, and many estimators actually hit this limit.
Therefore, rather than saying that one estimator is \emph{more} efficient than
another, we usually just say that an estimator is ``efficient'', meaning that it
hits the lower bound on variance and is maximally efficient.

The math behind efficiency is somewhat complex, so I will simply tell you that
a whole class of estimators, \emph{maximum likelihood} estimators, are efficient.

\subsection{Maximum likelihood estimators}

The German tank problem has a useful toy example, but it's hard to imagine deriving
estimators by hand for the kind of complex data analysis that most scientists do.
Fortunately, modern computing means that we can address all sorts of complex data
using a \emph{maximum likelihood} approach. In a maximum likelihood approach, you
specify a theoretical model for how your data are generated, and then you use optimization to estimate values for the parameters that best ``fit'' the data.

The word \emph{likelihood} is confusing because, in common speech, ``probability''
and ``likelihood'' are synonymous. In statistics jargon, the likelihood function
is actually related to the probability density function, only it emphasizes the
parameters over the data. Say we have some data $x$ that was drawn from a probability 
distribution with density function $f_X$. We often think about families of probability
distributions and name them by their parameters. For example, we say that a normal
distribution is defined by two parameters, its mean and its variance. So say that
the density function $f_X$ changes depending on some parameter (or parameters) $\theta$.
We write the density function as $f_X(x; \theta)$ to emphasize that, while the
parameters $\theta$ are relevant, we think of them as fixed. By contrast, we write
the likelihood function $L$ as $L(\theta; x) \defeq f_X(x; \theta)$. For likelihood,
we consider the data fixed and vary the parameters.

To see how this plays out, let's return to the German tank problem. Given some
data points $x_i$, what is the probability density of that data, as we
vary the parameter $B$? Earlier we saw that $f_X(x) = 1/B$ if $0 \leq x \leq B$.
Clearly $f_X(x) = 0$ if $x > B$: the chance that any data point is over the maximum
$B$ is zero. Thus, the joint probability density for all the data is:
\begin{equation*}
    f_X(x) = \begin{cases}
        (1/B) \text{ if $0 \leq x \leq B$} \\
        0 \text{ otherwise}
    \end{cases}
\end{equation*}

\vspace{1in}


the two terms are related
but subtly different. Both the probability and likelihood functions take data
and parameters as input and return a probability or probability density as output.

The probability function asks, given some parameters, how
likely are the data you observed? (Or, more precisely, if you have continuous data,
what is the value of the probability density function for the data you observed?)
The likelihood function asks the reverse: given some data, what parameter values


how you think your
data are generated

In the simple toy example of the German tank problem, it was pretty easy to
cook up a good estimator. There was a very short chain of logic between ``want
maximum $B$ of true distribution'' to ``look at maximum of observed values''.
It's usually not this easy.

For example, imagine you had never heard of least squares regression, but you
have some pairs of points $(x_i, y_i)$ that you thought had some linear
relationship $y_i = m x_i + b + \varepsilon_i$, where the $\varepsilon_i$ are
some random error values. What's a good estimator $\hat{m}$ to estimate the
slope $m$?

\emph{Maximum likelihood} estimator is an algorithm for creating sensible
estimators that doesn't require any reasoning about the problem. You put in
the distributions of all the relevant variables, and it spits out an
estimator. It was fun to think deeply about the German tank problem and
derive a consistent, unbiased estimator, but it's very rare that we'll be
able to pick up a pencil and paper and calculate out an answer like that.

\emph{Likelihood} in ``maximum likelihood'' is a confusing word. In common
speech, ``likelihood'' and ``probability'' are synonyms. In statistics jargon,
they mean related but distinct things. A ``probability'' density function asks
about the probability of different outcomes given some fixed parameters. A
``likelihood'' functions asks about the probability density of some fixed
outcome while varying the parameters. Imagine a probability density function
$f_X$ for some data value $x$ and some parameter $\theta$. We say that $f_X(x;
\theta)$, where $\theta$ is mostly fixed, is the probability function. We say
$L(\theta; x) \defeq f_X(x; \theta)$, where $\theta$ varies and $x$ is fixed,
is the likelihood function. We say ``maximum likelihood'' rather than
``maximum probability'' as a way of emphasizing that the data points are
fixed, and we're finding the parameters that best explain them.

The maximum likelihood (ML) approach says: as an estimator, pick the value
such that, if it were the true value, would maximize the probability density
of the observed data. For the German tank problem, this would read:
\begin{equation}
\hat{B}_\text{ML} \defeq \max_{B'} \prod_i f_X(x_i; B')
\end{equation}
Note that $B$ is still some fixed, true value. We're fiddling with $B'$, a
guess at $B$. When we find the $B'$ that best explains the data, that's our
estimator $\hat{B}_\mathrm{ML}$.

For any particular $B'$, the probability density for an $x$ to fall between 0
and $B'$ is $1/B'$. The probability for $x$ to fall above $B'$ is zero, since
it's impossible for a data point to fall above the maximum of its own
distribution. Thus, the probability the density value for all the $n$ observed
data points is 0 if any of the $x_i$ fall above $B'$ and $(1/B')^n$ if they all
fall below $B'$:
\begin{align*}
\hat{B}_\text{ML} = \max_{B'} \begin{cases}
0 &\text{if any $x_i > B'$} \\
(1 / B')^n &\text{otherwise}
\end{cases}
\end{align*}
The first case means that we can't overshoot at all, so $\hat{B}_\text{ML}$
cannot be smaller than the maximum data point. The second case shows that,
as $B'$ is increased, the likelihood of the observed data decreases. The maximum
likelihood, then, is just the value of the maximum observed data point, which
was the consistent estimator we guessed above.

In fact, all maximum likelihood estimators are consistent, and they are
maximally efficient in the limit of large sample size. (It can be that other
estimators are more efficient that the ML estimator for a finite sample size.)
Maximum likelihood estimators are not guaranteed to be unbiased. Indeed, in this
example, the maximum of the data is a biased estimator.

Although ML estimators are not perfect, it's nice that they are consistent and
have some guarantees on their efficiency. It's also really nice that they
arise from an algorithmic approach. If you can write down the likelihood
function, you can maximize it analytically, that is, exactly, by hand, like we
did in this example. If you can't write down the likelihood function but you
can sample from the distribution and compute the likelihood for every sample
you draw, then you can still do a maximum likelihood estimate. The fact that
you can take this algorithmic, computational approach means that maximum
likelihood estimation is used a lot in complex models, like we'll see later.
