%!TEX root=main

\chapter{Parameters and estimators}

\section{Orientation and definitions}

Up to this point, random variables and their values have occurred in a
deductive kind of way: you know what the distribution of the random variable
is, and then you ask what its expected value or variance is. Statistics is
about going in the opposite, inferential direction. Given that I have some data---that is,
realizations of the values of some random variables---what can I say about the
distributions that those values were drawn from?

This characterization embraces a common typology of statistics into
\emph{descriptive} statistics and \emph{inferential} statistics. Inferential
statistics, which includes hypothesis testing, certainly has some more
philosophical and technical hang-ups, but it's fair, at this point, to say
that both descriptive and inferential statistics hinge on how we can use the
data we have to say something about the populations that we drew that data
from.

The jargon for this deductive/inductive difference is that mathematical
probability considers \emph{statistical parameters}, which are functions of
the distributions that give numbers, and \emph{estimators}, which are
functions of the data that approximate something about the true distribution.
The expected value $\mathbb{E}$ and variance $\mathbb{V}$ that we learned
about earlier are two of this kinds of function. You can also think of
statistical parameters are an ``index'' of the distribution. We say ``a normal
distribution with mean $\mu$ and variance $\sigma^2$'' because, having said
those two things, we've perfectly specified what distribution we're talking
about. This chapter is about the second thing, estimators, which, as their
name implies, are functions of the data that estimate something about the true
distribution.

Up to this point, I've used the word ``distribution'' in the mathematical
probability sense: it's the cdf $F_X$ or the pmf/pdf $f_X$. After this point,
it will become crucial to draw a distinction between these mathematical
probability objects on the one hand and, on the other hand, the mass of data
that we get from an experiment.

\subsection{Distributions and samples}

what are these two things. definition of statistics with respect to samples.
what ``sampling'' from a distribution means with respect to our iid variables,
etc.

also need some more discussion earlier to say that thinking about $\Omega$ is
really confusing. What sample space would give rise to a normally-distributed
variable, for example? And where do we draw the liens around the universe we're
interested in? 

standard deviation vs. standard error in the mean

\section{Properties of estimators}

So if there are more and more $X_i$ with iid $F_X$, then the idea is that the estimator of the $\hat{\theta}(\vec{x})$ should approach $\theta(F_X)$.

There's a ``sample'' probability distribution $\hat{f}_X$. It's hard to say whether this is a maximum likelihood estimator or something. What does ``sample'' mean? Like ``sample mean''?

\section{Motivating example: uniform
distribution}\label{motivating-example-uniform-distribution}

Consider a uniform distribution from zero up to some number \(B\).
What's the maximum of this distribution? Well, if you know it, it's
easy: it's just \(B\). It's also easy if you can sample a zillion times
from the distribution: you are very likely at some point to get a value
very close to \(B\), so you can just use that as your estimate.

But what if you only drew one point? Then clearly the maximum of the
drawn values, just that point, is a pretty bad estimate for the upper
limit. If you draw 3 points, then the maximum is a little better, but
you can feel pretty sure that estimating the \emph{population} maximum
using your \emph{sample} maximum is biased: you're always going to
underestimate the population maximum.

When we use our data to make a guess about a population parameter,
that's an \emph{estimator}. (Good name.) We say that an estimator that
we expect to not be ``systematically''\footnote{Why the scare quotes?
  Like in all frequentist statistics, we're stuck with assuming that we
  know the truth, finding the best answer knowing the full truth, and
  then pretending we didn't know the truth in the first place. So
  ``systematically'' is going to mean: assume you know the true value,
  evaluate the behavior of the estimator knowing the truth, and then
  forget you know the truth.} above or below the true value is
\emph{unbiased}. Estimators are typically written with a hat (because
you use a cone hat to communicate with the mothership?), so \(\hat{B}\)
is an estimator for \(B\). Of course, there isn't just one estimator for
a value. I know plenty of people who are perfectly happy to guess that 3
is the right answer in statistics, so for them we might write
\(\hat{B} = 3\). This is clearly a bad estimator, but what does ``bad''
mean?

To answer that, let's consider some of the properties of different
estimators. To keep the different estimators straight, let's put
subscripts on them. I'll call the clearly bad estimator
\(\hat{B}_\mathrm{bad} = 3\) and I'll call our naive estimator \[
\hat{B}_1 = \max_i X_i,
\] where I called it ``1'' because it was our first guess. I also called
the variables we're drawing from the uniform distribution as \(X_i\).

\subsubsection{Consistent estimators}\label{consistent-estimators}

One reason that \(\hat{B}_\mathrm{bad}\) seems bad is that it doesn't
change. It won't ever get more ``right'', no matter how much data we
collect. On the other hand, \(\hat{B}_1\) will get better. In fact, the
more data we take, the closer we expect \(\hat{B}_1\) to get to the true
maximum.

Estimators whose expected value approach the true value as the number of
points approaches infinity is called \emph{consistent}. Call \(M\) the
true value and \(n\) the number of points we drew. Then \(\hat{B}_1\) is
consistent because \[
\lim_{n \to \infty} \hat{B}_1 = M.
\] In other words, if you specify an error threshold (e.g., you want an
estimate within 10\% of the true value), then I can tell you what \(n\)
you need to pick in order to get an estimate within that threshold (with
some finite probability). In contrast, with \(\hat{B}_\mathrm{bad}\),
there's no \(n\) you can pick that will get you arbitrarily close to the
true value.

\subsubsection{Unbiased estimators}\label{unbiased-estimators}

Clearly, though, \(\hat{B}_1\) is not perfect. No matter how many \(n\)
we draw, \(\hat{B}_1\) will always be less than \(M\). We therefore say
that \(\hat{B}_1\) is \emph{biased}. Mathematically, we might write
\(\mathbb{E}[\hat{B}_1] < B\). Because it feels natural to expect that
the expected value of your estimator will be the true value, an
estimator \(\hat{B}\) for which \(\mathbb{E}[\hat{B}] = B\) is called
\emph{unbiased}.\footnote{Another standard for ``good'' estimators is
  the \emph{best linear unbiased} estimator (BLUE). ``Linear'' means
  that the estimator is a linear function of the data. (You could maybe
  imagine that there is some wacky, convoluted function that's a better
  estimator, but we stick with linear.) The final qualifier, ``best'',
  means that the variance of the estimator is the minimum of all other
  linear unbiased estimators. Here ``variance of the estimator'' is
  computed in the same sense that the expected value of the estimator is
  computed for determining bias. In other words, ``best unbiased'' means
  its centered on the true value with the smallest variance possible.}

Let's try to salvage our naive estimator \(\hat{B}_1\) and make an
unbiased estimator \(\hat{B}_\mathrm{ub}\) instead.

Where to start? It's likely that there are many unbiased operators, so
we need to pick the form of the operator we want to look for. It seems
likely that all our observed \(X_i\) only have two interesting things
about them: their maximum, and how many of them there are. For example,
if my first draw was \(1.0\), then I don't learn that much if I draw
\(0.1\) the next five times, because I know that \(M > 1.0\), and,
because it's a uniform distribution, I was just as likely to draw
\(0.1\) as \(1.0\). If I draw many values below \(1.0\), I only learn
that the maximum is probably not far above \(1.0\), which is already
encoded in the number of points \(n\).

So I want \(\hat{B}_\mathrm{ub}\) to be some function of \(n\), which is
fixed, and \(M\), which is a random variable{[}\^{}wtf{]}, such that
\(\mathbb{E}[\hat{B}_\mathrm{ub}] = M\). That will require knowing the
distribution of \(M\).

\textbf{swo} I mixed a lot of this up. I called \(B\) the true value,
which got confusing. I should just use \(M\) and \(\hat{M}\).

The cumulative distribution function, the probability that the observed
maximum is less than some \(m\), is just the probability that there is
no point that is above \(m\): \[
F_M(m) = \mathbb{P}[M \leq m] = \mathbb{P}[X_i \leq m \text{ for all } i] = \left( \frac{m}{B} \right)^n.
\] Then we compute the probability density function: \[
f_M(m) = \frac{d}{dm} F_M(m) = \frac{n}{B} \left( \frac{m}{B} \right)^{n-1},
\] from which we compute the expected value of \(M\): \[
\mathbb{E}[M] = \int_0^B m \, f_M(m) \, \mathrm{d}m = \frac{n}{n+1} B.
\]

Interesting, the expected value of \(M\) is just a multiple of \(B\).
Consider that if \(n=1\), you just draw one point, the expected value of
the maximum, which is just that value you drew, is \(\tfrac{1}{2} B\).
Makes sense: you're likely to get something right in the middle.

If you draw 2 points, the expected maximum if \(\tfrac{2}{3}B\),
somewhat closer to \(B\). It's easy to see that as \(n\) increases,
\(\mathbb{E}[B]\) approaches \(B\), just as we reasoned about before
doing any math.

Now, by linearity of expectation, I know that
\(\mathbb{E}\left[\tfrac{n+1}{n} M \right] = B\), so lo! I have found an
unbiased estimator: \[
\hat{B}_\mathrm{ub} = \frac{n+1}{n} M.
\]

\subsubsection{Maximum likelihood
estimator}\label{maximum-likelihood-estimator}

It's nice that we found an unbiased estimator, which we think is
``good'' in the sense that we're not systematically off in one direction
or the other. In one way this is comforting, but it isn't foolproof.
Imagine you're working with a bimodal distribution: it tends to give
small numbers or big numbers. Then an unbiased estimator splits the
difference, potentially estimating values that aren't even possible!

For this reason, it's useful to think about \emph{maximum likelihood}
estimators (or MLEs, if you're into that sort of acronym). ``Maximum
likelihood'' means that, given the \emph{observed} value, you find the
\emph{true} value that, if it were true, would be most likely, out of
all the other true values, to give rise to your data.\footnote{Did that
  sound crazy? If so, get excited about the Bayesian section of this
  book. In there we'll show that there's a mathematically precise way to
  show that this kind of thinking---saying that the most likely state of
  nature given our data is the state of nature that, of all states of
  nature, was most likely to produce the data we observed---is coherent
  only under some pretty strong assumptions about the universe.}

In other words, given the observed \(M\), what \(B\) would have given
rise to that \(M\) with the greatest probability? Mathematically we
write \[
\hat{B}_\mathrm{ML} = \max_B \mathbb{P}[M | B],
\] where ``MLE'' stands for ``maximum likelihood''.\footnote{The
  promised Bayesian result is that the \(B\) that maximizes
  \(\mathbb{P}[M | B]\), the probability of the data given the state of
  nature, is also the \(B\) for which \(\mathbb{P}[B | M]\), the
  ``probability of the state of nature'' given the data, is maximum,
  although that's only true under a specific, and pretty
  hard-to-believe, criterion. I put more scare quotes here because, as
  mentioned before, if probability is a frequency, it doesn't make sense
  to talk about the probability of a state of nature.} This should be a
piece of cake, since we already found the probability density function
for \(M\), which is a function of \(B\). It's easy to see, just by
inspection, that the value between zero and \(B\) for which \(m\)
maximum \(f_M(m)\) is just \(B\). In other words,
\(\hat{B}_\mathrm{ML} = M\).

You may have noticed that this is the same estimator that I previous
denigrated as ``naive'' and ``biased''. To be clear, this is the maximum
likelihood estimator: you know that \(\hat{B}_\mathrm{ML}\) can't be any
less than \(M\), since the upper bound must be at least as high as the
biggest data point we observed, and some careful thought shows that
\(\hat{B}_\mathrm{ML}\) can't be any greater than \(M\), since
hypothetically increasing \(B\) above \(M\) just decreases the
probability that you would have observed \(M\) as the maximum
value.\footnote{Imagine hypothetically increasing \(B\) to a zillion
  times \(M\). Clearly, observing that small of an \(M\) is given that
  large of a \(B\) is unlikely, and there's no ``cut point'' so that an
  infinitesimal increase is reasonable but a zillion-fold increase is
  not.}

So this example tells an interesting lesson: it's not the case that an
estimator that is ``good'' in one sense (e.g., unbiased) will be
``good'' in another sense (e.g., maximum likelihood). We'll show that
there's a special case in which that's true. (Can you guess what it is?
No really, try. Or, don't try, and just say the first probability
distribution that comes to mind.)

\textbf{German tank problem} as an exercise.

\section{Variance}\label{variance}

One estimator is used so often that it's often not even explained as an
estimator, or even called an estimator, which is very confusing. Imagine
if someone said that the definition of a maximum of a set of points was
\(\tfrac{n+1}{n}\) times the biggest value. Crazy, right? If you agree,
then you're well prepared.

The definition of sample variance is the average square deviation from
the mean: \[
s^2 \equiv \frac{1}{n} \sum_i (x_i - \overline{x})^2,
\] where \(\overline{x}\) is just the normal arithmetic mean. Because
we're used to estimators, it's a nice question to ask, is this a
``good'' estimator of the true variance \(\sigma^2\)?

\textbf{Some math} will show that
\(\mathbb{E}[s^2] = \tfrac{n-1}{n} \sigma^2\), that is, that the sample
variance is a biased (underestimating) estimator for the true variance.
Happily, it requires merely multiplying by \(\tfrac{n}{n-1}\) (called
``Bessel's correction''), so that the unbiased estimator is: \[
\hat{\sigma^2} = \frac{1}{n-1} \sum_i (x_i - \overline{x})^2.
\]

This estimator is so commonly used that it is often simply referred to
as ``sample variance'', leaving students to wonder why the variance is
one thing for ``true distributions'' and another thing for samples. To
be clear, the sum of square deviations divided by \(n-1\) is an unbiased
\emph{estimator} for the population variance.

Unfortunately, the words ``sample variance'' are used to refer to the
actual sample variance (sum of square deviations divided by \(n\)) as
well as to this estimator. There's often no way to tell. I'm sorry. You
should probably guess that people mean the \(n-1\) denominator
\emph{estimator}, and you should probably guess that most people won't
know the difference.\footnote{Comfortingly, as \(n\) grows, the
  difference between the sample variance and the ``sample variance''
  becomes negligible.}

\section{The mean}


\section{Summary statistics}\label{summary-statistics}

It is so critical to look
at the distribution of your data \emph{before} selecting summary
statistics.

\subsection{The (arithmetic) mean}\label{the-arithmetic-mean}

Everyone knows the average: take your numbers, sum them, and divide by
the number of numbers.

Why is this a good idea? Early thinkers in statistics---who thought
about it before it had a name---were curious about the same thing. The
arithmetic mean was computationally simple (i.e., you could do it will
quill and parchment, OK, pen and paper) and it seemed to ``work'', but
why?

It turns out that the arithmetic mean has some nice properties for, you
guessed it, the normal distribution. In fact, we'll show that it is a
certain kind of ``best way'' to guess the true, population mean from our
sample mean. (It's the maximum likelihood estimator.)

\subsection{The ``average''}

In typical speech, we say something like, ``The average family has two
children'', by which we mean that a typical family has two children.
This is confusing because we also use the word ``average'' to refer to
the arithmetic mean. These two things are similar only in special cases,
including (you guessed it!) the normal distribution.

The arithmetic mean of number of children in families is somewhere
between 2 and 3 (let's say 2.5), but it's clear that no ``average
family'' has 2.5 children. Similarly, it's confusing that the mean
salary in the US is whatever, even thought that is above what whatever
percent of people get paid.

\subsection{Mode, median, and
percentiles}\label{mode-median-and-percentiles}

Typical can mean ``common'', so we might say, in a sense, that an
``average'' family has two children. Of the number of children you can
have (zero, one, two, etc.), two is the most common, so that's
``average''. Technically, the most common value is the \emph{mode}. No
one really talks about the mode except in stats textbooks.

A more interesting number is the middle point. We might say that an
average American makes whatever because half of people make more and
half of people make less. This is the \emph{median} (from Latin
\emph{medius}, meaning ``middle'').

It even feels natural to combine the median and mode ideas (have our
median cake and eat it a la mode?). We want a sense of what are
``middling'' numbers, and we want a sense of what are ``common''
numbers. So you might ask what range is covered by the most middle half
of numbers.

Children are often measured against a growth chart, which shows
\emph{percentiles}: if you are in the 25th percentile, you are taller
than 50\% of people. (The median is just another name for the 50th
percentile.) The range between the 25th and 75th percentiles, which
covers the middle half of people, is the \emph{interquartile range},
because the 25th, 50th, and 75th percentiles are also called the
\emph{quartiles}, because they divide the numbers into four quarters
(bottom quarter, bottom-middle, top-middle, and top).

\subsection{Mean, median, and mode are usually
different}\label{mean-median-and-mode-are-usually-different}

Roll a dice many times. What are the mean, median, and mode? Mean is
easy: each number is equally likely to come up, so we just take the mean
of 1, 2, 3, 4, 5, 6, which is 3.5. The median is a little tricky here:
it's between 3 and 4, and when we hit this situation we normally define
the median as the arithmetic mean of the two middle values. So the
median is also 3.5 here, but only because of some convention. The mode
is also confusing, since all numbers are equally likely, which means
that they are all equally the mode.

For the normal distribution, the mean, median, and mode are all the
same. So it's only in this very potentially unusual case that our
intuition is correct that that the ``average'' value, in the sense of
something common (i.e., the mode), is the same as the ``average'' value,
in the sense of something middling (i.e., the median), is the same as
the arithmetic mean.

Historical box for Quetelet: the idea of an ``average person'' is
directly traceable to a particular proto-statistician, who is remarkable
for having believed that almost \emph{everything} was normally
distributed. At the time, they only had very rudimentary methods for
determining if something was normally distributed (basically, eyeballing
it), and there were very appealing aesthetic/philosophical reasons to
believe that almost everything was normally distributed, so he believed
that too.
