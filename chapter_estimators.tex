%!TEX root=main

% swo: some figure that shows how random variables, their realizations, etc.
% are all related

\chapter{Estimators}

In common practice, we terms like expected value, variance, covariance, and
correlation coefficient to refer to numbers derived from our data. In the
previous section, we discussed these terms, but I emphasized that the objects
we were studying were functions of random variables, which are themselves
functions from outcomes to probabilities.

The bridge between the mathematical objects we examined in the previous section
and the computations we run on our data are \emph{estimators}. Up to this
point, random variables and their values have occurred in a deductive kind of
way: you know what the distribution of the random variable is, and then you ask
what its expected value or variance is. Estimators are the starting point of
statistics, which goes the opposite way: given some data points, what can we
say about the distribution of the random variables that gave rise to that data?

\section{Statistics and estimators}

The concept of estimators is closely linked with statistics, which, in this
sentence, is a plural noun. A \emph{statistic} is a function of the data that
depends only on the data. (Just as ``random variable'' is often loosely used
to refer to a function as well as the value taken on by that function, so
``statistic'' will refer sometimes to the function and sometimes to its
value.)

We will use functions of the data to estimate
the properties of the underlying distribution. For example, we might compute
the arithmetic mean of a bunch of numbers. The arithmetic mean is a statistic,
and it also estimates the expected value of the underlying distribution. True
values of the underlying distribution are sometimes called \emph{parameters},
especially if specifying those values specifies the distribution. For example,
for the normal distribution, the mean and variance are parameters because they
specify the shape of the distribution.

\section{Samples and populations}

Up to this point, I've used the word ``distribution'' in the mathematical
probability sense: it's the cdf $F_X$ or the pmf/pdf $f_X$.
It it now crucial to distinguish between these mathematical
probability objects and, on the other hand, the mass of data
that we get from an experiment, which might also be called the ``distribution''
of the data. (As we'll see later, this confusion of language is not
totally unwarranted, because the mass of data is itself a complicated estimator
of the mathematical distribution.)

The distinction between mathematical distributions and data distributions is
usually made in the jargon of the social sciences: from some
\emph{population}, you draw a \emph{sample}. The sample refers to the numbers
you collected; the population refers to the larger universe of numbers your
sample was drawn from. We use the sample to estimate properties of the
population. For example, if you're interested in the height of men versus
women in the US, then your target populations will be, say, all American males
and all American females. The sample is whatever heights you actually measure.
We then try to make some inference about all Americans based on our sample.

In the heights example, the target population is finite. There are only 300
million Americans. It is therefore theoretically possible to make a list of
the heights of all Americans. In a repeatable experiment, on the other hand,
the target population is infinite: you could theoretically repeat an
experiment infinitely many times, drawing an infinite number of data points.
In most cases, statistics proceeds as if the target population were infinite,
although there are corrections you can make when your sample size starts to
approach the size of the target population.

Many statistical methods also assume that the target population follows some
known distribution. The classic example is that the $t$-test assumes that the
target populations are normally distributed. This can make the language a
little confusing. If you are using the $t$-test to compare the heights of
males and females, is the ``true'' population the actual distribution of
heights among the 300 million Americans, or is the ``true'' population the
normal distribution? In most cases, ``true'' is used to refer to the abstract
mathematical distribution rather than the actual distribution of the target
population.

The important point is that the ``sample'' distribution refers to the data you
collected, while the ``true'' or ``theoretical'' distribution refers to the
theoretical approximation of the target population your data came from.

\subsection{Sources of variation}

In the ``hard'' experimental sciences, this language about ``population'' may
sound weird. If I'm trying to measure the speed of light, I repeat my
experiment many times and get many different values. Those values are the
sample, but what is the population? We think there is a single, true speed of
light, not a distribution of speeds of lights. In this case, the ``population''
refers, somewhat tautologically, to the distribution of the values that would
come from an infinite number of repetitions of our experiment. In other words,
the sample defines the population, not the other way around.

In the social sciences example, the shape of the distribution of measured
values says something about the variation of the values in the target
population. We get a range of measured values because people are different
heights. In the physics example, the shape of the distribution has everything lot to do
with the precision in our experiment. We get a range of measured values
because of all sorts of error that arise the in the process of experimentation.

In biology, the situation is somewhere in between. We think there is true
variation in, say, the physiology of the cells used in an experiment. We
might wish there weren't. It would be easier to do some experiments if all
cells started in exactly the same state and responded exactly the same way to
an experimental condition. The meaning of the range of values we get is then
something of a philosophical one. To what degree do we treat the range of
values as due to error, because we couldn't make all the cells the same, and
to what degree is it an honest report about the heterogeneity of behavior in
some ``target population'' of cells? Multilevel models, which will come up later,
are an attempt to deal with multiple sources of variation.

\section{Properties of estimators}

Estimators are the bridge between mathematical probability and actual data, so
I'll use a motivating example to show how estimators arise from looking at data.

\subsection{The ``German tank'' problem}

During World War II, it was important to the Allies to know how many tanks
Germany was producing. The traditional approach was to use spies and aerial
reconnaissance. The new approach was to use statistics.

A serial number is a number written on a manufactured part to uniquely identify
that part, typically following the sequential order in which the parts were
produced. German tanks had serial numbers on various parts of the tank.  When
the Allies captured German tanks, they took note of those numbers, which gave
them a clue about how may thanks there were. For example, if you captured three
tanks and found serial numbers 1, 3, and 5, you know there are at least 5 tanks
total, and there probably aren't more than 10 or so. If you find serial numbers
100, 300, and 500, then you know there are at least 500 tanks, and there are
probably more like 1,000.

The Allies had a fairly complex problem, because they wanted to estimate the
rate of production, and they had many serial numbers, some of which were not
exactly sequential. Let's instead consider an abstracted, simplified version of
this problem. Say you drew $n$ numbers from \emph{uniform distribution} ranging
from $A$ to some unknown upper limit $B$. A random variable $X$ follows the
uniform distribution if all values between $A$ and $B$ are equally probable:
\begin{gather*}
    f_X(x) = \frac{1}{B-A} \text{ for } A \leq x \leq B \\
    F_X(x) = \frac{x-A}{B-A} \text{ for } A \leq x \leq B
\end{gather*}
For simplicity, let's say that $A=0$ so that $f_X(x) = 1/B$ for $0 \leq x \leq B$.
Given the $n$ drawn values $x_1, \ldots, x_n$, what can we say about $B$?

Our approach is to create a statistic, a function of the data, and examine the
mathematical properties of the corresponding estimator, which is a random
variable. The true value is $B$. It's typical to denote estimators with a
``hat'', so we will write our estimators like $\hat{B}$. It's also typical to denote
statistics with lower-case letters reminiscent of the true value, so we'll
use names like $b$. To be clear, the statistic $b$ is a function of
the $n$ data points that returns some other number:
\begin{equation*}
b : \mathbb{R}^n \to \mathbb{R}.
\end{equation*}
The estimator $\hat{B}$ corresponding to this statistic is a random variable,
a function that maps outcomes to probabilities. The statistic is the thing
we can actually compute using our data; the estimator is the abstract mathematical
thing whose behavior we will explore as a way to understand the suitability
of our choice of statistic.

\subsection{Consistent estimators}

Let me start with what will seem like a very inane choice of statistic.
Say I draw $n$ data points, and then I ignore all of them and just
pick 3:
\begin{equation}
    b(x_1, \ldots, x_n) = 3.
\end{equation}
The corresponding estimator $\hat{B}$ is very simple. It takes on the
value 3 with 100\% probability:
\begin{equation*}
    f_{\hat{B}}(x) = \begin{cases}
        1 &\text{where } x = 3 \\
        0 &\text{otherwise}
    \end{cases}
\end{equation*}

This is clearly a bad estimator. The main problem is that, no matter how much
data I accumulate, my estimate doesn't improve. We would expect that,
in the limit of
collecting a lot of data, the statistic $b$ would be almost guaranteed to be
very close to $B$. More technically, we want the distribution of the estimator
$\hat{B}$ to narrow in around the true value $B$ and $n$ increases. This
property is \emph{consistency}.

To define a \emph{consistent} estimator, we need to define what a ``limit'' is
for a series of random variables. To say that a sequence of numbers has a limit means that, for any threshold difference
$\varepsilon$ you give me, I can give you an integer $n$ such that every value
in the sequence after the $n$-th one is within $\varepsilon$ of the true value.
For example, given the geometric series $1, \tfrac{1}{2}, \tfrac{1}{4}, \ldots$,
if you pick some threshold $\varepsilon$, I can always pick $n$ such that
$\varepsilon > \tfrac{1}{2^n}$.

In probability, if you give me $\varepsilon$, in most cases I cannot pick an
$n$ such that I'm guaranteed that a random variable will take on a value in a
certain range. Even after a trillion data points, there's always the chance, however
remote, of some crazy extreme outlier.

Instead, in probability, we say that a sequence of random variables $Y_n$
\emph{converges} toward a number $a$ if
\begin{equation}
\lim_{n \to \infty} \prob{|Y_n - a| > \varepsilon} = 0,
\end{equation}
that is, if the probability that $Y_n$
takes on a value more than $\varepsilon$ away from $a$ goes to zero as $n$
increases. An estimator is \emph{consistent} if the series of random variables
corresponding to collecting more and more data converge toward the true value.

In our example, $\hat{B}_1$ corresponds to the estimator when we only collect
1 data point, $\hat{B}_2$ when we collect 2, and so on. In the dumb example
where $\hat{B}$ only takes on the value 3, the $\hat{B}_n$ converge to 3, which
is a good thing only in the very unlikely case that the correct answer just so
happens to be exactly 3.

For our second guess, let's use a more reasonable statistic. Say that $b$
is the maximum of whatever data points we collected:
\begin{equation*}
    b(x_1, \ldots, x_n) = \max_i x_i.
\end{equation*}
If $X_i \stackrel{\text{iid}}{\sim} \text{Uniform}(0, B)$, then
\begin{equation*}
    \hat{B} = \max_i X_i.
\end{equation*}
Strictly speaking, this equation shouldn't make a lot of sense to you. The $X_i$
are all identical functions, so what does it mean to take the maximum of them?
This is an intuitive shorthand for the more correct definition of $\hat{B}$,
which is a random variable that, for any outcome $\omega$, takes on the maximum of the
values taken on by each of the $X_i$ for that $\omega$:
\begin{equation*}
    \hat{B}[\omega] = \max_i X_i[\omega].
\end{equation*}

It's fairly intuitive that the corresponding $\hat{B}$ converges toward $B$.
Say you give me some small allowed deivation $\varepsilon > 0$. Then I have to
show that, as $n$ increases, the probability that observed statistic is way
off goes to zero, where ``way off'' means that the maximum of the collected
data points is less than $B - \varepsilon$.

The probability that a single data point falls below $B - \varepsilon$ is $1 -
\varepsilon/B$. The probability that all $n$ independent data points fall
below that threshold is $(1 - \varepsilon/B)^n$, which goes to zero as $n$
increases. Thus, in the limit of infinite data, taking the maximum of the data
points is a number very close to the true maximum $B$.

\subsection{Unbiased estimators}

It is poor consolation that our statistics will be correct in the limit of
infinite data. The Allies certainly weren't capturing infinite German tanks.
A scientist certainly isn't collecting infinite data points.

Consider, on the othr extreme, the case $n=1$, where we draw only one data
point $x$ somewhere between 0 and the unknown $B$. Clearly the maximum of the
drawn values, just the single number $x$, is a pretty bad estimate for $B$.
Intuitively, if you draw one data point, you expect that to fall midway
between 0 and $B$. If you draw 2 points, then the maximum is a little better.
You might expect those data points to fall roughly at $\tfrac{1}{3} B$ and
and $\tfrac{2}{3} B$.

In any case, you can be sure that $\hat{B}$ is always going to underestimate
$B$. You're never going to take the maximum of the data points and find that
they are greater than $B$. This asymmetry is called \emph{bias}. A
\emph{biased} estimator is one whose expected value is not the value it's
trying to estimate. Conversely, an \emph{unbiased} estimator is one whose
expected value is the thing it's trying to estimate:
\begin{equation}
\text{$\hat{B}$ is an unbiased estimator for $B$ means that } \expect{\hat{B}} = B.
\end{equation}

Because of the linearity of expectation, if we can figure out $\expect{\hat{B}}$, we can
just subtract that number away from $\hat{B}$ to make an unbiased estimator. Make a new
estimator called $\hat{B}_\mathrm{unbiased}$, defined as $\hat{B} - \expect{\hat{B}}$. Then
\begin{equation*}
    \expect{\hat{B}_\mathrm{unbiased}} = \expect{\hat{B}} - \expect{\hat{B}} = 0.
\end{equation*}

Let's work out the math for this example. To find the expected value of
$\hat{B}$, we need its pdf $f_{\hat{B}}$. We derive that from its cdf $F_{\hat{B}}$. Recall that the cdf
is a function that, given a number $x$, returns the probability that $\hat{B}$
takes on a value less than $x$. In other words, what is the probability that
all data points fall between 0 and $x$?

Using a logic similar to what we used when examining the consistency of $\hat{B}$,
this probability is $(x / B)^n$. The pdf, which is the derivative of the cdf,
is
\begin{equation*}
    f_{\hat{B}}(x) = \frac{d}{dx} F_{\hat{B}}(x) = \frac{n}{B} \left( \frac{x}{B} \right)^{n-1}
\end{equation*}
The expected value is the integral over the probability distribution function:
\begin{equation}
    \expect{\hat{B}} = \int_0^B x f_{\hat{B}}(x) \,dx = \frac{n}{n+1} B.
\end{equation}

This result exactly follows the inuititive idea I gave above. For $n=1$, you
expect to observe a value like $\expect{\hat{B}} = \tfrac{1}{2} B$, somewhere
halfway between 0 and $B$. For $n=2$, you expect the maximum data point to fall
somewhere around $\tfrac{2}{3} B$.

Rearranging this result,
\begin{equation}
    B = \frac{n+1}{n} \, \expect{\hat{B}},
\end{equation}
we can write an unbiased estimator:
\begin{equation}
\hat{B}_\mathrm{unbiased} \defeq \frac{n+1}{n} \max_i X_i.
\end{equation}
This ensures that our estimates for $B$ are ``centered'' around $B$.

\subsection{Efficient estimators}

Consistency and unbiasedness are two important property for ``good'' estimators.
How much further can we go? Is there a ``best'' estimator?

One unbiased estimator $\hat{X}_1$ is said to be more \emph{efficient} than another
unbiased estimator $\hat{X}_2$ if it has lower variance:
\begin{equation*}
    \var{\hat{X}_1} < \var{\hat{X}_2}.
\end{equation*}
Unbiasedness means that the estimator is ``centered around'' the true value.
Efficiency encodes how closely the estimator is distributed around the true
value. A more efficient estimator puts more narrow (i.e., better) confidence
intervals on the true value using the same number of data points.

You might go even further, and demand that an estimator, aside from being
cenetered around $B$, also have a minimal spread around $B$. In a very
confusing turn of phrase, the estimator that, in a class of estimator, has
minimum variance is the \emph{best} estimator. You're most likely to encounter
this terminology in the specific case of linear regression models, for which
it turns out that ordinary least squares gives the \emph{best linear} unbiased
estimation (BLUE) of the slope and intercept. In ``best linear'', ``best'' we just
defined and ``linear'' means that the estimator is a linear combination of the
observed data points.

There is a theoretical lower limit to the variance of estimators called the
\emph{Cram\'{e}r-Rao bound}. Rather than making a comparison, saying that one
estimator is more efficient than another, it's typical for an estimator to be
described as simply and superlatively ``efficient'', meaning that it hits this
lower variance limit and is therefore maximully efficient.

\subsection{Maximum likelihood estimators}

In the simple toy example of the German tank problem, it was pretty easy to
cook up a good estimator. There was a very short chain of logic between ``want
maximum $B$ of true distribution'' to ``look at maximum of observed values''.
It's usually not this easy.

For example, imagine you had never heard of least squares regression, but you
have some pairs of points $(x_i, y_i)$ that you thought had some linear
relationship $y_i = m x_i + b + \varepsilon_i$, where the $\varepsilon_i$ are
some random error values. What's a good estimator $\hat{m}$ to estimate the
slope $m$?

\emph{Maximum likelihood} estimator is an algorithm for creating sensible
estimators that doesn't require any reasoning about the problem. You put in
the distributions of all the relevant variables, and it spits out an
estimator. It was fun to think deeply about the German tank problem and
derive a consistent, unbiased estimator, but it's very rare that we'll be
able to pick up a pencil and paper and calculate out an answer like that.

\emph{Likelihood} in ``maximum likelihood'' is a confusing word. In common
speech, ``likelihood'' and ``probability'' are synonyms. In statistics jargon,
they mean related but distinct things. A ``probability'' density function asks
about the probability of different outcomes given some fixed parameters. A
``likelihood'' functions asks about the probability density of some fixed
outcome while varying the parameters. Imagine a probability density function
$f_X$ for some data value $x$ and some parameter $\theta$. We say that $f_X(x;
\theta)$, where $\theta$ is mostly fixed, is the probability function. We say
$L(\theta; x) \defeq f_X(x; \theta)$, where $\theta$ varies and $x$ is fixed,
is the likelihood function. We say ``maximum likelihood'' rather than
``maximum probability'' as a way of emphasizing that the data points are
fixed, and we're finding the parameters that best explain them.

The maximum likelihood (ML) approach says: as an estimator, pick the value
such that, if it were the true value, would maximize the probability density
of the observed data. For the German tank problem, this would read:
\begin{equation}
\hat{B}_\text{ML} \defeq \max_{B'} \prod_i f_X(x_i; B')
\end{equation}
Note that $B$ is still some fixed, true value. We're fiddling with $B'$, a
guess at $B$. When we find the $B'$ that best explains the data, that's our
estimator $\hat{B}_\mathrm{ML}$.

For any particular $B'$, the probability density for an $x$ to fall between 0
and $B'$ is $1/B'$. The probability for $x$ to fall above $B'$ is zero, since
it's impossible for a data point to fall above the maximum of its own
distribution. Thus, the probability the density value for all the $n$ observed
data points is 0 if any of the $x_i$ fall above $B'$ and $(1/B')^n$ if they all
fall below $B'$:
\begin{align*}
\hat{B}_\text{ML} = \max_{B'} \begin{cases}
0 &\text{if any $x_i > B'$} \\
(1 / B')^n &\text{otherwise}
\end{cases}
\end{align*}
The first case means that we can't overshoot at all, so $\hat{B}_\text{ML}$
cannot be smaller than the maximum data point. The second case shows that,
as $B'$ is increased, the likelihood of the observed data decreases. The maximum
likelihood, then, is just the value of the maximum observed data point, which
was the consistent estimator we guessed above.

In fact, all maximum likelihood estimators are consistent, and they are
maximally efficient in the limit of large sample size. (It can be that other
estimators are more efficient that the ML estimator for a finite sample size.)
Maximum likelihood estimators are not guaranteed to be unbiased. Indeed, in this
example, the maximum of the data is a biased estimator.

Although ML estimators are not perfect, it's nice that they are consistent and
have some guarantees on their efficiency. It's also really nice that they
arise from an algorithmic approach. If you can write down the likelihood
function, you can maximize it analytically, that is, exactly, by hand, like we
did in this example. If you can't write down the likelihood function but you
can sample from the distribution and compute the likelihood for every sample
you draw, then you can still do a maximum likelihood estimate. The fact that
you can take this algorithmic, computational approach means that maximum
likelihood estimation is used a lot in complex models, like we'll see later.
