%!TEX root=main

\chapter{Parameters and estimators}

\section{Orientation and definitions}

\subsection{Descriptive and inferential statistics}

Up to this point, random variables and their values have occurred in a
deductive kind of way: you know what the distribution of the random variable
is, and then you ask what its expected value or variance is. Statistics is
about going in the opposite, inferential direction. Given that I have some data---that is,
realizations of the values of some random variables---what can I say about the
distributions that those values were drawn from? Indeed, the word \emph{statistic}
means a function of the data (or the value of that function) that depends only
on the data.

This characterization of statistics embraces a common typology of statistics into
\emph{descriptive} statistics and \emph{inferential} statistics. Inferential
statistics, which includes hypothesis testing, certainly has some more
philosophical and technical hang-ups, but it's fair, at this point, to say
that both descriptive and inferential statistics hinge on how we can use the
data we have to say something about the populations that we drew that data
from. Indeed, the reason the field of study is called ``descriptive statistics''
is because it studies statistics---functions of the data---that are descriptive
of the distributions from which the data were drawn.

The jargon for this deductive/inductive difference is that mathematical
probability considers \emph{statistical parameters}, which are functions of
the distributions that give numbers, and \emph{estimators}, which are
functions of the data that approximate something about the true distribution.
The expected value $\mathbb{E}$ and variance $\mathbb{V}$ that we learned
about earlier are statistical parameters: they take a random variable (or, loosely
speaking, a ``theoretical distribution'') and
return a number. You can also think of
statistical parameters are an ``index'' of the distribution. We say ``a normal
distribution with mean $\mu$ and variance $\sigma^2$'' because, having said
those two things, we've perfectly specified what distribution we're talking
about.

This chapter is about the second thing, estimators, which, as their name
implies, are functions of the data that estimate something about the true
distribution.

\subsection{Samples and populations}

Up to this point, I've used the word ``distribution'' in the mathematical
probability sense: it's the cdf $F_X$ or the pmf/pdf $f_X$. After this point,
it will become crucial to draw a distinction between these mathematical
probability objects on the one hand and, on the other hand, the mass of data
that we get from an experiment, which might also be called the ``distribution''
of the data.

The distinction is usually made in the jargon of the social sciences. You draw
some \emph{samples} from a target \emph{population}. If you're interested in
the height of male versus female humans, then your target populations will be,
say, \emph{all} American (or whatever) males and \emph{all} American female.
In some alternate universe, you can somehow just measure the height of all
American males and female. After this, the analysis doesn't require any
probability or statistics: you just look at the complete, real data and draw
whatever conclusions you wanted to. In contrast, in this universe, you need to
take a sample of each of those populations. You use the sample data to
estimate the properties of the population.

I take this language another step further, because it's typical to make
further assumptions about the target population. In reality, the distribution
of the true population is going to be some messy curve. If you squint, it will
probably look normally distributed. But if you reallydid measure all 300 million
Americans, you would certainly find \emph{some} deviation from the normal
distribution.\footnote{This fact, that taking more data makes it hard to fit
traditional statistical models, will come up later.}

However, it's typical to assume that the target population is normally
distributed and call that normal distribution the ``population''. In that
sense, a lot of statistical methods are trying to infer the properties of some
ideal, mathematical distribution that \emph{approximates} the literal target
population. This distinction is usually not made, so when someone says
``population'' they mean ``normal distribution with mean and standard
deviation whatever''. It's also not unusual to hear about the ``true''
population, by which again is meant the idealized $\mathcal{N}(\mu,
\sigma^2)$, which might be confusing because the literal target population
will almost certainly \emph{not} be normally distributed, since the normal
distribution is just a mathematical abstraction. So just keep in mind that
most statistics only distinguishes between the \emph{sample}---the data you
have, which are a subset of the literal target population---and the ``true
population''---which is some idealized approximation of the literal target
population.

\subsection{Variation and error}

In the ``hard'' experimental sciences, this language about ``population'' may
sound weird. If I'm trying to measure the speed of light, I'll repeat my
experiment many times and get many different values. In the jargon, those
values are the sample. What, then, is the population? We think there is a
single, true speed of light. In this case, the ``population'' refers, somewhat
tautologically, to the distribution of the values that would come from an
infinite number of repetitions of our experiment. In other words, the sample
defines the population, not the other way around.

In the social sciences example, the shape of the distribution of measured
values says something about the variation of the values in the target
population: we get a range of measured values because people are different
heights. In the physics example, the shape of the distribution has a lot to do
with the precision in our experiment: we get a range of measured values
because of all sorts of error that arie the in the process of experimentation.

In biology, the situation is somewhere in between: we think there is true
variation in, say, the physiology of the cells used in an experiment. We
might wish there weren't: it would be easier to do some experiments if all
cells started in exactly the same state and responded exactly the same way to
an experimental condition. The meaning of the range of values we get is then
something of a philosophical one. To what degree do we treat the range of
values as due to error, because we couldn't make all the cells the same, and
to what degree is it an honest report about the heterogeneity of behavior in
some ``target population'' of cells? The answer depends on your research
question.

\subsection{The logic of statistics}

Regardless of whether the variation we observe is due to ``real'' variation in
the target population or to mere error, the whole logic of statistics is to
collect multiple data points and use them to infer a single truth.

This was a radical notion when it was first proposed. How could multiple,
trashy bits of data somehow make something great? It's like saying that you
can ask a thousand untrained people to make a sketch of an enigmatic, smiling
woman and somehow expect the combination of those thousand sketches to turn
into the Mona Lisa. The assumed correct answer, before statistics, was to pick
the best value. If multiple scientists reported different values, you would
examine their experimental methods, see how good they were with their hands,
and pick the value from the scientist with the best method.

This idea of ``combination of observations'' is today so ingrained that it's
hard to imagine \emph{not} using a combination of observations to infer a
truth. I think a useful bridge is the idea of meta-analysis. For example, does
a certain educational intervention improve students' learning? Until the
1970s, the assumed correct answer was to examine all the interventions, see
which one worked well, and mostly ignore the ones that were imperfect
according to the measure established by that one study (or small set of
studies). In the 1970s, meta-analysis started comparing multiple studies---
which detractors said was like comparing apples and oranges---to ask about
whether an entire class of education intervention works. The 1970s was not
that long ago.


\section{Properties of estimators}

Let's get to it! What are these estimators? What do they do? Rather than give
some dry definitions, I'll use a motivating example, from which the
definitions will arise (I hope) naturally.

\subsection{A motivating example: the German tank problem}

During World War II, it was important to the Allies to know how many tanks
Germany was producing. One way to find this out was with spies, aerial
reconnaissance, etc. The other way---you guessed it!---was with statistics.

The German tanks had sequential serial numbers for various parts of the tank.
This means, if you had a list of all the serial numbers, you could figure out
easily figure out how many tanks there were, since they would go from 1 up to
the number of tanks. Of course, if you had all the serial numbers, you
probably got them from the tanks, and you could probably just count the tanks.

Consider this simplified problem: say you know the serial numbers go from 1 up
to $N$, the actual number of tanks produced, and say you capture a few tanks
with serial numbers $x_1, x_2,$ etc. How many tanks do you think there
are?

It turns out that the math will be simpler for the continuous case, in which
we draw numbers from some continuous, uniform distribution between $A=0$ and
$B>A$. The probability distribution function for a random variable $X$ that
follows this distribution is:
\begin{equation}\label{eq:uniform_pdf}
f_X(x) = \begin{cases}
  1/B &\text{if $0 \leq x \leq B$} \\
  0 &\text{otherwise}
\end{cases}
\end{equation}
The idea is, if you don't know $B$, we'll draw many data points from this
distribution and use them to make guesses about the value of $B$.\footnote{The math
of the German tank problem, with integers, is very similar, except that a
certain sum over integers is hard and the corresponding integral over
continuous numbers is easy.}

The idea is to generate an estimator, a function of our data that will
approximate the true value $B$. It's typical to write estimators as the
true value with a ``hat'', so we're interested in making a function $\hat{B}$.

\subsection{Consistent estimators}

Let me start with what will seem like a very inane example.\footnote{Perhaps
this sounds silly, and it is, but I've more than once heard respected and
well-meaning scientists say that the answer to statistical questions is ``3'',
just 3.}  Say I draw $n$ data points, and then I ignore all of them and just
pick 3:
\begin{equation}
\hat{B}_\mathrm{inane}(\{x_i\}) \defeq 3,
\end{equation}
where $\{x_i\}$ means that $x_i$ for $i = 1, \ldots, n$.
Now, this technically is an estimator: it's a function of the
data alone, only it just so happens to not use any of that data. It's also,
and not just technically, not a good estimator at all. The main problem is
that, no matter how much data I accumulate, it's not guaranteed to be right.
(In fact, it's guaranteed to be right only if $B=3.000\ldots$ just by sheer luck.)

A very valuable improvement presents itself: if I drew $n$ data points, why not
just take the maximum of those data points? I'll call this $\hat{B}_1$ since
it's our first reasonable guess:
\begin{equation}\label{eq:hatb1}
\hat{B}_1\left( \{x_i : i \in 1, \ldots, n\} \right) \defeq \max_i x_i
\end{equation}
Note that $\hat{B}_1$ is a function only of our data, the sampled numbers $x_i$.

This new estimator has the crucial property of being \emph{consistent}, which
means that, as you collect more and more data, the value of the estimator
approaches the true value of the thing being estimated:
\begin{equation}\label{eq:consistent-estimator}
\text{$\hat{B}$ is a consistent estimator for $B$}
  \implies \lim_{n\to\infty} \hat{B}(\{x_i : i \in 1,\ldots,n\}) = B
\end{equation}

\subsection{Estimators are random variables}

I pulled two fast tricks in \eqref{eq:consistent-estimator} that I should explain.

First, I said that an estimator is a function of the data. That's true, from a
certain point of view: you can think of an estimator that takes numbers and
gives back another number. Analogously, you can think of an estimator as a
random variable that takes on values in the same way that the function-of-
numbers estimator takes numbers as input. In other words, I could write \eqref{eq:hatb1}
as
\begin{equation}
\hat{B}_1\left( \{X_i\} \right) \defeq \max_i X_i.
\end{equation}
That's right: I just changed the lowercase $x_i$ to uppercase $X_i$. So what?

The point is that, so long as we think of $\hat{B}_1$ as a function that just
takes numbers, we can't do anything about probability with it. However, it is
easy to translate a function-of-numbers into a function-of-random-variables.
Now that $\hat{B}_1$ is itself a random variable, we can ask interesting questions
about its behavior without examining the outcome of any particular experiment.
Later in this section, you'll see why looking at the expected value and variance
of an estimator is interesting.

Second, I used the word ``limit'' in \eqref{eq:consistent-estimator}.
If you're a math wizzkid you'll have pounced that. To say that
a \emph{sequence of numbers} has a limit means that, for any threshold difference
$\varepsilon$ you give me, I can give you an integer $N$ such that for every value
in the sequence after the $N$-th one is within $\varepsilon$ of the true value.
Clearly this definition is not sufficient in probability, where you could never
be guaranteed, even after a trillion data points, of anything.
In this example, there's always some
chance that all your data points happen to fall really short of $B$.

Instead, in
probability, we say that a sequence of random variables $\{Y_i\}$ \emph{converges}
toward a number $a$
\begin{equation}
\lim_{n\to\infty} \prob{|Y_n - a| > \varepsilon} = 0,
\end{equation}
in other words, if the probability that $Y_n$ takes on a value more than $\varepsilon$
away from $a$ vanishes. In our example, $a$ is $B$,
and the $Y_n$ are the random-variable estimators
for each $n$: $\max_i \{X_i : i \in 1,\ldots, n\}$.

It's fairly intuitive that $\hat{B}_1$ converges to $B$. To prove it, you give me
some small $\varepsilon > 0$.
The probability of a point falling more than $\varepsilon$ below $B$
is $1 - \varepsilon/B$. The probability of all $n$ points falling $\varepsilon$ from $B$
is $(1 - \varepsilon/B)^n$,
which goes to zero as $n$ increases, no matter how small you make $\varepsilon$.

\subsection{Unbiased estimators}

Great! We have a consistent estimator $\hat{B}_1$. If I collect a zillion data
points, I'll get really close to $B$.

This seems like poor consolation. I'm rarely drawing a zillion data points.
The Allies certainly weren't capturing a zillion German tanks.
Consider the opposite case, if I draw only one data point. Clearly the maximum
of the drawn values, just one that point, is a pretty bad estimate for the
upper limit. If you draw 3 points, then the maximum is a little better, but
you can still feel very sure that estimating population maximum $B$ using the
sample maximum is \emph{biased}: it is always going to underestimate the
population maximum. This is true even if I take a zillion data points: I'm
never ever going to overestimate $B$ using $\hat{B}_1$.

Formally, a \emph{biased} estimator is one whose expected value is not the
thing it's trying to estimate. Conversely, an \emph{unbiased} estimator is
one whose expected value is the thing it's trying to estimate:
\begin{equation}
\text{$\hat{B}$ is an unbiased estimator for $B$} \implies \expect{\hat{B}} = B.
\end{equation}

How do we turn a biased estimator into an unbiased one? Maybe, if we compute
$\expect{\hat{B}}$, we can just look and find some way. This will require the
probability density function for $\max_i X_i$. Similar to the calculation above,
it's actually not hard to write the probability of a single data point being
below $x$, that is between zero and $x$: it's just $x/B$. The probability of all
$n$ data points falling between zero and $x$---which is the same as saying that
the maximum is $x$---is $(x/B)^n$. I'll write a new random variable $M = \max_i X_i$
so that
\begin{equation}
F_M(x) = (x/B)^n.
\end{equation}
Recall that the probability density function is the derivative of the cumulative
distribution function:
\begin{equation}
f_M(x) = \frac{d}{dx} F_M(x) = \frac{n}{B} \left( \frac{x}{B} \right)^{n-1}.
\end{equation}
The expected value is the integral over this probability distribution function:
\begin{equation}
\expect{M} = \int_0^B x f_M(x) \,dx = \frac{n}{n+1} B.
\end{equation}

Let's examine this result: if $n=1$, then $\expect{M} = \tfrac{1}{2}B$. In
other words, if you draw one point, you expect it to be right in the middle of
the range $[0, B]$. Seems reasonable. For $n=2$, $\expect{M} = \tfrac{2}{3}B$.
In other words, you expect your data points to fall at something like
$\tfrac{1}{3}$ and $\tfrac{2}{3}$, so you expect a maximum of $\tfrac{2}{3}$.
Clearly, as $n \to \infty$, $\expect{M} \to B$, like we had shown previously.

I can just flip the numerical factor out front to get
an unbiased estimator:
\begin{equation}
\hat{B}_\mathrm{unbiased} \defeq \frac{n+1}{n} \max_i X_i.
\end{equation}
I did this so that:
\begin{equation}
\expect{\hat{B}_\text{unbiased}} = \expect{\frac{n+1}{n} \max_i X_i} = \frac{n+1}{n} \expect{\max_i X_i} = B.
\end{equation}
So now we've assured that our estimates for $B$ are ``centered'' around
$B$.\footnote{I put ``cenetered'' in quotes because the expected value can be
a weird way to quantify ``middleness''. We'll get to that soon.} Sometimes we
will overestimate $B$, but at least we won't, in a sense, be systematically
underestimating it.

\subsection{Best linear unbiased estimators (BLUEs)}

You might go even further, and demand that an estimator, aside from being
cenetered around $B$, also have a minimal spread around $B$. In a very
confusing turn of phrase, the estimator that, in a class of estimator, has
minimum variance is the \emph{best} estimator. You're most likely to encounter
this terminology in the specific case of linear regression models, for which
it turns out that ordinary least squares gives the \emph{best linear} unbiased
estimator, where ``best'' we just defined and ``linear'' means that the
estimator is a linear combination of the data.

It's worth noting that our $\hat{B}_\text{unbiased}$ is a linear combination
of the $X_i$: it has coefficient zero for all the $X_i$ except for the one
that takes on the biggest value, which we give coefficient 1. Beyond that,
though, I don't think it's worth digging into the variance of
$\hat{B}_\text{unbiased}$. I do think it's worth understanding that different
estimators can vary in their precision.

\subsection{Maximum likelihood estimators}

In this simple case, it was pretty easy to cook up a good estimator. There was
a very short chain of logic between ``want maximum of true distribution'' to
``look at maximum of observed values''. It's usually not this easy. To revisit
the question of regression, if I have a bunch of data points, what's the a
good way to make a slope out of them? It's not \textit{a priori} obvious.

There's a convenient methodology for making sensible estimators, called
\emph{maximum likelihood} estimators. \emph{Likelihood} is a confusing word:
in common speech, ``likelihood'' and ``probability'' are synonyms. In
statistics jargon, they mean similar things, with slightly different emphases.
A ``probability'' function typically asks about the probability of different
outcomes given some fixed parameters. A ``likelihood'' functions asks about
the probability (or probability density) of some fixed outcome while varying
the parameters. Imagine a probability density function $f_X$ for some data
value $x$ and some parameter $\theta$. We say that $f_X(x; \theta)$ is the
probability function and $L(\theta; x) \defeq f_X(x; \theta)$ is the
likelihood function. We say ``maximum likelihood'' rather than ``maximum
probability'' as a way of emphasizing that the maximization will be done for
fixed data points over varying parameters.

For our example, the maximum likelihood (ML) approach says: as an estimator, pick
the value such that, if it were the true value, would maximize the probability
density of the observed data. That is:
\begin{equation}
\hat{B}_\text{ML} \defeq \max_b \prod_i f_X(x_i; b)
\end{equation}
We can use \eqref{eq:uniform_pdf} to compute the probability:
\begin{align*}
\hat{B}_\text{ML} = \max_b \begin{cases}
0 &\text{if any $x_i > b$} \\
\prod_i (x_i/b) &\text{otherwise}
\end{cases}
\end{align*}
The first case means that we can overshoot at all, and the second says that
the likelihood decreases if we decrease $b$. The solution is that
$\hat{B}_\text{ML} = \max_i X_i$, which was our first good guess.

As an exercise, redo these calculations for the German tank problem, where you
draw integers between 1 and an unknown $N$. Start by assuming that $N$ is much
larger than the number of integers you draw so that the chance of drawing the
same integer twice is really small and you can treat the separate draws as
independent. The expected value will be tough to calculate, so just do it for
drawing one point. Then, do everything with a computer.

\section{Descriptive estimators}

\subsection{The arithmetic mean: a first estimator of central tendency}

One of the crucial misunderstandings in pre-statistical experimentation was
that the amount of information you had---or, similarly, the precision of your
measurement---increased linearly with the number of data points you took. So
if I took 100 data points and you took 25, I would have 4 times more
information than you.

A simple and very important instance of this is estimating the expected value
of a distribution. People were doing this before they knew they were doing it,
and you probably have too: the arithmetic mean of a bunch of numbers is an
estimator of the expected value of the distribution they were drawn from.

I'm suggesting that a good estimator for $\mathbb{E}[X]$ is $(1/N)\sum_i X_i$,
where the $X_i$ are iid random variables distributed just like $X$. I think
the most consistent notation would be to name this estimator
$\hat{\mathbb{E}}_X$, to emphasize that it is a random variable that is
estimating the expected value of $X$. It's typical and more compact however,
to write it as $\overline{X}$, in analogy with the arithmetic mean of numbers
$\overline{x}$. Regardless of what notation you use, the important this to
remember is that recall that $\mathbb{E}$ is a function that takes random
variables like $X$ and returns numbers, so $\expect{X}$ is a fixed number,
while $\hat{\mathbb{E}}_X$ (a.k.a. $\overline{X}$) is an estimator, that is, a
random variable.

I'm going to show you that the arithmetic mean is an
unbiased estimator of the expected value:
\begin{equation}
\expect{\overline{X}} = \expect{\frac{1}{N} \sum_i X_i} = \frac{1}{N} \sum_i \expect{X_i} = \expect{X}.
\end{equation}
I showed that the expected value $\expect{\overline{X}}$ of a random variable
$\overline{X}$---which happens to be an estimator for the expected value
$\expect{X}$ of the random variable $X$---is exactly equal to the expected
value $\expect{X}$, the value that was being estimated. That means that $\overline{X}$
is an unbiased estimator for $\expect{X}$.

Note that the arithmetic mean is also automatically a consistent estimator of
the expected value. Even if you draw just one data point, the expected value
of the arithmetic mean is the true expected value of the target distribution.

So I showed you, with some crazy symbology, something you felt was true for a
long time. So what? Well, this formalism set us up to answer harder questions.
Here's one: what's the \emph{variance} of $\overline{X}$? This will help
answer the question: what's a typical deviation of the arithmetic mean from
the true expected value of a distribution? Now that we're thinking of the
arithmetic mean as a random variable, we have the power!
\begin{align}\label{eq:sem}
\var{\overline{X}} &= \var{\frac{1}{N} \sum_i X_i} \nonumber\\
  &= \frac{1}{N^2} \sum_i \var{X_i} \quad\text{(variances of independent rv's add)} \nonumber\\
  &= \frac{\var{X}}{N}.
\end{align}
Variance is a little confusing, being the square of the normal deviation, so
normally we take it's square root and call it the standard deviation. The
standard deviation of the arithmetic mean---as an estimator for $X$---is the
standard deviation of $X$ divided by $\sqrt{n}$.\footnote{Take careful note
that the words ``standard deviation'' here mean ``square root of the variance'',
so, in this case, standard deviation is a function of a random variable, \emph{not}
a function of numbers.}

This is a fundamental and critical result in statistics: if I take 100 data
points and you took 25, then my arithmetic mean's standard deviation has a
term like $\sqrt{100} = 10$ in the denominator, and yours has $\sqrt{25} = 5$.
In other words, even thought I took four times the data, my estimate is only
twice as precise as yours. We get diminishing returns from every data point
we take.

You may see a resemblance between the results above and the familiar equation
$s/\sqrt{n}$ for the ``standard error'' or ``standard error of the
mean''.\footnote{I haven't defined $s$ because we're going to get to that in
the next section.} The reason the ``standard error'' is different from the
``standard deviation'' by a factor of $\sqrt{n}$ is exactly because of what we
just went through: $s$ is some estimate of the standard deviation of the true
population---that is, of the variation inherent in the thing being
sampled---and $s/\sqrt{n}$ is the variation inherent in the arithmetic mean.

\subsection{Variance: a first estimator of spread}

You may have been disappointed by $\eqref{eq:sem}$: when you asked me about
the variance in my estimate of the expected value, I told you that it had to
do with the true variance of the population $\var{X}$. That's normally not
information we have access to: we usually also need to estimate the population
variance.

What's a reasonable estimator the true variance $\var{X}$? Well, just as the
true variance is the square of the values in the distribution from their true
expected value, we might guess that an estimate for the variance is the
arithmetic mean of the squares of the deviations of the data points from their
arithmetic mean:
\begin{equation}\label{eq:estimator-vx1}
\hat{\mathbb{V}}_X \defeq \frac{1}{N} \sum_i \left( X_i - \overline{X} \right)^2.
\end{equation}

Before diving into that equation, note that both $X_i$ and $\overline{X}$ are
random variables, and note that they are not independent: the value of
$\overline{X}$ certainly depends on each of the $X_i$. So first let's imagine
a simpler case, where we're in a universe where happen to know the expected
value of the distribution we're trying to determine the variance of. To make
the equations simpler to read, I'll use the standard notation $\mu \defeq
\expect{X}$. In this case, having known expected value (``kEV''), our
estimator will be a little simpler:
\begin{equation}
\hat{\mathbb{V}}_{X,\mathrm{kEV}} = \frac{1}{N} \sum_i ( X_i - \mu )^2
\end{equation}
This estimator is unbiased:
\begin{align*}
\expect{\hat{\mathbb{V}}_{X,\mathrm{kEV}}}
  &= \expect{\frac{1}{N} \sum_i ( X_i - \mu )^2} \\
  &= \frac{1}{N} \sum_i \expect{X_i^2 - 2\mu X_i + \mu^2} \\
  &= \expect{X^2} - 2 \mu \expect{X} + \mu^2 \quad\text{(since $X_i$ are identic. distrib.)} \\
  &= \expect{X^2} - \mu^2 \\
  &= \var{X}. \quad\text{(since $\var{X} = \expect{X^2} - \expect{X}^2$)}
\end{align*}
So if we happen to know the true expected value $\mu$, then we can compute
variance in the naive way, and it's exactly correct.

Now we can use this result to pull a little mathematical trick. Even if we
don't know $\mu$ and $\var{X}$, we do know they exist, so we can manipulate
\eqref{eq:estimator-vx1} to make it easier to work with. Specifically, I'm
going to write ``standardized''\footnote{You're probably used to seeing
standardized \emph{normal} variables written this way, but note that I haven't
assumed that the $X_i$ are normally distributed. This logic holds for any
random variable that has a well-defined expected value and variance.}
random variables:
\begin{equation}
Z_i = \frac{X_i - \mu}{\sqrt{\var{X}}} \implies X_i = \sqrt{\var{X}} Z_i + \mu
\end{equation}
It should be easy to see that $Z_i$ has expected value 0 and variance $\expect{Z_i^2} = 1$,
and $\overline{Z}$ has expected value
0 and variance $\expect{\overline{Z}^2} = 1/n$. Now I'll rewrite \eqref{eq:estimator-vx1}
so it has $Z_i$ instead of $X_i$:
\begin{align*}
\hat{\mathbb{V}}_X
  &= \frac{1}{n} \sum_i \left( X_i - \overline{X} \right)^2 \\
  &= \frac{1}{n} \sum_i \left( \left[\sqrt{\var{X}} Z_i + \mu\right] - \left[ \sqrt{\var{X}} \overline{Z} + \mu \right] \right)^2 \\
  &= \frac{\var{X}}{n} \sum_i \left( Z_i - \overline{Z} \right)^2
\end{align*}
Analogous to how we showed that $\var{X} = \expect{X^2} - \expect{X}^2$, some
algebra shows that
\begin{equation}
\sum_i \left(Z_i - \overline{Z}\right)^2 = \sum_i Z_i^2 - n \overline{Z}^2.
\end{equation}
Thus, the expected value of this estimator is
\begin{align*}
\expect{\hat{\mathbb{V}}_X}
  &= \expect{\frac{\var{X}}{n} \sum_i \left( Z_i - \overline{Z} \right)^2} \\
  &= \frac{\var{X}}{n} \expect{\sum_i Z_i^2 - n \overline{Z}^2} \\
  &= \frac{\var{X}}{n} \left( \sum_i \expect{Z_i^2} - n \expect{\overline{Z}^2} \right)\\
  &= \frac{\var{X}}{n} (n - 1) \quad\text{(using the little identities)} \\
  &= \frac{n-1}{n} \var{X}.
\end{align*}
Note that the expected value of our estimator is not equal to the thing we're
trying to estimate, so this estimator is biased! It systematically \emph{underestimate}
the true variance. Interestingly, like when we tried to cook up
an estimator for the upper limit of a uniform distribution, we ended up with
something that was off by a multiplicative factor. The solution is to make a new
estimator that has the inverse, cancelling factor out front:
\begin{equation}
\hat{\mathbb{V}}_{X,\mathrm{unbiased}}
  = \frac{n}{n-1} \times \frac{1}{n} \sum_i \left(X_i - \overline{X}\right)^2
  = \frac{1}{n-1} \sum_i \left(X_i - \overline{X}\right)^2.
\end{equation}
Using $n-1$ instead of $n$ in the denominator is known as Bessel's
correction.\footnote{Gauss was using the correction before Bessel discovered
it, as early as 1823. I hope the late date, 1823, impresses upon you how
subtle this reasoning must be. The ancient Greeks were using the arithmetic
mean, but an unbiased estimator for standard deviation took thousands of
years.}

This equation may look familiar as the ``sample variance'', typically written
\begin{equation}
s^2 = \frac{1}{n-1} \sum_i \left(x_i - \overline{x}\right)^2.
\end{equation}
I always wondered why, in Stats 101, I was told that the mean had $n$ in the
denominator but variance had $n-1$.\footnote{The traditional answer says that it's
about ``degrees of freedom'': you're ``using up'' one ``degree of freedom''
when computing $\overline{x}$, so you only have $n-1$ to compute $s^2$.
``Degrees of freedom'' is an ill-defined concept that I don't think it useful.
Explaining one mystery in terms of another mystery is not good pedagogy!}
This is why! The point is to make an unbiased estimator.

Here's the intuitive explanation. Remember that we previously derived that, if
we know $\mu$, then the equation $(1/n) \sum_i (x_i-\mu)^2$ works just fine:
we get an unbiased estimator for $\var{X}$. The trouble, then, is simultaneously
estimating $\expect{X}$ and $\var{X}$. Our estimate $\overline{x}$---although unbiased,
so not systematically above or below the true $\expect{X}$---will always be in the
middle of the our data $x_i$. Therefore, the distance between each $x_i$ and our
estimated $\overline{x}$ will be systematically smaller than the distance between
each $x_i$ and the true $\expect{X}$. By how much? Well, that's asking about how
much larger, on average, $(x_i - \overline{x})^2$ is smaller than $(x_i - \mu)^2$.
We did just that calculation: the typical values for $(x_i - \overline{x})^2$ are
part of our estimate $\hat{\mathbb{V}}_X$, while the typical values for
$(x_i - \mu)^2$ come from the true variance $\var{X}$. The difference between those
values is the bias, which we can correct for with that factor of $n/(n-1)$.

Comfortingly, as $n$ grows, the difference between using $n$ and $n-1$ in the
denominator becomes unimportant. If you're a big data person, then there's
really no need to worry about Bessel's correction. The point of all this was
to show the logic of looking for unbiased estimators, and to show you that
there's a good example of a bias-corrected estimator right under all our
noses.

\hl{consistency?}

\subsection{Maximum likelihood estimate of the mean of a normal distribution}

Why is this a good idea? Early thinkers in statistics---who thought
about it before it had a name---were curious about the same thing. The
arithmetic mean was computationally simple (i.e., you could do it will
quill and parchment, OK, pen and paper) and it seemed to ``work'', but
why?

It turns out that the arithmetic mean has some nice properties for, you
guessed it, the normal distribution. In fact, we'll show that it is a
certain kind of ``best way'' to guess the true, population mean from our
sample mean. (It's the maximum likelihood estimator.)

\subsection{Median and mode: other estimators of central tendency}

In typical speech, we say something like, ``The average family has two
children'', by which we mean that a typical family has two children.
This is confusing because we also use the word ``average'' to refer to
the arithmetic mean. These two things are similar only in special cases,
including---you guessed it---the normal distribution.

The arithmetic mean of number of children in families is somewhere
between 2 and 3 (let's say 2.5), but it's clear that no ``average
family'' has 2.5 children. Similarly, it's confusing that the mean
salary in the US is whatever, even thought that is above what whatever
percent of people get paid.

Typical can mean ``common'', so we might say, in a sense, that an
``average'' family has two children. Of the number of children you can
have (zero, one, two, etc.), two is the most common, so that's
``average''. Technically, the most common value is the \emph{mode}. No
one really talks about the mode except in stats textbooks.

A more interesting number is the middle point. We might say that an
average American makes whatever because half of people make more and
half of people make less. This is the \emph{median} (from Latin
\emph{medius}, meaning ``middle'').

It even feels natural to combine the median and mode ideas (have our
median cake and eat it a la mode?). We want a sense of what are
``middling'' numbers, and we want a sense of what are ``common''
numbers. So you might ask what range is covered by the most middle half
of numbers.

Children are often measured against a growth chart, which shows
\emph{percentiles}: if you are in the 25th percentile, you are taller
than 50\% of people. (The median is just another name for the 50th
percentile.) The range between the 25th and 75th percentiles, which
covers the middle half of people, is the \emph{interquartile range},
because the 25th, 50th, and 75th percentiles are also called the
\emph{quartiles}, because they divide the numbers into four quarters
(bottom quarter, bottom-middle, top-middle, and top).

\subsection{Mean, median, and mode are usually
different}\label{mean-median-and-mode-are-usually-different}

Roll a dice many times. What are the mean, median, and mode? Mean is
easy: each number is equally likely to come up, so we just take the mean
of 1, 2, 3, 4, 5, 6, which is 3.5. The median is a little tricky here:
it's between 3 and 4, and when we hit this situation we normally define
the median as the arithmetic mean of the two middle values. So the
median is also 3.5 here, but only because of some convention. The mode
is also confusing, since all numbers are equally likely, which means
that they are all equally the mode.

For the normal distribution, the mean, median, and mode are all the
same. So it's only in this very potentially unusual case that our
intuition is correct that that the ``average'' value, in the sense of
something common (i.e., the mode), is the same as the ``average'' value,
in the sense of something middling (i.e., the median), is the same as
the arithmetic mean.

Historical box for Quetelet: the idea of an ``average person'' is
directly traceable to a particular proto-statistician, who is remarkable
for having believed that almost \emph{everything} was normally
distributed. At the time, they only had very rudimentary methods for
determining if something was normally distributed (basically, eyeballing
it), and there were very appealing aesthetic/philosophical reasons to
believe that almost everything was normally distributed, so he believed
that too.
