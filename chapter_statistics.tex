\chapter{What is statistics?}

``Statistics'' is confusing because it means two things. It's a plural noun that
refers to multiple things, each of which is called a ``statistic''. It's also a
singular noun that refers to the study of these mathematical objects.

Statistics is a tool.\footnote{I really enjoyed reading this anecdote from a
paper by Gene Glass, the inventor of meta-analysis (doi:10.1002/jrsm.1133).
Glass. He ends up sitting on the plane next to the famous statistician
Tukey. Tukey asks, "Along which dimension do you see the greatest
variability in effects?" By investigator. "Then jackknife on investigator."
The point is that there's a wide gap between being able to understand what
the jackknife is in a mathematical sense (i.e., how to compute it, whether
it's an unbiased way to determine the variance in the median) and how to apply
it. It also goes to show that "real" statisticians can be pretty hacky!}

A \emph{statistic} is some function of the sample data. Although the field of
statistics is often described as consisting of "descriptive statistics" and
"inferential statistics"---that is, the study of mathematical objects used to
describe data and to make inferences about the populations the data were taken
from---in this book I want to emphasize that, for both purposes, we are
interested in the properties of statistics.

The most interesting statistics are the ones that are used to estimate some
property of the population. These kinds of statistics are sensibly called
\emph{estimators}. Most of the study of statistics comes down to figuring out things
about estimators. One of the most critical is understanding the variance of
estimators, which is essential for both descriptive statistics---so that you
can put error bars on your measurements---and inferential statistics---so you
can make a guess about how probable it is that your data arose under some null
hypothesis. All of this is about estimators and their variance.