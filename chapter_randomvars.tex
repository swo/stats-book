%!TEX root=main.tex

\chapter{Random variables}

In the previous chapter, we examined outcomes, events, and their
probabilities. Outcomes are specific configurations of atoms, which is not at
all a practical thing for a scientist think about. Events are abstract
groupings of those configurations, which is also impractical. Instead, in
science, we measure the results of experiments with numbers. We link whatever
we see with a number. In microbiology, we could cells. In physics, we measure
the energy of an electron.

In probability theory, \emph{random variables} link outcomes with numbers.
They are the mathematical analogue of scientific measurement. Random variables
will lead us to familiar concepts like means, standard deviations, and
probability distributions.

\section{Definition of random variables}

Confusingly, a ``random variable'' is a function, not a number. We might say
that a random variable ``takes on'' a value, but the random variable itself,
in mathematical terms is always a function.

A \emph{discrete} random variable $X$ is a function that associates outcomes
$\omega \in \Omega$ with a finite set of numbers $x_i$:
\begin{gather*}
X : \Omega \to \{x_1, \ldots x_k\} \\
X[\omega] = x_i
\end{gather*}
for some finite $k$. A \emph{continuous} random variables associates outcomes with real
numbers $x \in \mathbb{R}$:
\begin{gather*}
X : \Omega \to \mathbb{R} \\
X[\omega] = x
\end{gather*}
This is a simplification: measure theory says that a random variable can map
the outcome space to any \emph{measurable space}. We won't dive into that.

For example, when flipping a die, you could imagine a random variable $X$
encoding ``number of heads flipped'':
\begin{align*}
X[H] &= 1 \\
X[T] &= 0
\end{align*}
Roughly speaking, the values of the random variable group the outcomes into
events, which each have probabilities. This means it makes sense to ask about
the event in which a random variable takes on a value, e.g., $\prob{X = 1} = \tfrac{1}{2}$.
Note that ``$X = 1$'' does not mean that $X$ is the number one; instead, the
entire phrase ``$X = 1$'' refers to the event where $X$ took on the value one.

Random variables therefore allow us to abstract over the outcomes and look
only at the values taken on by the random variable. From now on, we will have
no reason to think about the specific configuration of atoms in our
experiment. We only think about the numbers delivered by our apparatus. Soon
we'll be able to say things like ``$X$ is normally distributed'' without having
to specify what are the outcomes and events.

The previous chapter was not a waste, however, because the
rules for manipulating events and probabilities work just as well for
events like ``$X = 1$'' as they did for ``flipped heads''. The philosophical and
technical definition of probability still holds. For example, $\prob{(X = 1)
\cup (X = 0)}$, the probability that $X$ takes on the values zero or one, is the
sum of the two constituent probabilities $\prob{X=1}$ and
$\prob{X=0}$.

\section{Distribution of random variables}

How is $X$ ``distributed''? If you're like me, you think in terms of
\emph{probability distribution functions} (pdf's) and consider
\emph{cumulative distribution functions} (cdf's) as a derived quantity, but in
fact it's mathematically easier to go the reverse route. If you aren't
familiar with either, never fear.

\subsection{Cumulative distribution functions}

The first way of defining the ``distribution'' of a random variable $X$ is its
\emph{cumulative distribution function}, also called the cumulative
\emph{density} function and abbreviated ``cdf''. The cdf of a discrete random
variable $X$ is the probability that the random variable takes on a value
below some threshold $x_i$:
\begin{gather*}
F_X : \{x_1, \ldots, x_k\} \to [0, 1] \\
F_X(x_i) \defeq \prob{X \leq x_i} = \sum_{j \,:\, x_j \leq x_i} \prob{X = x_j}
\end{gather*}
A continuous random variable has a cdf that takes real numbers:
\begin{gather*}
F_X : \mathbb{R} \to [0, 1] \\
F_X(x) \defeq \prob{X \leq x},
\end{gather*}
Note that the cdf is a function of numbers, which I emphasize by
using regular parentheses around $x$ in $F_X(x)$.

Take careful note that ``$X \leq x$'' is an event, not a Boolean expression
like ``$1 \leq 2$''. This may seem pedantic, but I think having a really good
grasp of the notation will help you articulate correct thoughts more clearly.
Things will quickly get confusing if you think that $\prob{X \leq x}$ means the same
thing as $\prob{x \leq x}$.

% swo: need a figure here with the laddering

A discrete random variable takes on a maximum value $x_\mathrm{max}$ so that
\begin{gather}
\text{if } x < x_\mathrm{min} \text{, then } F_X(x) = 0 \\
F_X(x_\mathrm{max}) = 1.
\end{gather}
If a continuous random variable can take on any real number, then  the cdf of
a continuous random variable approaches zero and one as $x$ goes out toward
infinity:
\begin{gather}
\lim_{x \to -\infty} F_X(x) = 0 \\
\lim_{x \to \infty} F_X(x) = 1
\end{gather}

% need a figure here with, say, normal distribution

\subsection{Probability distribution function}

For a discrete random variable, it's straightforward to define its
\emph{probability mass function} or ``pmf'', which is just the probability
that it takes on each discrete value $x_i$:
\begin{gather*}
f_X : \{x_1, \ldots, x_k\} \to [0, 1] \\
f_X(x_i) = \prob{X = x_i}
\end{gather*}

The analogue for continuous random variables is the probability
\emph{distribution} function, also called probability \emph{density} function
or ``pdf'':
\begin{gather*}
f_X(x) : \mathbb{R} \to [0, 1] \\
f_X(x) \defeq \frac{d}{dx} F_X(x)
\end{gather*}

Note that the pdf of a continuous random variable is not $\prob{X = x}$. This
may seem like a pedantic diversion, but I actually think it's important to
avoid confusion. For a continuous random variable, $\prob{X = x}$ is zero. For
example, say $X$ takes on values between 0 and 1 symmetrically, e.g., the
probability that $X$ comes out between 0 and 0.5 is the same as between 0.5
and 1. Say you're asking about the probability that $X$ comes out as 0.5. I'll
grant the following:
\begin{align*}
\prob{0 \leq X \leq 1} &= 1 \\
\prob{0.495 \leq X \leq 0.505} &= 0.1 \\
\prob{0.4995 \leq X \leq 0.5005} &= 0.01 \\
&\vdots
\end{align*}
and so on. We normally don't say that $\prob{X = 0.500\ldots} = 0$, since that
makes it sound like it's impossible for $X$ to take on the value $0.5$.
Nevertheless, it should be clear that, from any practical point of view, it
doesn't make sense to write things like $\prob{X = 0.5}$. It does make sense
to write $f_X(0.5)$; it does not make sense to write $\prob{X =
0.5}$.\footnote{Those dissatisfied with this explanation will need to learn
some measure theory.}

The word ``density'' in probability density function emphasizes that the pdf,
when integrated, gives probabilities:
\begin{equation}\label{eq:integrated_pdf}
\int_{x_0}^{x_1} f_X(x') \,dx' = F_X(x_1) - F_X(x_0) = \prob{x_0 < X \leq x_1}
\end{equation}
If $X$ can take on any value, then it follows from the definition of the cdf and
some fundamental calculus that
\begin{equation}
\int_{-\infty}^\infty f_X(x') \,dx' = 1
\end{equation}
that is, that all the probability must be somewhere, and
\begin{equation}
\lim_{x \to \pm \infty} = 0.
\end{equation}

These definitions I've given are simple ones, and they need to be refined to
deal with more complicated functions. For example, if the cdf has
discontinuities, you need careful with the limits on the integrals.

\subsection{Known distributions}

If a random variable $X$ has the same cdf as another random variable $Y$ (and
therefore also the same pdf), we say that $X$ is ``distributed like'' $Y$:
$$
X \sim Y.
$$
This notation is used to refer to well-documented distributions. For example,
you might say that $X$ is distributed like a normal random variable with mean
$\mu$ and variance $\sigma^2$:
$$
X \sim \mathcal{N}(x; \mu, \sigma^2),
$$
which is a shorthand for saying
\begin{equation*}
f_X(x) = f_\mathcal{N}(x; \mu, \sigma^2) \defeq
  \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left\{-\frac{1}{2} \frac{(x-\mu)^2}{\sigma^2} \right\}.
\end{equation*}
Similarly $X \sim \mathrm{Binom}(n, p)$ means that $X$ follows a binomial distribution:
\begin{equation*}
f_X(x) = f_\mathrm{binom}(x; n, p) \defeq \binom{n}{x} x^p (n-x)^{1-p}.
\end{equation*}
We'll come back to these distributions and their pdf's later; I just want to
you understand that, when we say that ``$X$ is normally distributed'', we
making a mathematically precise statement about $X$'s pdf and cdf.

\subsection{Independent, identically-distributed random variables}

A common construct is to consider \emph{independent, identically-distributed}
(or iid) random variables. This means you have a collection of random variables $X_i$
that are all independent of one another but follow the same distribution. This
might be written as
\begin{equation*}
X \stackrel{\text{iid}}{\sim} \mathcal{N}(0, 1)
\end{equation*}
to mean that $f_{X_i}(x)$ is the standard normal pdf for every random variable
$X_i$ and that all the $X_i$ and $X_j$ are independent for all $i \neq j$.
For example, in my card drawing example, if I draw one card, shuffle the deck,
and draw another card, any random variable associated with the cards in the
two draws will be independent and identically distributed.\footnote{I hope
it's clear that ``independent'' and ``identically-distributed'' are not
contradictory: the only knowledge you could potentially get from the outcomes
$X_1, X_2, \ldots X_n$ that would tell you something about the as-of-yet
hidden value of $X_{n+1}$ is what distribution the $X_i$ are drawn from. This,
however, is not a secrete when you say that ``the $X_i$ are iid standard
normal variables''. Later on we will definitely get into how you use the
values taken on by $X_i$ to try to guess what distribution they come. That's
the point of inferential statistics!}

If some random variables are independent and indentically distributed, then
their histogram approximates the probability distribution function. To see why
this is, remember that, at least in the frequentist scheme, the proportion of
``trials'', which we can now more concretely associate with the individual
random variables in this group of iid random variables, that fall into some
range $[a, b]$ is the probability of \emph{each} of the identically-distributed
random variables taking on values in that range, which is $\prob{a
< X \leq b} = \int_a^b f_X(x') \,dx'$. When we talk about ``drawing from a
normal distribution'', what we're talking about in a rigorous mathematical
sense is examining a collection of iid variables with that distribution.

\section{Sums and products of random variables}

If you know the cdf and pdf for a random variable $X$, and you also know these
values for another random variable $Y$, you might be interested in the
behavior of these variables together.\footnote{An important example is the sum
of many independent, identically-distributed random variables, which we will
see are intimately linked to the normal distribution.} Many specific
distributions have nice shortcuts for figuring out the behavior of sums of
random variables with those distributions.\footnote{For example, if $X \sim
\mathcal{N}(\mu_1, \sigma_1^2)$ and $Y \sim \mathcal{N}(\mu_2, \sigma_2^2)$,
then $X + Y \sim \mathcal{N}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$.} Where
do those nice rules come from?

First, let's be clear about the notation. If $X$ and $Y$ are random variables,
then what does $Z = X + Y$ mean? Remember that $X$ and $Y$ are both functions,
so the plus sign in $X + Y$ doesn't mean the same thing as the plus sign in $1
+ 2$. Instead, it's an intuitive shorthand:
\begin{equation}
Z = X + Y \implies Z[\omega] = X[\omega] + Y[\omega],
\end{equation}
that is, for every outcome $\omega$, $Z$ takes on the sum of the values that
$X$ and $Y$ take on. More pedantically, $Z$ is a function that, for every input
$\omega$, outputs the sum of the outputs of $X$ and $Y$ when they in turns are
fed the input $\omega$.

This means that, for some event, if $X$ takes on the value $x$ and $Y$ takes
on the value $y$, then $Z$ takes on $x+y$. Reversing that, we can say that, if
$Z$ takes on the value $z$ and $X$ takes on $x$, then it must be that $Y$
takes on $z - x$. This is how we write the pmf for a discrete $Z = X + Y$ when
$X$ and $Y$ are independent:
\begin{align*}
Z = X + Y \implies f_Z(z) &= \sum_i \prob{(X=x_i) \cap (Y=z - x_i)} \\
  &= \sum_i \prob{X=x_i} \prob{Y=z-x_i} \text{ (independence)} \\
  &= \sum_i f_X(x_i) f_Y(z - x_i)
\end{align*}
Note that $f_Y(z-x_i)$ could be zero for many cases. For example, if $X$ and
$Y$ are two die rolls, then $f_Z(5)$ if $X$ was 6 then $Y$ would have to be $5
- 6 = -1$, which is clearly impossible.

A similar definition holds for when $Z = X + Y$ is continuous. Rather than
summing over all the outcomes in which $X$ takes on certain values, we integrate:
\begin{equation}
f_Z(z) = \int f_X(x') f_Y(z - x') \,dx'
\end{equation}
The right way to derive this is to start with the cdf for $Z$, but it's pretty
boring algebraic manipulation, so you can trust that it works out.

The process is similar for a product of random variables $Z = XY$, expect
that, if $Z = z$ and $X = x$, then it must be that $Y = z/x$. You can also
take a ratio of random variables $Z = X/Y$, which will have terms where $Y =
zx$. The math behind all these can be a real pain, and cases in which there is
a nice syllogism like ``if $X$ is whatever distributed and $Y$ is whatever
distributed, then $Z = X + Y$ is whatever distributed'' are glorious
exceptions.

I want you to see that, even if the math showing some fact about the sum,
product, or ratio of random variables is hard---for example, the ratio of two
normally distributed variables is \emph{not} normally distributed---there's
nothing mysterious about where the fact comes from, or how you could derive
your own facts like that yourself, if you had the time and algebraic ability.

\section{Properties of random variables}

Two properties of random variables, their \emph{expected values} and
\emph{variances}, will come up repeatedly.

\subsection{Expected value (but don't always expect it)}

A random variable's \emph{expected value} is a sort of ``average'' value of
the random variable. For a discrete variable, you find the weighted average of
the values the random variable takes on, where the weights are the probability
that that value comes up. For a continuous random variable, you need to
integrate:
\begin{equation}
\expect{X} = \begin{dcases}
  \sum_i x_i f_X(x_i) & \text{ for discrete random variables} \\
  \int x' f_X(x') \,dx' & \text{ for continuous random variables}
\end{dcases}
\end{equation}
where the $i$ refer to the different values taken by $X$ and the limits of the
integral are the limits of the values taken on by $X$. You might also see
people use notation like $E[X]$ or $\mathbb{E}X$ or, for reasons that will
become confusing later, $\overline{X}$.

The name ``expected value'' is a little confusing. We previously noted that,
for continuous random variables, the probability of getting any particular
number is essentially zero, so there's no particular number you should
``expect''. For discrete random variables, where there is finite probability
of getting each particular number, the expected value need not be any of the
values actually taken on by $X$. In the example of rolling a die, each of the
numbers 1 through 6 arises with equal probability, so:
\begin{equation}
\expect{X} = \sum_{k=1}^6 k f_X(k) = \sum_{k=1}^6 k \times \frac{1}{6} = 3.5.
\end{equation}
You certainly don't ever expect to roll a 3.5!

What the expected value \emph{is} is a handy mathematical tool and a measure
of ``central tendency'', a rough of measure of what kind of values $X$ is
``centered'' around, although even this is slippery, since, as you might
recall from learning in high school about the difference between mean, mode, and median, what ``the
center'' means is not necessarily obvious. It's worth noting, though, that if
the values $x_i$ taken on by a random variable $X$ are all equiprobable, then
the expected value of $X$ is just the plain-old arithmetic mean of the $x_i$:
\begin{equation}
f_X(x_i) = f_X(x_j) \text{ for all } i, j \implies \expect{X} = \frac{1}{n} \sum_i x_i.
\end{equation}

\hl{proof of linearity of expectation}

Expected values have some nice properties that make them really easy to deal
with. For example, the expected value of a shifted random variable like $X +
1$ is just $\expect{X} + 1$: you're still ``averaging'' $X$ overall the
outomes, but you're just adding 1 to everything you averaged, which is clearly
just the ``average'' $X$ plus 1. Similarly, you should be able to see that
$\expect{2X} = 2 \,\expect{X}$.

You could think of ``1'' in the previous example as being a really boring
random variable: it gives 1 for every outcome. This might lead you to suspect
that $\expect{X + Y} = \expect{X} + \expect{Y}$, which is true. The fact that
you can easily transform the expected values of random variables multiplied by
numbers and the sum of random variables is called the \emph{linearity of expectation}:
\begin{equation}
\expect{aX + bY} = a \,\expect{X} + b \,\expect{Y}.
\end{equation}

Perhaps surprisingly, this property holds whether $X$ and $Y$ are independent
or not. Here's the proof: We previously showed that if $Z = X + Y$, then
\begin{equation}
f_Z(z) = \sum_i f_X(x_i) f_Y(z - x_i).
\end{equation}
With some re-arranging, you can show that this is equivalent to
\begin{equation}
f_Z(z) = \sum_i \sum_j (x_i + y_j) f_X(x_i) f_Y(y_j).
\end{equation}
Split this up into two sums:
\begin{align*}
f_Z(z) &= \sum_i \sum_j x_i f_X(x_i) f_Y(y_j) + \sum_i \sum_j y_j f_X(x_i) f_Y(y_j) \\
  &= \left(\sum_i x_i f_X(x_i)\right) \left(\sum_j f_Y(y_j)\right) +
    \left(\sum_j y_j f_Y(y_j)\right) \left(\sum_i f_X(x_i)\right).
\end{align*}
You'll recall that summing up the pmf gives 1, that is, $\sum_i x_i f_X(x_i) =
1$, so the second and fourth bits are just 1. Then notice that the first bit is
just $\expect{X}$ and the third is just $\expect{Y}$.

\hl{Another way to think about this is to say that, even if $X$ and $Y$ depend on
each other,}

\footnote{Here's a problem that sounds really hard, but becomes
really easy with expectation: there are $n$ chickens, standing in a circle.
Suddenly, every chicken pecks the chicken to its left or to its right with
equal probability. How many chickens got pecked?---If you interpret ``how many''
as expected value, then notice that the total number of pecked chickens is the
sum of iid random variables, one for each chicken, with 0 indicating not pecked
and 1 indicating pecked. To be not pecked, the chicken to the left must peck
left, and the chicken to the right must peck right, each with probability
$1/2$, so the expected value for each chicken is $1/4$. The
expected number of pecked chickens is $n/4$. Computing the
\emph{variance} in the number of pecked chickens is definitely harder! \hl{cite}}

Interestingly, the \emph{product} of two random variables is nice only if the
variables are independent. In that case, $\expect{XY} = \expect{X}\,\expect{Y}$.
More generally, there is some residual term call the \emph{covariance}:
\begin{equation}
\expect{XY} = \expect{X} \, \expect{Y} + \cov{X, Y}.
\end{equation}
Before getting too much into covariance, we should start with regular variance!

\subsection{Variance}

If the expected value is some measure of position, then \emph{variance} is the
corresponding measurement of spread. For a random variable $X$, the variance
is the expected value of a random variable $(X - \expect{X})^2$.\footnote{We
learned about how to take the product of random variables, so this is totally
kosher: we start with $X$, then make a new random variable $X - \expect{X}$,
and then look at a third random variable $(X - \expect{X})^2$.} In words, this
is the expectation value (i.e., some kind of sense of position or magnitude)
or the square of the deviations of $X$ from \emph{its} expected value. I'll
write variance with a weird V:
\begin{equation}
\var{X} \defeq \expect{(X - \expect{X})^2}.
\end{equation}
You might also see notation like $\mathrm{Var}[X]$ or $V(X)$, or, for reasons
that will become confusing later on, $\sigma^2_X$. Again, I use the square brackets
to emphasize that $\mathbb{V}$ is not a function over numbers: it takes random
variables and gives back a number.

Note that $\var{X} \geq 0$, since it's an expected value over squares, which
are nonnegative.\footnote{The uninteresting case $\var{X} = 0$ means that $X$
always takes on the same value---that is, that it's a constant.}

Unlike the expected value, variance is not linear. Consider the random variable $aX$,
where $a$ is just a number:
\begin{equation}
\var{aX} = \expect{(aX - \expect{aX})^2} = \expect{a^2(X - \expect{X})} = a^2 \expect{X}.
\end{equation}
For a sum, the situation is even more confusing. First I'll note that
\begin{equation}
\var{X} = \expect{X^2 - 2X \expect{X} - \expect{X}^2}
  = \expect{X^2} - \expect{X}^2.
\end{equation}
In words, the variance is the expectation of the square minus the square of
the expectation. I did this so I could more easily expand out:
\begin{align*}
\var{X + Y} &= \expect{(X+Y)^2} - \expect{X+Y}^2 \\
  &= \expect{X^2} + \expect{Y^2} + 2\, \expect{XY} - \left(\expect{X}+\expect{Y}\right)^2 \\
  &= \left(\expect{X^2} - \expect{X}^2\right) +
    \left(\expect{Y^2} - \expect{Y}^2\right) +
    2 \left(\expect{XY} - \expect{X} \, \expect{Y}\right) \\
  &= \var{X} + \var{Y} + 2 \cov{X, Y}
\end{align*}
where $\cov{X, Y}$, introduced in the end of the last section, is the
\emph{covariance} of $X$ and $Y$. I think it's way easier to write it out as
\begin{equation}
\cov{X, Y} \defeq \expect{(X - \expect{X})(Y - \expect{Y})}.
\end{equation}
In other words, the covariance is the expected value of the product of the
deviations of $X$ and $Y$ from their expected values.

\subsection{Correlation}

Rather than covariance, you're probably more
familiar with the term \emph{correlation}, especially the familiar Pearson's
correlation coefficient, usually written $\rho$, which is
\begin{equation}
\rho[X, Y] \defeq \frac{\cov{X, Y}}{\sqrt{\var{X} \var{Y}}}.
\end{equation}
I want to emphasize that we're still talking about random variables, which are
functions, and not numbers, the ``realizations'' of random variables. So, as I
wrote it, $\rho$ is a \emph{function} that takes two random variables, which
are themselves functions, and returns a number.\footnote{This is different
from the correlation coefficient as you're probably used to it, which is
computed as a number from a set of numbers. If you find this curious, don't
worry, that's what the next chapter is about!}

To see how covariance and correlation are related, think about a correlated
$X$ and $Y$, like a simple bivariate normal distribution. \hl{figure} In
general, the points for which $X$ is greater than its mean value are the same
points for which $Y$ is greater than its mean value. Similarly, when one
deviation is negative, the other tends to be negative. This way, the overall
\emph{product} of the two deviations tends to be positive. In constrast, if
two variables are \emph{anticorrelated}, then when one is higher than average
the other tends to be lower than average, so the product of deviations tends
to be negative.

The reason for dividing by the two variances was so that $\rho$ is between
$-1$ and 1. To see that that's true, define the ``standardized'' random
variable \begin{equation} X' = \frac{X - \expect{X}}{\sqrt{\var{X}}}
\end{equation} which has has mean 0 and variance 1. Similarly define $Y'$, and
then check that $\rho[X, Y] = \rho[X', Y'] = \expect{X'Y'}$. Now make another
random variable $Z = X' - \rho Y'$.\footnote{If you're familiar with linear
regression, notice that $Z$ is like the residuals that come from predicting
$X'$ using independent variable $Y'$ and slope $\rho$: $\rho Y'$ are the
predicted values. The fact that $\var{Z} = 1 - \rho^2$ is why sometimes people
call $\rho^2$, or $R^2$, the ``fraction of variance explained'' by the
regression. If $\rho$ is 1, then $Z$, the residuals, are a constant zero:
you've ``explained'' all the variation.} Its variance is
\begin{align*}
\var{Z} &= \expect{(X' - \rho Y')^2} \\
  &= \expect{X'^2 - 2 \rho X' Y' + \rho^2 Y'^2} \\
  &= \expect{X'^2} - 2\rho\,\expect{X'Y'} + \rho^2 \expect{Y'^2} \\
  &= 1 - 2\rho^2 + \rho^2 \\
  &= 1 - \rho^2
\end{align*}
Variances are always nonnegative, so $1 - \rho^2 \geq 0$, from which you can see
that $-1 \leq \rho \leq 1$.\footnote{This proof is a specific case of the
Cauchy-Schwartz inequality.}
