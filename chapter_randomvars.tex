%!TEX root=main.tex

\chapter{Random variables}

Now that we know how to ``and'' and ``or'' events, we want to be able to do
something more than ask what's going to happen or not. We need a mathematical
bridge between the abstract stuff we just did and the familiar concepts like
means, standard deviations, and probability distributions. The bridge is
\emph{random variables}. What does it mean to say that some ``thing'' is
normally distributed? We'll get there in this section.

\section{Definition of random variables}

As a toy example, consider rolling a six-sided die. There are six outcomes of
interest. We want to associate each of those outcomes with a number.
Mathematically, we call that association a random variable: it is a function
that takes outcomes and returns numbers:
$$
X : \Omega \to \mathbb{R},
$$
that is, a random variable $X$ is a function from the space of outcomes to the
real numbers.\footnote{Again, this is a simplification: measure theory says
that a random variable can map the outcome space to any \emph{measurable
space}.} For example, $X$ takes the event ``two dots came up'' and associates
it with the number 2.

I've now given you what seems like a lot of weird mathematical symbology to do something really natural. If $X$ is the number that comes up from a die roll, then
$$
\prob{X = 2} = \frac{1}{6},
$$
where you'll notice that ``$X = 2$'' is an \emph{event}: it's the
set of outcomes for which $X$ returns the value 2.

This example probably seems pretty dopey, since there's only one outcome that
$X$ maps to 2. The important thing that just happened is that we were able to
abstract over the outcomes $\Omega$ and instead look at just the numbers
popping out of the random variable $X$. It will be nice to say ``$X$ is
normally distributed'' without having to figure out exactly which outcomes $X$
is mapping to what numbers. We're interested in the behavior of $X$ and not
necessarily in the $\Omega$: you'll want to develop statistical tools that
will work for anything old thing that's normally distributed, and you don't
want to be developing a new theory of probability each time.

A slightly less dopey example is: when I draw cards from the deck, my random
variable is 1 for Ace, the rank of the card for cards 2 through 10, and 10 for
Jack, Queen, and King. In this case $\prob{X = 10} = \tfrac{16}{52}$.

If you're asking ``then why did we bother with last chapter'', note that the
rules for manipulating events and probabilities will work just as well for
events like ``$X = 2$'' as they did for ``drew a Jack''. The philosophical and
technical definition of probability still holds. For example, $\prob{(X = 9)
\cup (X = 10)}$, the probability that $X$ takes on the values 9 or 10, is the
sum of the two constituent probabilities $\prob{X=9}$ and
$\prob{X=10}$.\footnote{We know that $X=9$ and $X=10$ are disjoint because
every event can be associated with only one random variable value.}

\section{Distribution of random variables}

Now we're talking about something you've probablty thought about: how is $X$
``distributed''? If you're like me, you think in terms of \emph{probability
distribution functions} (pdf's) and consider \emph{cumulative distribution
functions} (cdf's) as a derived quantity, but in fact it's mathematically
easier to go the reverse route. (If you aren't familiar with either, never
fear, this section is here!)

\subsection{Cumulative distribution functions}

The first way of defining the ``distribution'' of a random variable $X$ is its
\emph{cumulative distribution function}\footnote{For reasons that will become
clear, these are also called cumulative \emph{density} functions. They mean
the same thing.} (``cdf'' for short), which is a function\footnote{Finally, a
function that's actually named ``function''!} that takes a number $x$ and
returns the probability that $X$ is at most $x$:\footnote{I use the regular
parentheses with $F_X$ because it's the kind of function that I expect you
normally think of as a function: it takes a number and returns a number. I use
the square brackets for functions like $\mathbb{P}$ to emphasize that they
take things that aren't numbers.}
$$
F_X(x) \defeq \prob{X \leq x},
$$
where $F_X$ is the cdf of $X$. This is the typical notation.

Take careful note that $X$ is a function, $x$ is a number, and
``$X \leq x$'' is an event. In other words, ``$\leq$'' here
means something different\footnote{In computer programming this kind of
behavior is called \emph{polymorphism}. In some languages, like Python,
\texttt{1 + 2} means 3 but \texttt{"hello " + "world"} means ``hello world''.
In one case, \texttt{+} is adding numbers and in the other case it's concatenating
strings. So to understand what action is being taken, you need to look not
only at the function, but also what stuff is being fed to it.} than in an
inequality with two numbers (e.g., $1 \leq 2$).\footnote{Indeed, $\prob{1 \leq
2} = 1$!} This may seem pedantic, but I think having a really good grasp of
the notation will help you articulate correct thoughts more clearly. Things
will quickly get confusing if you think that $X \leq x$ means the same thing
as $x \leq x$!

If the number of outcomes is finite (e.g., for a die roll, there are six
outcomes), then $X$ takes on a finite number of values (at most, the number of
outcomes), and it's pretty easy to say in words what $F_X$ is doing:
it looks at every outcome, sees if $X$ maps that outcome to a value less than
or equal to the number $x$, and adds up the probabilities of just those
outcomes. If $X$ takes on a finite number of values, we call it a
\emph{discrete} random variable.\footnote{Note, however, that $X$ can take on
a finite number of values even if the number of outcomes is \emph{not} finite.
There might be an infinite number of outcomes---say, one for each of the real
numbers between 0 and 6---and the random variable returns an integer between 1
and 6---just the real number associated with each outcome, rounded up. The
combination of that space of outcomes and that random variable gives exactly
the same behavior as the simple $X$ on die rolls. That's part of the beauty of
using random variables rather than just raw events.}

If $X$ is a discrete random variable, and it takes on minimum value
$x_\mathrm{min}$ and maximum value $x_\mathrm{max}$, then it should be clear
that
\begin{gather}
F_X(x_\mathrm{min}) = \prob{X = x_\mathrm{min}} \\
F_X(x_\mathrm{max}) = 1
\end{gather}
If $X$ takes on an infinite number of values, it is an \emph{continuous}
random variable. If those values are bounded---the common example being a
uniform random variable that takes values between 0 and 1---then the same
rules apply.
If $X$ takes on every real value (i.e., any number from $-\infty$ to $\infty$),
then
\begin{gather}
\lim_{x \to -\infty} F_X(x) = 0 \\
\lim_{x \to \infty} F_X(x) = 1
\end{gather}

\subsection{Probability distribution function}

The cumulative distribution function, though relative easy to define, isn't
always very nice to look at and think about. For discrete random variables, it's
often just as easy to use the values $\prob{X = x}$, which is called the \emph{probability mass function} (or ``pmf'') of $X$:
\begin{equation}
f_X(x) \defeq \prob{X = x}.
\end{equation}
Note that, for a discrete random variable $X$, $F_X$ means the cdf and $f_X$
means the pdf. This function takes numbers---the values that $X$ takes on---
and returns probabilities, which are always between 0 and 1.

What's the analogue of the pmf for continuous random variables? It's tempting,
but ultimately incorrect, to just write $\prob{X = x}$ again. This may seem
like a pedantic diversion, but I actually think it's important to avoid
confusion.

For a continuous random variable, $\prob{X = x}$ is zero. For example, say $X$
takes on values between 0 and 1 symmetrically, e.g., the probability that $X$
comes out between 0 and 0.5 is the same as between 0.5 and 1. Say you're
asking about the probability that $X$ comes out as 0.5. I'll grant the
following:
\begin{align*}
\prob{0 \leq X \leq 1} &= 1 \\
\prob{4.95 \leq X \leq 5.05} &= 0.1 \\
\prob{4.995 \leq X \leq 5.005} &= 0.01 \\
\end{align*}
and so on. We normally don't say that $\prob{X = 0.500\ldots} = 0$, since that
makes it sound like it's not allowed for $X$ to take on 0.5. But it should be
clear that, from any practical point of view, it doesn't make sense to write
things like $\prob{X = 0.5}$.

So whereas the pmf gives actually probabilities, for continuous random
variables we need something else. The solution is the \emph{probability
distribution function} (or pdf), which, when integrated, gives probabilities.
The pdf is also written as $f_X$, and it's the derivative of the cdf:
\begin{equation}
f_X(x) \defeq \frac{d}{dx} F_X(x).
\end{equation}
The other name for the pdf is the probability \emph{density} function, which I
actually like a little better, because it emphasizes that the pdf is something
that, when integrated, gives probabilities:
\begin{equation}\label{eq:integrated_pdf}
\prob{x_0 < X \leq x_1} = \int_{x_0}^{x_1} f_X(x') \,dx'
\end{equation}
If $X$ can take on any value, then it follows from the definition of the cdf and
some fundamental calculus that
\begin{equation}
\int_{-\infty}^\infty f_X(x') \,dx' = 1
\end{equation}
that is, that all the probability must be somewhere, and
\begin{equation}
\lim_{x \to \pm \infty} = 0.
\end{equation}

These definitions I've given are simple ones, and they need to be refined to
deal with more complicated functions.\footnote{For example, if the cdf has
discontinuities, then you need to be more careful with the limits on the
integrals. That's why there's one ``$<$'' and one ``$\leq$'' in \eqref{eq:integrated_pdf}.}

\subsection{Examples of distributions}

In yet more notation, we use a tilde to denote that a random variable
``follows'' a certain distribution, that is, that it has a pdf and cdf that
we've already heard of. The familiar example is the normal distribution, which
is so common that we write it with just one letter, so that $X \sim
\mathcal{N}(\mu, \sigma^2)$ is a shorthand for
\begin{equation*}
f_X(x) = f_\mathcal{N}(x; \mu, \sigma^2) \defeq
  \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left\{-\frac{1}{2} \frac{(x-\mu)^2}{\sigma^2} \right\}.
\end{equation*}
Similarly $X \sim \mathrm{Binom}(n, p)$ means that $X$ follows a binomial distribution:
\begin{equation*}
f_X(x) = f_\mathrm{binom}(x; n, p) \defeq \binom{n}{x} x^p (n-x)^{1-p}.
\end{equation*}
We'll come back to these distributions and their pdf's later; I just want to
you understand that, when we say that ``$X$ is normally distributed'', we
making a mathematically precise statement about $X$'s pdf and cdf.

A common construct is to consider \emph{independent, identically-distributed}
(or iid) variables. This means you have a collection of random variables $X_i$
that are all independent of one another but follow the same distribution. This
might be written as
\begin{equation*}
X \stackrel{\text{iid}}{\sim} \mathcal{N}(0, 1)
\end{equation*}
to mean that $f_{X_i}(x)$ is the standard normal pdf for every random variable
$X_i$ and that all the $X_i$ and $X_j$ are independent for all $i \neq j$. For
example, in my card drawing example, if I draw one card, shuffle the deck, and
draw another card, any random variable associated with the cards in the two
draws will be independent and identically distributed.

If some random variables are independent and indentically distributed, then
their histogram approximates the probability distribution function. To see why
this is, remember that, at least in the frequentist scheme, the proportion of
``trials'', which we can now more concretely associate with the individual
random variables in this group of iid random variables, that fall into some
range $[a, b]$ is the probability of \emph{each} of the identically-
distributed random variables taking on values in that range, which is $\prob{a
< X \leq b} = \int_a^b f_X(x') \,dx'$. When we talk about ``drawing from a
normal distribution'', what we're talking about in a rigorous mathematical
sense is examining a collection of iid variables with that distribution.

\section{Sums and products of random variables}

If you know the cdf and pdf for a random variable $X$, and you also know these
values for another random variable $Y$, you might be interested in the
behavior of these variables together.\footnote{An important example is the sum
of many independent, identically-distributed random variables, which we will
see are intimately linked to the normal distribution.} Many specific
distributions have nice shortcuts for figuring out the behavior of sums of
random variables with those distributions.\footnote{For example, if $X \sim
\mathcal{N}(\mu_1, \sigma_1^2)$ and $Y \sim \mathcal{N}(\mu_2, \sigma_2^2)$,
then $X + Y \sim \mathcal{N}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$.} Where
do those nice rules come from?

First, let's be clear about the notation. If $X$ and $Y$ are random variables,
then what does $Z = X + Y$ mean? Remember that $X$ and $Y$ are both functions,
so the plus sign in $X + Y$ doesn't mean the same thing as the plus sign in $1
+ 2$. Instead, it's an intuitive shorthand:
\begin{equation}
Z = X + Y \implies Z[\omega] = X[\omega] + Y[\omega],
\end{equation}
that is, for every outcome $\omega$, $Z$ takes on the sum of the values that
$X$ and $Y$ take on.

This means that, for some event, if $X$ takes on the value $x$ and $Y$ takes
on the value $y$, then $Z$ takes on $x+y$. Reversing that, we can say that, if
$Z$ takes on the value $z$ and $X$ takes on $x$, then it must be that $Y$
takes on $z - x$. This is how we write the pmf for a discrete $Z = X + Y$ when
$X$ and $Y$ are independent:
\begin{align*}
Z = X + Y \implies f_Z(z) &= \sum_i \prob{(X=x_i) \cap (Y=z - x_i)} \\
  &= \sum_i \prob{X=x_i} \prob{Y=z-x_i} \text{ (independence)} \\
  &= \sum_i f_X(x_i) f_Y(z - x_i)
\end{align*}
Note that $f_Y(z-x_i)$ could be zero for many cases. For example, if $X$ and
$Y$ are two die rolls, then $f_Z(5)$ if $X$ was 6 then $Y$ would have to be $5
- 6 = -1$, which is clearly impossible.

A similar definition holds for when $Z = X + Y$ is continuous. Rather than
summing over all the outcomes in which $X$ takes on certain values, we integrate:
\begin{equation}
f_Z(z) = \int f_X(x') f_Y(z - x') \,dx'
\end{equation}
The right way to derive this is to go back to the cdf for $Z$, but you can
trust that it works out.

The process is similar for a product of random variables $Z = XY$, expect
that, if $Z = z$ and $X = x$, then it must be that $Y = z/x$. You can also
take a ratio of random variables $Z = X/Y$, which will have terms where $Y =
zx$. The math behind all these can be a real pain, but I want you to see that
there's nothing mysterious about it.

\section{Properties of random variables}

Two properties of random variables, their \emph{expected values} and
\emph{variances}, will come up repeatedly.

\subsection{Expected value (but don't always expect it)}

A random variable's \emph{expected value} is a sort of ``average'' value of
the random variable. For a discrete variable, you find the weighted average of
the values the random variable takes on, where the weights are the probability
that that value comes up. For a continuous random variable, you need to
integrate:
\begin{equation}
\expect{X} = \begin{dcases}
  \sum_i x_i f_X(x_i) & \text{ for discrete random variables} \\
  \int x' f_X(x') \,dx' & \text{ for continuous random variables}
\end{dcases}
\end{equation}
where the $i$ refer to the different values taken by $X$ and the limits of the
integral are the limits of the values taken on by $X$. You might also see
people use notation like $E[X]$ or $\mathbb{E}X$ or, for reasons that will
become confusing later, $\overline{X}$.

The name ``expected value'' is a little confusing. We previously noted that,
for continuous random variables, the probability of getting any particular
number is essentially zero, so there's no particular number you should
``expect''. For discrete random variables, where there is finite probability
of getting each particular number, the expected value need not be any of the
values actually taken on by $X$. In the example of rolling a die, each of the
numbers 1 through 6 arises with equal probability, so:
\begin{equation}
\expect{X} = \sum_{k=1}^6 k f_X(k) = \sum_{k=1}^6 k \times \frac{1}{6} = 3.5.
\end{equation}
You certainly don't ever expect to roll a 3.5!

What the expected value \emph{is} is a handy mathematical tool and a measure
of ``central tendency'', a rough of measure of what kind of values $X$ is
``centered'' around, although even this is slippery, since, as you might
recall from learning in high school about the difference between mean, mode, and median, what ``the
center'' means is not necessarily obvious. It's worth noting, though, that if
the values $x_i$ taken on by a random variable $X$ are all equiprobable, then
the expected value of $X$ is just the plain-old arithmetic mean of the $x_i$:
\begin{equation}
f_X(x_i) = f_X(x_j) \text{ for all } i, j \implies \expect{X} = \frac{1}{n} \sum_i x_i.
\end{equation}

\hl{proof of linearity of expectation}

Expected values have some nice properties that make them really easy to deal
with. For example, the expected value of a shifted random variable like $X +
1$ is just $\expect{X} + 1$: you're still ``averaging'' $X$ overall the
outomes, but you're just adding 1 to everything you averaged, which is clearly
just the ``average'' $X$ plus 1. Similarly, you should be able to see that
$\expect{2X} = 2 \,\expect{X}$.

You could think of ``1'' in the previous example as being a really boring
random variable: it gives 1 for every outcome. This might lead you to suspect
that $\expect{X + Y} = \expect{X} + \expect{Y}$, which is true. The fact that
you can easily transform the expected values of random variables multiplied by
numbers and the sum of random variables is called the \emph{linearity of expectation}:
\begin{equation}
\expect{aX + bY} = a \,\expect{X} + b \,\expect{Y}.
\end{equation}

Perhaps surprisingly, this property holds whether $X$ and $Y$ are independent
or not. Here's the proof: We previously showed that if $Z = X + Y$, then
\begin{equation}
f_Z(z) = \sum_i f_X(x_i) f_Y(z - x_i).
\end{equation}
With some re-arranging, you can show that this is equivalent to
\begin{equation}
f_Z(z) = \sum_i \sum_j (x_i + y_j) f_X(x_i) f_Y(y_j).
\end{equation}
Split this up into two sums:
\begin{align*}
f_Z(z) &= \sum_i \sum_j x_i f_X(x_i) f_Y(y_j) + \sum_i \sum_j y_j f_X(x_i) f_Y(y_j) \\
  &= \left(\sum_i x_i f_X(x_i)\right) \left(\sum_j f_Y(y_j)\right) +
    \left(\sum_j y_j f_Y(y_j)\right) \left(\sum_i f_X(x_i)\right).
\end{align*}
You'll recall that summing up the pmf gives 1, that is, $\sum_i x_i f_X(x_i) =
1$, so the second and fourth bits are just 1. Then notice that the first bit is
just $\expect{X}$ and the third is just $\expect{Y}$.

\hl{Another way to think about this is to say that, even if $X$ and $Y$ depend on
each other,}

\footnote{Here's a problem that sounds really hard, but becomes
really easy with expectation: there are $n$ chickens, standing in a circle.
Suddenly, every chicken pecks the chicken to its left or to its right with
equal probability. How many chickens got pecked?---If you interpret ``how many''
as expected value, then notice that the total number of pecked chickens is the
sum of iid random variables, one for each chicken, with 0 indicating not pecked
and 1 indicating pecked. To be not pecked, the chicken to the left must peck
left, and the chicken to the right must peck right, each with probability
$1/2$, so the expected value for each chicken is $1/4$. The
expected number of pecked chickens is $n/4$. Computing the
\emph{variance} in the number of pecked chickens is definitely harder! \hl{cite}}

Interestingly, the \emph{product} of two random variables is nice only if the
variables are independent. In that case, $\expect{XY} = \expect{X}\,\expect{Y}$.
More generally, there is some residual term call the \emph{covariance}:
\begin{equation}
\expect{XY} = \expect{X} \, \expect{Y} + \cov{X, Y}.
\end{equation}
Before getting too much into covariance, we should start with regular variance!

\subsection{Variance}

If the expected value is some measure of position, then \emph{variance} is the
corresponding measurement of spread. For a random variable $X$, the variance
is the expected value of a random variable $(X - \expect{X})^2$.\footnote{We
learned about how to take the product of random variables, so this is totally
kosher: we start with $X$, then make a new random variable $X - \expect{X}$,
and then look at a third random variable $(X - \expect{X})^2$.} In words, this
is the expectation value (i.e., some kind of sense of position or magnitude)
or the square of the deviations of $X$ from \emph{its} expected value. I'll
write variance with a weird V:
\begin{equation}
\var{X} \defeq \expect{(X - \expect{X})^2}.
\end{equation}
You might also see notation like $\mathrm{Var}[X]$ or $V(X)$, or, for reasons
that will become confusing later on, $\sigma^2_X$. Again, I use the square brackets
to emphasize that $\mathbb{V}$ is not a function over numbers: it takes random
variables and gives back a number.

Note that $\var{X} \geq 0$, since it's an expected value over squares, which
are nonnegative.\footnote{The uninteresting case $\var{X} = 0$ means that $X$
always takes on the same value---that is, that it's a constant.}

Unlike the expected value, variance is not linear. Consider the random variable $aX$,
where $a$ is just a number:
\begin{equation}
\var{aX} = \expect{(aX - \expect{aX})^2} = \expect{a^2(X - \expect{X})} = a^2 \expect{X}.
\end{equation}
For a sum, the situation is even more confusing. First I'll note that
\begin{equation}
\var{X} = \expect{X^2 - 2X \expect{X} - \expect{X}^2}
  = \expect{X^2} - \expect{X}^2.
\end{equation}
In words, the variance is the expectation of the square minus the square of
the expectation. I did this so I could more easily expand out:
\begin{align*}
\var{X + Y} &= \expect{(X+Y)^2} - \expect{X+Y}^2 \\
  &= \expect{X^2} + \expect{Y^2} + 2\, \expect{XY} - \left(\expect{X}+\expect{Y}\right)^2 \\
  &= \left(\expect{X^2} - \expect{X}^2\right) +
    \left(\expect{Y^2} - \expect{Y}^2\right) +
    2 \left(\expect{XY} - \expect{X} \, \expect{Y}\right) \\
  &= \var{X} + \var{Y} + 2 \cov{X, Y}
\end{align*}
where $\cov{X, Y}$, introduced in the end of the last section, is the
\emph{covariance} of $X$ and $Y$. I think it's way easier to write it out as
\begin{equation}
\cov{X, Y} \defeq \expect{(X - \expect{X})(Y - \expect{Y})}.
\end{equation}
In other words, the covariance is the expected value of the product of the
deviations of $X$ and $Y$ from their expected values.

\subsection{Correlation}

Rather than covariance, you're probably more
familiar with the term \emph{correlation}, especially the familiar Pearson's
correlation coefficient, usually written $\rho$, which is
\begin{equation}
\rho[X, Y] \defeq \frac{\cov{X, Y}}{\sqrt{\var{X} \var{Y}}}.
\end{equation}
I want to emphasize that we're still talking about random variables, which are
functions, and not numbers, the ``realizations'' of random variables. So, as I
wrote it, $\rho$ is a \emph{function} that takes two random variables, which
are themselves functions, and returns a number.\footnote{This is different
from the correlation coefficient as you're probably used to it, which is
computed as a number from a set of numbers. If you find this curious, don't
worry, that's what the next chapter is about!}

To see how covariance and correlation are related, think about a correlated
$X$ and $Y$, like a simple bivariate normal distribution. \hl{figure} In
general, the points for which $X$ is greater than its mean value are the same
points for which $Y$ is greater than its mean value. Similarly, when one
deviation is negative, the other tends to be negative. This way, the overall
\emph{product} of the two deviations tends to be positive. In constrast, if
two variables are \emph{anticorrelated}, then when one is higher than average
the other tends to be lower than average, so the product of deviations tends
to be negative.

The reason for dividing by the two variances was so that $\rho$ is between
$-1$ and 1. To see that that's true, define the ``standardized'' random
variable \begin{equation} X' = \frac{X - \expect{X}}{\sqrt{\var{X}}}
\end{equation} which has has mean 0 and variance 1. Similarly define $Y'$, and
then check that $\rho[X, Y] = \rho[X', Y'] = \expect{X'Y'}$. Now make another
random variable $Z = X' - \rho Y'$.\footnote{If you're familiar with linear
regression, notice that $Z$ is like the residuals that come from predicting
$X'$ using independent variable $Y'$ and slope $\rho$: $\rho Y'$ are the
predicted values. The fact that $\var{Z} = 1 - \rho^2$ is why sometimes people
call $\rho^2$, or $R^2$, the ``fraction of variance explained'' by the
regression. If $\rho$ is 1, then $Z$, the residuals, are a constant zero:
you've ``explained'' all the variation.} Its variance is
\begin{align*}
\var{Z} &= \expect{(X' - \rho Y')^2} \\
  &= \expect{X'^2 - 2 \rho X' Y' + \rho^2 Y'^2} \\
  &= \expect{X'^2} - 2\rho\,\expect{X'Y'} + \rho^2 \expect{Y'^2} \\
  &= 1 - 2\rho^2 + \rho^2 \\
  &= 1 - \rho^2
\end{align*}
Variances are always nonnegative, so $1 - \rho^2 \geq 0$, from which you can see
that $-1 \leq \rho \leq 1$.\footnote{This proof is a specific case of the
Cauchy-Schwartz inequality.}
