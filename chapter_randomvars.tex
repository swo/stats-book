%!TEX root=main.tex

\chapter{Random variables}

In the previous chapter, we examined outcomes, events, and their
probabilities. Outcomes are specific configurations of atoms, which is not at
all a practical thing for a scientist think about. Events are abstract
groupings of those configurations, which is also impractical. Instead, in
science, we measure the results of experiments with numbers. We link whatever
we see with a number. In microbiology, we could cells. In physics, we measure
the energy of an electron.

In probability theory, \emph{random variables} link outcomes with numbers.
They are the mathematical analogue of scientific measurement. Random variables
will lead us to familiar concepts like means, standard deviations, and
probability distributions.

\section{Definition of random variables}

Confusingly, a ``random variable'' is a function, not a number. We might say
that a random variable ``takes on'' a value, but the random variable itself,
in mathematical terms is always a function.

A \emph{discrete} random variable $X$ is a function that associates outcomes
$\omega \in \Omega$ with a finite set of numbers $x_i$:
\begin{gather*}
X : \Omega \to \{x_1, \ldots x_k\} \\
X[\omega] = x_i
\end{gather*}
for some finite $k$. A \emph{continuous} random variables associates outcomes with real
numbers $x \in \mathbb{R}$:
\begin{gather*}
X : \Omega \to \mathbb{R} \\
X[\omega] = x
\end{gather*}
This is a simplification: measure theory says that a random variable can map
the outcome space to any \emph{measurable space}. We won't dive into that.

For example, when flipping a die, you could imagine a random variable $X$
encoding ``number of heads flipped'':
\begin{align*}
X[H] &= 1 \\
X[T] &= 0
\end{align*}
Roughly speaking, the values of the random variable group the outcomes into
events, which each have probabilities. This means it makes sense to ask about
the event in which a random variable takes on a value, e.g., $\prob{X = 1} = \tfrac{1}{2}$.
Note that ``$X = 1$'' does not mean that $X$ is the number one; instead, the
entire phrase ``$X = 1$'' refers to the event where $X$ took on the value one.

Random variables therefore allow us to abstract over the outcomes and look
only at the values taken on by the random variable. From now on, we will have
no reason to think about the specific configuration of atoms in our
experiment. We only think about the numbers delivered by our apparatus. Soon
we'll be able to say things like ``$X$ is normally distributed'' without having
to specify what are the outcomes and events.

The previous chapter was not a waste, however, because the
rules for manipulating events and probabilities work just as well for
events like ``$X = 1$'' as they did for ``flipped heads''. The philosophical and
technical definition of probability still holds. For example, $\prob{(X = 1)
\cup (X = 0)}$, the probability that $X$ takes on the values zero or one, is the
sum of the two constituent probabilities $\prob{X=1}$ and
$\prob{X=0}$.

\section{Distribution of random variables}

How is $X$ ``distributed''? If you're like me, you think in terms of
\emph{probability distribution functions} (pdf's) and consider
\emph{cumulative distribution functions} (cdf's) as a derived quantity, but in
fact it's mathematically easier to go the reverse route. If you aren't
familiar with either, never fear.

\subsection{Cumulative distribution functions}

The first way of defining the ``distribution'' of a random variable $X$ is its
\emph{cumulative distribution function}, also called the cumulative
\emph{density} function and abbreviated ``cdf''. The cdf of a discrete random
variable $X$ is the probability that the random variable takes on a value
below some threshold $x_i$:
\begin{gather*}
F_X : \{x_1, \ldots, x_k\} \to [0, 1] \\
F_X(x_i) \defeq \prob{X \leq x_i} = \sum_{j \,:\, x_j \leq x_i} \prob{X = x_j}
\end{gather*}
A continuous random variable has a cdf that takes real numbers:
\begin{gather*}
F_X : \mathbb{R} \to [0, 1] \\
F_X(x) \defeq \prob{X \leq x},
\end{gather*}
Note that the cdf is a function of numbers, which I emphasize by
using regular parentheses around $x$ in $F_X(x)$.

Take careful note that ``$X \leq x$'' is an event, not a Boolean expression
like ``$1 \leq 2$''. This may seem pedantic, but I think having a really good
grasp of the notation will help you articulate correct thoughts more clearly.
Things will quickly get confusing if you think that $\prob{X \leq x}$ means the same
thing as $\prob{x \leq x}$.

% swo: need a figure here with the laddering

A discrete random variable takes on a maximum value $x_\mathrm{max}$ so that
\begin{gather}
\text{if } x < x_\mathrm{min} \text{, then } F_X(x) = 0 \\
F_X(x_\mathrm{max}) = 1.
\end{gather}
If a continuous random variable can take on any real number, then  the cdf of
a continuous random variable approaches zero and one as $x$ goes out toward
infinity:
\begin{gather}
\lim_{x \to -\infty} F_X(x) = 0 \\
\lim_{x \to \infty} F_X(x) = 1
\end{gather}

% need a figure here with, say, normal distribution

\subsection{Probability distribution function}

For a discrete random variable, it's straightforward to define its
\emph{probability mass function} or ``pmf'', which is just the probability
that it takes on each discrete value $x_i$:
\begin{gather*}
f_X : \{x_1, \ldots, x_k\} \to [0, 1] \\
f_X(x_i) = \prob{X = x_i}
\end{gather*}

The analogue for continuous random variables is the probability
\emph{distribution} function, also called probability \emph{density} function
or ``pdf'':
\begin{gather*}
f_X(x) : \mathbb{R} \to [0, 1] \\
f_X(x) \defeq \frac{d}{dx} F_X(x)
\end{gather*}

Note that the pdf of a continuous random variable is not $\prob{X = x}$. This
may seem like a pedantic diversion, but I actually think it's important to
avoid confusion. For a continuous random variable, $\prob{X = x}$ is zero. For
example, say $X$ takes on values between 0 and 1 symmetrically, e.g., the
probability that $X$ comes out between 0 and 0.5 is the same as between 0.5
and 1. Say you're asking about the probability that $X$ comes out as 0.5. I'll
grant the following:
\begin{align*}
\prob{0 \leq X \leq 1} &= 1 \\
\prob{0.495 \leq X \leq 0.505} &= 0.1 \\
\prob{0.4995 \leq X \leq 0.5005} &= 0.01 \\
&\vdots
\end{align*}
and so on. We normally don't say that $\prob{X = 0.500\ldots} = 0$, since that
makes it sound like it's impossible for $X$ to take on the value $0.5$.
Nevertheless, it should be clear that, from any practical point of view, it
doesn't make sense to write things like $\prob{X = 0.5}$. It does make sense
to write $f_X(0.5)$; it does not make sense to write $\prob{X =
0.5}$.\footnote{Those dissatisfied with this explanation will need to learn
some measure theory.}

The word ``density'' in probability density function emphasizes that the pdf,
when integrated, gives probabilities:
\begin{equation}\label{eq:integrated_pdf}
\int_{x_0}^{x_1} f_X(x') \,dx' = F_X(x_1) - F_X(x_0) = \prob{x_0 < X \leq x_1}
\end{equation}
If $X$ can take on any value, then it follows from the definition of the cdf and
some fundamental calculus that
\begin{equation}
\int_{-\infty}^\infty f_X(x') \,dx' = 1
\end{equation}
that is, that all the probability must be somewhere, and
\begin{equation}
\lim_{x \to \pm \infty} = 0.
\end{equation}

These definitions I've given are simple ones, and they need to be refined to
deal with more complicated functions. For example, if the cdf has
discontinuities, you need careful with the limits on the integrals.

\subsection{Known distributions}

If a random variable $X$ has the same cdf as another random variable $Y$ (and
therefore also the same pdf), we say that $X$ is ``distributed like'' $Y$:
$$
X \sim Y.
$$
This notation is used to refer to well-documented distributions. For example,
you might say that $X$ is distributed like a normal random variable with mean
$\mu$ and variance $\sigma^2$:
$$
X \sim \mathcal{N}(x; \mu, \sigma^2),
$$
which is a shorthand for saying
\begin{equation*}
f_X(x) = f_\mathcal{N}(x; \mu, \sigma^2) \defeq
  \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left\{-\frac{1}{2} \frac{(x-\mu)^2}{\sigma^2} \right\}.
\end{equation*}
Similarly $X \sim \mathrm{Binom}(n, p)$ means that $X$ follows a binomial distribution:
\begin{equation*}
f_X(x) = f_\mathrm{binom}(x; n, p) \defeq \binom{n}{x} x^p (n-x)^{1-p}.
\end{equation*}
We'll come back to these distributions and their pdf's later; I just want to
you understand that, when we say that ``$X$ is normally distributed'', we
making a mathematically precise statement about $X$'s pdf and cdf.

\subsection{Independent, identically-distributed random variables}

A common construct is to consider \emph{independent, identically-distributed}
(or ``iid'') random variables. This means you have a collection of random variables $X_i$
that all have the same cdf and are all independent of one another. For example,
\begin{equation*}
X \stackrel{\text{iid}}{\sim} \mathcal{N}(\mu, \sigma^2)
\end{equation*}
means that $f_{X_i}(x) = f_\mathcal{N}(x; \mu, \sigma^2)$ for every random variable
$X_i$ and that all the $X_i$ and $X_j$ are independent for all $i \neq j$.

Independent, identically-distributed random variables are important for
frequentist statistics because they represent the ``many independent trials''
in the frequentist definition of probability as a proportion of many
independent trials in which an outcome emerges. As the number of iid variables
increases, the proportion of discrete iid random variables that take on each
discrete value will approach the probability of that value. Similarly, the
proportion of continuous iid random variable that take on values in a range $a
\leq X < b$ will approach $F_X(b) - F_X(a)$.

In day-to-day speech, we might say, ``I draw $n$ values from this random
variable $X$''. Mathematically, this means, ``Consider $n$ iid random
variables distributed like $X$''.

% swo: do events or outcomes get probabilities?

\section{Sums and products of random variables}

If you know the cdf and pdf for a random variable $X$, and you also know these
values for another random variable $Y$, you might be interested in the
behavior of these variables together. An important example is the sum of many
independent, identically-distributed random variables, which we will see are
intimately linked to the normal distribution. Many specific distributions have
nice shortcuts for figuring out the behavior of sums of random variables with
those distributions. For example, if $X \sim \mathcal{N}(\mu_1, \sigma_1^2)$
and $Y \sim \mathcal{N}(\mu_2, \sigma_2^2)$, then $X + Y \sim
\mathcal{N}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$. Where do those nice
rules come from?

First, let's be clear about the notation. If $X$ and $Y$ are random variables,
then what does $Z = X + Y$ mean? Remember that $X$ and $Y$ are both functions,
so the plus sign in $X + Y$ doesn't mean ``add numbers''. Instead, it's an intuitive shorthand:
\begin{equation}
Z = X + Y \text{ means } Z[\omega] = X[\omega] + Y[\omega].
\end{equation}
That is, for every outcome $\omega$, $Z$ takes on the sum of the values that
$X$ and $Y$ take on. In other words, $Z$ is a function that, for every input
$\omega$, outputs the sum of the outputs of $X$ and $Y$ when they in turns are
fed the input $\omega$.

This means that, for some event, if $X$ takes on the value $x$ and $Y$ takes
on the value $y$, then $Z$ takes on $x+y$. It follows that, if
$Z$ takes on the value $z$ and $X$ takes on $x$, then it must be that $Y$
took on $z - x$. Therefore the probability that $Z$ takes on value $z$ is
sum of the probabilities that $X$ took on $x_i$ and $Y$ takes on $z - x_i$.
If $X$ and $Y$ are independent,
$\prob{(X = x_i) \cap (Y = z - x_i)} = \prob{X = x_i} \times \prob{Y = z - x_i}$
so that
\begin{align*}
f_Z(z) &= \sum_i \prob{(X=x_i) \cap (Y=z - x_i)} \\
  &= \sum_i \prob{X=x_i} \prob{Y=z-x_i} \\
  &= \sum_i f_X(x_i) f_Y(z - x_i).
\end{align*}

A similar definition holds for when $Z = X + Y$ is continuous. Rather than
summing over all the outcomes in which $X$ takes on certain values, we integrate:
\begin{equation}
f_Z(z) = \int f_X(x') f_Y(z - x') \,dx'
\end{equation}

For a product of random variables $Z = XY$, we say that,
if $Z = z$ and $X = x$, then it must be that $Y = z/x$.
For a ratio of random variables $Z = X/Y$, it must be that $Y = zx$.

For any particular $X$ and $Y$, the derived variables $X + Y$, $XY$, and $X/Y$
are usually distributed in some really messy way. For example, although the
sum of two normally-distributed random variables is normally distributed,
neither the product nor the ratio of normally-distributed random variables are
normally distributed.

\section{Properties of random variables}

Two properties of random variables, their \emph{expected values} and
\emph{variances}, will come up repeatedly.

\subsection{Don't expect the expected value}

A random variable's \emph{expected value} is the probability-weighted average
of the values it takes on. It is a function that links random variables with
numbers:
\begin{equation*}
\mathbb{E} : \text{random variables} \to \mathbb{R}
\end{equation*}
For a discrete random variable, you simply sum. For a continuous random
variable, you need to integrate:
\begin{equation}
\expect{X} = \begin{dcases}
  \sum_i x_i f_X(x_i) & \text{ for discrete random variables} \\
  \int x' f_X(x') \,dx' & \text{ for continuous random variables}
\end{dcases}
\end{equation}
You might also see people use notation like $E[X]$, $\mathbb{E}X$, or $\mu_X$.

% swo: expected vs. expectation. go with "expected"

The name ``expectation value'' is a little confusing. We previously noted that,
for continuous random variables, the probability of getting any particular
number is essentially zero, so there's no particular number you should
``expect''. For discrete random variables, where there is finite probability
of getting each particular number, the expected value need not be any of the
values actually taken on by $X$. In our coin flip example, the random variable
$X$ measuring the number of heads flipped has expectation value:
\begin{equation}
\expect{X} = 0 \times \prob{T} + 1 \times \prob{H} = \tfrac{1}{2}.
\end{equation}
You certainly don't ever expect to flip $0.5$ heads.

Even if you don't expect the expectation value, it is a handy mathematical
tool, and it is a rough of measure of what kind of values $X$ is
``centered'' around. Even this is slippery, since, as you might
recall from learning in high school about the difference between mean, mode, and median, what ``the
center'' means is not necessarily obvious.

\subsection{Linearity of expectation}

Expected values have some nice properties that make them really easy to deal
with. For example, the expectation value of a shifted random variable like $X +
1$ is just $\expect{X} + 1$: you're still ``averaging'' $X$ overall the
outomes, but you're just adding 1 to everything you averaged, which is clearly
just the ``average'' $X$ plus 1. Similarly, you should be able to see that
$\expect{2X} = 2 \,\expect{X}$.

You could think of ``1'' in the previous example as being a really boring
random variable: it gives 1 for every outcome. This might lead you to suspect
that $\expect{X + Y} = \expect{X} + \expect{Y}$, which is true. The fact that
you can easily transform the expectation values of random variables multiplied by
numbers and the sum of random variables is called the \emph{linearity of expectation}:
\begin{equation}
\expect{aX + bY} = a \,\expect{X} + b \,\expect{Y}.
\end{equation}
The proof of this fact follows from the logic used to find the pmf of $Z = X +
Y$. It's not hard, but it's not particularly enlightening either.

Interestingly, the product $XY$ of two random variables is nice only if the
variables $X$ and $Y$ are independent. In that case, $\expect{XY} = \expect{X}\,\expect{Y}$.
More generally, there is some residual term call the \emph{covariance}:
\begin{equation}
\expect{XY} = \expect{X} \, \expect{Y} + \cov{X, Y}.
\end{equation}
Before getting too much into covariance, we should start with regular variance.

\subsection{Variance}

If the expected value is some measure of position, then \emph{variance} is the
corresponding measurement of spread. It is a function of random variables that
returns a nonnegative number:
\begin{equation*}
\mathbb{V} : \text{random variables} \to [0, \infty).
\end{equation*}
The variance of $X$ is also often written $\mathrm{Var}[X]$, $V(X)$, or $\sigma^2_X$.

For a random variable $X$, the variance is the expected value of the square of
the deviation of the random variable from its own expected value:
\begin{equation*}
\var{X} \defeq \expect{(X - \expect{X})^2}
\end{equation*}
Because all those nested brackets are confusing, people sometimes write
$\expect{X}$, which is just a number, as $\mu$ so that $\var{X} = \expect{(X -
\mu)^2}$.

Variance is always nonnegative because it's a weighted average over squares,
which are always nonnegative. The variance of a random variable is zero only
in the very boring case where that random variable always takes on the same
value.

Note that the square in $(X - \expect{X})^2$ is a shorthand for taking the
product of two iid random variables: $X^2$ refers to the product $X_1 X_2$,
where $X_1$ and $X_2$ are distributed like $X$.

Unlike the expected value, variance is not linear. Consider the random variable $aX$,
where $a$ is just a number:
\begin{equation}
\var{aX} = \expect{(aX - \expect{aX})^2} = \expect{a^2(X - \expect{X})} = a^2 \, \expect{X}.
\end{equation}
For a sum, the situation is even more confusing. Some algebra shows that
\begin{equation*}
\var{X + Y} = \var{X} + \var{Y} + 2 \,\cov{X, Y},
\end{equation*}
where $\cov{X, Y}$, introduced in the end of the last section, is the
\emph{covariance} of $X$ and $Y$:
\begin{equation}
\cov{X, Y} \defeq \expect{(X - \expect{X})(Y - \expect{Y})}.
\end{equation}

In words, the covariance is the expected value of the product of the
deviations of $X$ and $Y$ from their expected values. If, when $X$ takes on
values greater than its expected value, $Y$ also tends to take on values
greater than its expected value, then $\cov{X, Y} > 0$. Conversely, if, when
$X$ has positive deviations, $Y$ has negative deviations, then $\cov{X, Y} <
0$. If $X$ and $Y$ are independent, then $\cov{X, Y} = 0$.

\subsection{Correlation}

Rather than covariance, you're probably more familiar with the term
\emph{correlation}, especially the familiar Pearson's correlation coefficient,
usually written $\rho$ or $r$. The correlation coefficient is a function of
two random variables that gives a number between $-1$ and $1$:
\begin{gather*}
\rho : (\text{random variables}) \times (\text{random variables}) \to [-1, 1] \\
\rho[X, Y] \defeq \frac{\cov{X, Y}}{\sqrt{\var{X} \var{Y}}},
\end{gather*}
In other words, the correlation between $X$ and $Y$ is their covariance
``normalized'' by their variances. The reason for dividing by the two
variances was so that $\rho$ is between $-1$ and $1$. The math isn't hard, but
it's not particularly useful for us.

% swo: figure, or integrate this paragraph

To see how covariance and correlation are related, think about a correlated
$X$ and $Y$, like a simple bivariate normal distribution. \hl{figure} In
general, the points for which $X$ is greater than its mean value are the same
points for which $Y$ is greater than its mean value. Similarly, when one
deviation is negative, the other tends to be negative. This way, the overall
\emph{product} of the two deviations tends to be positive. In constrast, if
two variables are \emph{anticorrelated}, then when one is higher than average
the other tends to be lower than average, so the product of deviations tends
to be negative.
