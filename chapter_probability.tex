%!TEX root = stats.tex

\chapter{Probability theory: statistics's toolkit}

Probability is an entire branch of mathematics. To understand statistics, it's
critical to have a grasp of basic concepts in probability. Indeed, inferential
statistics was originally called ``inverse probability'': ``probability''
meant the study of going from rules to outcomes (e.g., given the rules of
chance, how likely am I to draw a royal flush?) and ``inverse probability''
meant going from outcomes to rules (e.g., given the large number of times
you've drawn royal flushes, can I conclude that you are a cheater?).

Probability theory is all about (you guessed it!) a thing called
\emph{probability}. Before going any further, let's examine this concept.

\section{The definition of probability}

We talk about and reason about probability every day, mostly for matters of
prediction. Given the weather report, it it worth it for me to carry an
umbrella? When will the line at the supermarket be shortest? Given the results
of my experiment, how likely is it that my hypothesis was correct?

I bring this up because the way we think about probability is mostly in a
\emph{Bayesian} way, but statistics, when not prefaced by the word
``Bayesian'', refers to a different philosophical branch call
\emph{frequentist} statistics. These philosophical distinctions will have
practical technical implications for your statistics.

In Bayesian statistics, \emph{probability} refers to a difficult-to-articulate
sense of confidence about a future event. Practically, if I say the
probability of an event is 1\%, it means I'm willing to bet a lot of money it
won't happen. Philosophically, it's hard to say what me being ``1\%
confident'' means. Even if it's philosophically challenging, I think this is
how we all think about probability, so I won't go on about it.

A more mathematically and philosophically simple way to think about
probability is the \emph{frequentist} approach, in which probabilities are all
frequencies (or proportions). In the frequentist scheme, a 50\% chance of
flipping a coin and seeing heads means that, as you flip the coin more and
more times, the proportion of flips that come up heads will approach 50\%.

The problem with the frequentist approach is that probabilities can only be
assigned to events that can be repeated. It therefore doesn't make sense to
ask about the probability that your hypothesis is correct, since your
hypothesis \emph{is} either correct or incorrect, and no number of experiments
will change whether it is or not. We live in just one universe, so you can't
ask, in a frequentist scheme, about the probability of a state of nature, like
whether your hypothesis is correct.

This is deeply dissatifying: the whole point of statistical inference is to
figure out what's going on in the world. But the Bayesian approach, which does
allow you to ask about the probability of states of nature, is mathematically
and technically more challenging. Also, some prominent statisticians thought
it was nonsense and mostly directed all statistics work away from the Bayesian
approach for many years.

So what's the definition of probability? It's either:
\begin{enumerate}
\item the proportion of identically-repeated trials that have some outcome (in the frequentist approach), or
\item some number between 0 and 1 that encodes our sense of the---for lack of a better word---probability of that outcome (in the Bayesian approach).
\end{enumerate}

\section{The mathematical definition of probability}

In the last section I used the words ``trial'', ``outcome'', and
``probability''.  There is a sophisticated mathematical theory about all these
concepts called \emph{measure theory}. Although fascinating, it's pretty heavy
lifting, so I'll aim to use definitions that will give us a lot of practical
benefit, albeit at the cost of creating a mathematically incomplete theory.

To start, we define \emph{outcomes} as the set of all things that could happen
when we run some trial. For example, if I draw a single card from a shuffled
deck, there are 52 possible outcomes, one for each card. The collection of all
possible combinations of outcomes are the possible
\emph{events}.\footnote{This is where the mathematical incompleteness comes
in. If the outcomes are finite (i.e., discrete, like in our cards example),
it's straightforward to enumerate the events as the finite set of all possible
subsets of the outcomes. If the outcomes are continuous, things get tricky,
and you need to be more careful about how you define ``all possible
combinations''. The ``right'' way to do things is with a
\emph{$\sigma$-algebra}.} In my card-drawing example, events including the
individual outcomes (e.g., the Jack of Diamonds) and combinations of outcomes
(e.g., any Diamond), including the ``everything'' outcome (drew any card).

We associate events with probability values using a function that is,
confusing, also called ``probability''. Let $\Omega$ represent the set of all
possible events, and let $\omega$ be some particular event (e.g., drew a
Diamond). Then the probability function looks like:
$$
\mathbb{P} : \Omega \to [0, 1],
$$
that is, the probability function $\mathbb{P}$ is a function that takes
elements in $\Omega$ (i.e., $\omega$) to values between 0 and 1. This will
typically take the form
$$
\prob{\omega} = p,
$$
which can be read as ``the probability of $\omega$ is $p$''. For example,
$$
\prob{\text{drew a Diamond}} = \tfrac{1}{4}.
$$
Again, in a Bayesian mindset, $\tfrac{1}{4}$ means that I'm 25\% confident
(whatever that means) that the card I draw will be a Diamond. In the
frequentist mindset, it means that, if I shuffle the deck and draw a single
card many times, the proportion of cards I draw that are Diamonds will
approach 25\%.

It's a requirement that all the probability goes somewhere, so $\prob{\Omega} = 1$,
that is, the probability of \emph{something} happening is 1. It's also a
requirement that it can't be that nothing happens: $\prob{\varnothing} = 0$,
where the empty set $\varnothing$ is, weirdly, an event.

\subsection{Manipulating probabilities}

We're often interested in the relationships between events, and the math
behind this should be pretty intuitive.

If $A$ and $B$ are two events that don't have any constituent outcomes in
common, we call then \emph{disjoint}, and their probabilities add. For
example, the probability of drawing a Queen \emph{or} a King is just the sum
of the probabilities of drawing a Queen plus the probability of drawing a
King. Mathematically we write this as
$$
A \cap B = \varnothing \implies \prob{A \cap B} = \prob{A} + \prob{B}.
$$


A core concept that presented a lot of theoretical confusion in the
development of statistics is \emph{conditional probability}: what is the
probability of \(A\) ``given that'' \(B\) is true?

This feels like a natural concept: given that I drew a Jack from the
deck, what's the probability I drew the Jack of Hearts? Clearly
\(\tfrac{1}{4}\). What's the chance I drew the Queen of Spades? Clearly
zero.\footnote{Those who took note of the frequentist definition will
  say that I'm not being accurate with my language, and they'd be right.
  I should say, ``Among trials in which a Jack is drawn, in what
  proportion was the Jack of Hearts drawn?'' This idea of ``the chance
  that'' is clearly more in the Bayesian vein.}

Mathematically, we write \(A | B\) to mean ``\(A\) given that \(B\)''.
The definition of conditional probability is \[
\mathbb{P}[A | B] \equiv \frac{\mathbb{P}[A \cap B]}{\mathbb{P}[B]}.
\] I think the best way to read this is: rather than considering the
entire universe of possible events, go into the smaller universe of
events \(B\), and calculate probabilities there. Then the denominator is
just the ``size'' of the universe, and the numerator is the ``size'' of
the event of interest within the scope of the universe I'm thinking
about.

\textbf{independence}

\subsection{Random variables}\label{random-variables}

We can associated this space of outcomes and events with \emph{random
variables}, which are functions of the event space that return numbers.
For the coin flip, the only interesting random variable is: if the flip
is heads, the value is one; if tails, then zero. For a dice roll, the
easy-to-think-of random variable is the number of dots that came up: if
the dice rolls one, the value is 1; if two, then 2; etc. You could think
up other variables: maybe you get 1 on even dice rolls and 0 on odds.

To a scientist, this sounds really abstract, which it is, but it lets us
write something that feels more natural: for a random variable \(X\),
what's the probability that \(X\) is less than some particular value
\(x\)? I'll write this \(\mathbb{P}[X < x]\). This is called the
\emph{cumulative distribution function}, of cdf, of \(X\). Traditionally
it's written as \(F_X\): \[
F_X(x) \equiv \mathbb{P}[X < x].
\]

If \(X\) is a finite, discrete function (that can take on only
particular values), then \(F_X\) is zero for the smallest \(x\)
supported by \(X\) (although it's less than 1 for the highest \(x\)).
For a function that can take on arbitrarily large or small values, we
can see that \[
\lim_{x \to -\infty} F_X(x) = 0 \text{ and } \lim_{x \to \infty} F_X(x) = 1.
\]

In the finite, discrete case, like a dice roll, it's straightforward to
compute the cumulative distribution function from the \emph{probability
mass function}, which is just the probability that \(X\) takes on any
particular value. For other kinds of random variables, the corresponding
concept is the \emph{probability distribution function}, or pdf,
traditionally written \(f_X\), which is actually derived from the
cumulative distribution function: \[
f_X(x) \equiv \frac{d}{dx} F_X(x), \text{ or equivalently } F_X(x) = \int_{-\infty}^x f_X(x') dx'.
\]

\subsection{Detritus}\label{detritus}

I'll need an introduction to some basic ideas: events, unions,
conditional probability, expected values, variance, pdf, cdf. I think
that's most of what you need to get the basic gist. (As an aside, you
might need some really complicated stuff to answer the finer point
questions, like what's the space of rational ``coin flips''?)