%!TEX root = main.tex

\chapter{Probability}
\label{chapter:probability}

To understand statistics, it's critical to have a grasp of basic concepts in
probability, which is an entire branch of mathematics. Here I will give you
the essentials so you can have a grasp of probability theory.

\section{Probability has two definitions}

We talk about and reason about probability every day, mostly for matters of
prediction. Given the weather report, is it worth it for me to carry an
umbrella? Given the results of my experiment, how likely is it that my
hypothesis was correct?

We intuitively think about probability in a mostly \emph{Bayesian} way.
Statistics, when not prefaced by the word ``Bayesian'', refers to a different
philosophical branch call \emph{frequentist} statistics. The philosophical
distinction between Bayesian probability and frequentist probability has
practical implications, so it's important to understand the difference.

In Bayesian statistics, \emph{probability} refers to a sense of confidence
about an unknown thing, often a future event. If I say the probability of an
event is 1\%, it means, in a practical sense, that I'm willing to bet a fair
amount of money it won't happen. Philosophically, it's hard to say what ``1\%
confident'' really means, but the concept should be very intuitive.

The mathematically and philosophically simpler definition of probability is as
a proportion or ``frequency''. A 50\% probability of flipping a coin and seeing
heads means that, as you flip the coin more and more times, the proportion of
flips that come up heads will approach 50\%. One might equivalently say that
the ``frequency'' of heads approaches 50\%. This is called the
\emph{frequentist} definition of probability.


\section{The frequentist system is more coherent but dissatisfying}

The problem with the frequentist definition is that probabilities can only
be assigned to experiments or situations that can be repeated infinitely many
times. It does not make sense to ask about the probability that it will rain
tomorrow any more than it makes sense to ask about the probability that it
rained yesterday: it either rained or it didn't, either it will rain or it
won't.

The fact that we live in just one universe means that, in a frequentist scheme,
you cannot ask about the probability of a state of nature. You cannot ask about
the probability your hypothesis is correct. If you get anything other than zero
or one, you did the math wrong.

As a scientist, this is deeply dissatisying.  The whole point of statistical
inference is to figure out what's going on in the world. I don't want to feed
my hard-won experimental data into a statistical algorithm that says, ``If your
hypothesis is true, then it is; and if it's not, it's not.''

The Bayesian approach is more appealing in that it does in fact allow you to
ask about the probability of states of nature. There is such a thing as a
Bayesian probability that it will rain tomorrow or that your scientific
hypothesis is correct.

Unfortunately, Bayesian statistics is more mathematically and technically
challenging. It also requires an assertion of a \emph{prior probability}: you
must state the probability that your hypothesis was true before you started the
experiment. This idea is bizarre and repugnant to many scientists today. More
importantly, some early, prominent statisticians thought it was nonsense and
mostly directed the field of statistics away from the Bayesian approach for
many years.

Perhaps not surprisingly, frequentist statistics are more common.
``Statistics'' means frequentist statistics unless someone explicitly says
``Bayesian''. I follow this convention and will discuss only frequentist
statistics. Sorry.


\section{Probability is a function}

Above we said that the frequentist definition of probability has to do with the
proportions of infinitely-repeatable trials that have some \emph{outcome}. An
outcome is any thing that can happen as a result of an infinitely-repeatable
trial. For example, if you flip a coin, you will get heads, or you will get
tails.

The \emph{sample space}, written $\Omega$, consists of all possible subsets of
\emph{outcomes}, each written $\omega$. Members of the sample space are called
\emph{events}.  Clearly, each outcome is itself an event: ``flipped heads'' and
``flipped tails'' are both events. But the empty set $\varnothing$ is also an
event (``nothing happened''), and the set of all outcomes $\Omega$ is also an
event (``something happened''). Some other examples of outcomes, events, and
sample spaces are in Table \ref{tab:outcome-event}. I emphasize that
\emph{outcomes} are individual, real things that might happen, while
\emph{events} are abstract groupings of outcomes.

\begin{table}
\centering
\begin{tabular}{p{3cm}p{4cm}p{6cm}}
\toprule
Situation & Outcomes & Events \\
\midrule
Flipping a coin & heads, tails & $\varnothing$, $H$, $T$, $\Omega$ \\
Rolling a die & $1, 2, \ldots, 6$ & $\varnothing$; $1, 2, \ldots, 6$; $\binom{6}{2}$ 2-event outcomes (e.g., 1 or 2); $\binom{6}{3}$ 3-outcome events (e.g., 1, 2, or 3); $\ldots$; $\binom{6}{5}$ 5-outcome events (e.g., any except 1); $\Omega$ \\
Drawing a card & 2 of Clubs, $\ldots$, Ace of Hearts & $\varnothing$; 52 individual events (e.g., 2 of Clubs); $\binom{52}{2}$ 2-outcome events (e.g., black Ace); $\ldots$; $\binom{52}{51}$ 51-outcome events (e.g., any except 2 of Clubs); $\Omega$ \\
Pick a random number between 0 and 1 & All real numbers between 0 and 1 & $\varnothing$; intervals like $[0, \tfrac{1}{2}]$; ``all subsets'' of $[0, 1]$; $\Omega$ \\
Your experiment & Each possible configuration of atoms at measurement & ``Cancer cell died'', ``Electron had energy between 1 and 2 eV'', etc. \\
\bottomrule
\end{tabular}
\caption{Distinction between outcomes and events for various example
situations.}
\label{tab:outcome-event}
\end{table}

As an aside, I note that defining ``all possible subsets of outcomes'' is
straightforward for a discrete case like flipping a coin or drawing a card, but
it is complicated for a continuous case like drawing a random real number
between $0$ and $1$. In continuous cases, a rigorous definition for the event
space $\Omega$ requires concepts from \emph{measure theory}, including a
$\sigma$-\emph{algebra}. I avoid these complexities in this chapter at the cost
of presenting a mathematically incomplete theory of probability.

\emph{Probability} is a function that maps events to numbers between $0$ and
$1$:
\begin{equation*}
\mathbb{P} : \Omega \to [0, 1]
\end{equation*}
For example, if $H$ is the event of flipping heads, then $\prob{H} =
\tfrac{1}{2}$. I use the square brackets to emphasize that $\mathbb{P}$ is a
function of something other than numbers: $H$ is not a number like $5$, it is
an \emph{event}, a distinct mathematical object.

Confusingly, ``probability'' refer both to the function $\mathbb{P}$ as well as
the number $p = \prob{\omega}$ that comes from applying this function to an
event $\omega$.

Comfortingly, probability functions are axiomatically required to be defined so
that that the probability that ``nothing happened'' is zero and the probability
that ``something happened'' is one:
\begin{align*}
\prob{\Omega} &= 1 \\
\prob{\varnothing} &= 0
\end{align*}
Something must happen, and nothing cannot happen.

The axioms about probability functions also mean they follow specific rules
around manipulating probabilities. For example, we're often interested in the
relationships between events. What is that probability that this \emph{or} that
happened? What is the probability that this \emph{and} that happened?

\section{``Or'' adds event probabilities; ``and'' multiplies}

If $A$ and $B$ are two events that don't have any constituent outcomes in
common, we call then \emph{disjoint}, and their probabilities add. For example,
the probability of flipping a heads \emph{or} flipping a tails is the
probability of heads plus the probability of tails.  Mathematically we write
this as
$$
\text{if } A \cap B = \varnothing \text{, then } \prob{A \cup B} = \prob{A} + \prob{B},
$$
where the ``cap'' $\cap$ means \emph{intersection} (``and'') and the ``cup'' $\cup$ means
\emph{union} (``or''), so this reads ``if no outcomes are in both $A$ and $B$,
then the probability of $A$ or $B$ is the sum of their individual probabilities.''

If $A$ and $B$ do have some overlap, you need to subtract out the probability
of the overlap. For example, consider drawing a card
from a standard 52-card deck. What is the probability of drawing a Jack \emph{or} a
Diamond? If you add up the probability of drawing a Jack and the probability
of drawing a Diamond, you end up double-counting the Jack of Diamonds event,
so the solution is to subtract out the double-counted event:
$$
\prob{A \cup B} = \prob{A} + \prob{B} - \prob{A \cap B}.
$$
We mostly deal with disjoint events, so I won't belabor this point.

We said that ``or'' for disjoint events adds probabilities. How do we
find the probability of $A$ and $B$? Say I flip two coins. What's the
probability that I flip heads on the first coin \emph{and} heads on the second?

If $A$ and $B$ are \emph{independent} events, then their probabilities
multiply. The probability of flipping two heads in a row is $\tfrac{1}{2} \times
\tfrac{1}{2} = \tfrac{1}{4}$.

\section{Independence and conditional probability}

Mathematically, $A$ and $B$ are indepedent if and only if their probabilities
multiply. This might feel circular, so I will show you the logic.

Intuitively, two events are independent is they do not depend on each other. If
I flip two separate coins, the flip of one coin can't affect the flip of the
other.

Perhaps surprisingly, ``depends on'' hits on a deep philosophical question.
Just as ``probability'' had frequentist and Bayesian definitions, so there are
multiple definitions of \emph{conditional probability}. The easiest
(``Kolomogorov'') definition is:
\begin{equation*}
\prob{A | B} \defeq \frac{\prob{A \cap B}}{\prob{B}},
\end{equation*}
where $\prob{A | B}$ is pronounced ``the probability of $A$ given $B$''.

Although the definition of conditional probability is a mathematical axiom that
cannot be proven or disproven, it is easy to get an intuitive picture of why it
is chosen as an axiom. In a frequentist interpretation of probability, $\prob{A
| B}$ is, on a denominator of the trials in which $B$ happened, the proportion
of trials in which $A$ also happened. Say $n_B$ is the number of trials in
which $B$ happened, $n_{AB}$ is the number of trials in which $A$ and $B$
happened, and $n$ is the total number of trials. Then the proportion we're
talking about is $n_{AB} / n_B$, which corresponds to $\prob{A \cap B} /
\prob{B}$. I like to think of conditional probability as ``zooming in'' to a
smaller sample space: rather than thinking about all of $\Omega$, we are
thinking only about the subset of outcomes $\omega$ where $B$ happened (Figure
\ref{fig:conditional-probability}).

\begin{figure}
\caption{Conditional probability as shrinking of universe}
\label{fig:conditional-probability}
\end{figure}

If $A$ and $B$ are independent, then $A$ does not ``depend on'' $B$: the
probability of $A$ given that $B$ happened is equal to the probability of $A$
without knowing anything about $B$: if $A$ and $B$ are independent, then
$\prob{A | B} = \prob{A}$, and thus $\prob{A \cap B} = \prob{A} \times
\prob{B}$.

\section{Wait, but what's the actual probability of anything?}

SWO
