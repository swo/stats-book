%!TEX root=main.tex

\chapter{Preface}

\section*{Who this book is for}

I finished my PhD at MIT in Biological Engineering in 2016. While I was a
graduate student, and afterward when I was a postdoc, I found that graduate
students, especially in the life sciences, have a fair amount of quantitative
training, often including statistics, and are capable of hacking together
fairly sensible quantitative methods that answer their scientific questions. I
found, though, that there was often a steep drop-off when it came time to apply
statistical rigor to these hacky methods. I say this, in part, because it is my
own story too.

This book is for people who are experienced quantitative and scientific
thinkers, have decent algebra and maybesome rusty stats, and who want to learn
about how to use statistical thinking to improve their scientific thinking.

I also tried to have fun when I wrote this book, so there are lots of things
in it that I think are fun, like history\footnote{I think history is actually
a useful part of the study of statistics, because it gets you away from
thinking about statistics as some monolithic thing and gets you toward
thinking about it as a work-in-progress whose shape has been determined by the
abilities and prejudices of its makers. Ronald Fisher's personal tastes will
come up a lot; I have a pet theory that a lot of the flavor of contemporary
statistics is due to his tastes.} and footnotes.

\section*{What this book is for}

As I tried to fill gaps in my statistics knowedge, I found that books about
statistics tended to come in two varities: utterly simplistic and
overwhelmingly complex. It was a similar feeling as trying to learn a new
language as an adult: I have the mind of an adult, and am interested in
grown-up things, but I can only read my target language at the level of an
elementary schooler. Books about grown-up things have overwhelming vocabulary,
while books are childish things are way below my level.

I intend this book to be conceptually advanced but technically manageable. Very
often there is sophisticated mathematical machinery behind something that can
be summarized, from a scientific view, very quickly. On the other hand, there
are some times that a little algebra goes a long way to illustrating a
statistical concept.

For example, "maximum likelihood" is a key concept in statistics. Rather than
show you how you can use linear algebra to compute maximum likelihood values
for linear regression, I will just tell you that there is such software as an
optimizer that finds maxima. On the other hand, I won't shy away from
explaining to you why you divide by $n$ when you compute the mean but you
divide by $n-1$ when you compute the standard deviation, which is pretty easy
to derive algebraically.

\section*{What this book is not}

Maybe it's just as easy to say that this \emph{is} a book for people who want
to \emph{use} rigorous statistics to do science, and maybe even to develop some
new statistical methods, but it is \emph{not} a book for people who want to
push the boundaries of what statisticians think are interesting mathematical
problems.

This is not a cookbook that tells you what steps to do to get a $p$-value. This
is more like Harold McGee's \textit{On Food and Cooking}, that tries to help
give you the principles by which you can understand what the tests are doing.

I also do not intend to fill this book with examples, because I find that it's
very easy to think that your particular scientific question is very
itneresting, and that the statistics that motivate are interesting, but that
it's hard to get motivated by hearing about someone \emph{else's} statistical
woes. I try to keep it general so you can imagine you are the hero of the story.

\section*{What you should be able to do after reading this book}

I basically want you to be able to articulate you own statistical tests, modify
and critique existing methodologies, and develop a healthy skepticism of the
idea of statistical inference in general.

\section*{What else should I read}

My principal sources of motivation for this book were Allen Downey's
\textit{Think Stats} and \textit{Think Bayes}, Miran Lipva\v{c}a's
\textit{Learn You a Haskell for Great Good}, and Stephen Stigler's
\textit{History of Statistics}. That order is probably the most profitable.

\section*{Notes on notation}

\paragraph{Functions} When introducing a function $f$ that maps things in a domain
$D$ to things in a range $R$, I write that as
$$
f : D \to R.
$$
When writing a function $f$ that takes numbers, I use normal parentheses:
$f(x)$. When writing a function that takes things that aren't numbers, I use
square brackets: $\prob{A}$. My idea is to emphasize that which functions
are number-taking and which are something-else-taking.

When a function takes multiple inputs, I use a semicolon to distinguish
between the input(s) that, in that context, we think of as changing and the
input(s) that we think of as fixed. I put the ``variables'' before the
semicolon and the ``parameters'' after. So I would write the probability of
getting $x$ successes out of $n$ trials each with probability $p$ of success
as $f(x; n, p)$. Other people might write this as $f(x | n, p)$, especially
for likelihood, but I like to reserve the bar specifically for conditional
probabilities $\prob{A | B}$.

\paragraph{Probability} When probability is a function, I use the
``blackboard'' letter $\mathbb{P}$. When probability is a number---that is, an
output of the function $\mathbb{P}$---I write $p$. I try to avoid capital
$P$, because I think it's not clear whether that's a function or a number.

\paragraph{Expected value} When expected value is a function, I use the
``blackboard'' letter $\mathbb{E}$. I use an ``overline'' to write the arithmetic
mean of a group of numbers: $\overline{x} = (1/n)\sum_i x_i$. I try to avoid
using the overline with random variables and use, say, $\mu$ as the true mean
and, as per the note below, $\hat{\mu}$ as an estimator of the mean (which,
incidentally, $\bar{x}$ is).

\paragraph{Variance} When variance is a function, I use the ``blackboard''
letter $\mathbb{V}$. As per the note below, I write the estimated variance
as $\hat{\mathbb{V}}$ but the sample variance as $\sigma^2$. I do this in
part to clarify why we use $n$ in the denominator of $\sigma^2$ and $n-1$
in the denominator of $\hat{\mathbb{V}}$.

\paragraph{Estimators} I use ``hats'' for estimators, so $\hat{X}$ is an
estimator for $X$. When it's important to distinguish between estimators,
I use subscripts, so maybe $\hat{X}_\mathrm{ML}$ is the maximum likelihood
estimator for $X$.
