\documentclass{book}

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{bm}
\usepackage{mathtools} % for dcases
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{color,soul} % for hl
\usepackage{fullpage}
\usepackage{xfrac} % for sfrac

\providecommand{\tightlist}{\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\newcommand{\prob}[1]{\mathbb{P}\left[{#1}\right]}
\newcommand{\expect}[1]{\mathbb{E}\!\left[{#1}\right]}
\newcommand{\var}[1]{\mathbb{V}\left[{#1}\right]}
\newcommand{\cov}[1]{\mathrm{Cov}\!\left[{#1}\right]}
\newcommand{\defeq}{\stackrel{\text{def}}{=}}
\newcommand{\rveq}{\stackrel{\text{rv}}{=}}
\newcommand{\dd}{\, \mathrm{d}}

\title{A short introduction to advanced statistics for scientists}
\author{Scott W. Olesen}

\begin{document}
\maketitle

\tableofcontents

\frontmatter

\include{chapter_preface}

\mainmatter

\include{chapter_introduction}

\part{Probability}

\include{chapter_functions}
\include{chapter_probability}
\include{chapter_randomvars}

\part{Descriptive statistics}

\include{chapter_statistics}
\include{chapter_estimators}
\include{chapter_descriptive}

\part{Inferential statistics}

\include{chapter_inference}
\include{chapter_tests}

\chapter{copied material}

\section{Things to include}\label{things-to-include}

\begin{itemize}
\item Why can't we make confidence intervals with the LR test? Like, how big does a value have to be for us to reject the null? I imagine that this is an overly conservative approach, but I think it gives a nice exmaple of how to think about stuff.
\item
  Cramer-Rao bound: inverse of Fisher information matrix is the lower
  bound for the variance of unbiased estimators. An estimator that hits
  this bound is minimum variance unbiased (MVU). (BLUE as a weird name,
  since B and MV mean the same thing?) Not \emph{a priori} clear that
  some estimator is an MVU.
\item
  Start with Bayesian, because that's how scientists are used to
  thinking, and then go onto frequentist, because that's how it's easier
  to do the math, and then return to Bayes.
\item
  Gauss: a minimum-variance, mean-unbiased estimator minimizes the
  squared-error loss function. Laplace: among median-unbiased
  estimators, a minimum-average-absolute-deviation estimator minimizes
  the absolute loss function. Maybe it's better to allow some bias so
  you can get less variance. That's the domain of statistical theory.
\item
  Linear mixed models using relationship-matrix?
\item
  Fisher's crazy sum test is the same thing as is used in \emph{TRANSIT}
  (DeJesus \emph{et al}.): they treat TA sites in the same gene as
  independent; the statistic is the difference in the sum of the
  (normalized) number of insertions in two treatments; the null
  distribution is generated by shuffling the values across the two
  datasets. OK, it's not \emph{exactly} like Fisher's test, since it's
  not paired, but it's pretty close. Fisher probably wouldn't have
  wanted to to the \(\binom{n}{2}\) options, compared to the \(2^n\)
  that he did.
\end{itemize}

\section{GEE notes}\label{gee-notes}

The estimator for the covariance matrix of the estimator for the
parameters \(\bm{\theta}\) has typical element \[
\widehat{\mathrm{Cov}}[\hat{\bm{\theta}}]_{i,j} =
  \left[ \left( -\frac{\partial^2 \mathcal{L}}{\partial \theta_u \, \partial \theta_v} \right) \right]^{-1}_{i,j}.
\] The inverse is a matrix inverse. This is weird notation.

For example, consider the easy case with just one parameter, so that \[
\widehat{\mathrm{Var}}[\hat{\theta}] =
  -\left( \frac{\partial^2 \mathcal{L}}{\partial \theta^2} \right)^{-1}.
\]

Then try the easy example where we're looking at the sample mean, which
is the estimator \(\hat{\mu}\) for some population with unknown mean
\(\mu\) and known variance \(\sigma^2\). Then the log likelihood is \[
\mathcal{L}(\theta) = \sum_i \left\{ -\frac{1}{2} \log (2 \pi \sigma^2) -
  \frac{(x_i - \mu)^2}{2 \sigma^2} \right\}
\] and the second derivative is just \[
\frac{\partial^2 \mathcal{L}}{\partial \mu^2} = -\frac{n}{\sigma^2},
\] from which it's clear that
\(\widehat{\mathrm{Var}}[\hat{\mu}] = \sigma^2/n\). This is just the
result that we got from our more straightforward approach, where we
asked about the variance of the estimator, before we had called it that.

Note, however, that this estimate depends on \(\sigma^2\), the
\emph{true} population variance. In an exercise, you'll compute the
estimator covariance matrix for the estimator of
\(\bm{\theta} = (\mu, \sigma^2)\). That will show that there is some
estimated covariance between \(\hat{\mu}\) and \(\hat{\sigma}^2\), but
you find that each of \(\widehat{\mathrm{Var}}[\hat{\mu}]\),
\(\widehat{\mathrm{Var}}[\hat{\sigma}^2]\), and
\(\widehat{\mathrm{Cov}}[\hat{\mu}, \hat{\sigma}^2]\) depend on the true
values, not on the estimates themselves. This isn't unreasonable: each
of these thing is a well-defined random variable that will have a real,
honest, true distribution, and we're deriving its properties. Clearly
the true properties of these random variables should depend on the true
values, but it leaves us in a pickle when we're doing estimation: when
computing the standard errors on our estimators, we need to use our
points estimates as the true values!

\textbf{Relationship between ``sampling distribution'' and estimator.
``Standard error'' as SD of sampling distribution OR estimate of
standard deviation.}



\section{Estimators}\label{estimators}

\subsection{The tyranny of the normal, part
XLIV}\label{the-tyranny-of-the-normal-part-xliv}

You get some data points from a normal distribution (I hope you guessed
it), and you're interested in the mean and standard deviation of that
distribution (because those are the only things to know about).

What is the maximum likehood estimator for the mean? In other words,
given your data points \(x_i\), what \(\mu\), among all the other
\(\mu\), would have given rise to these particular data with the
greatest probability? \[
\begin{aligned}
\hat{\mu}_\mathrm{ML}
  &= \mathrm{argmax}_\mu \, \prod_i \mathbb{P}[X_i = x_i] \\
  &= \mathrm{argmax}_\mu \, \prod_i \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left\{ -\frac{(x_i - \mu)}{2\sigma^2} \right\} \\
  &= \mathrm{argmax}_\mu \, \left(2\pi\sigma^2\right)^{-\frac{n}{2}} \exp\left\{ \sum_i -\frac{(x_i-\mu)^2}{2\sigma^2} \right\}
\end{aligned}
\]

At this point, it's nice to pull a little trick: the value \(x\) that
maximum a function \(f(x)\) is the same value that maximizes
\(\log f(x)\). In other words: \[
\mathrm{argmax}_x f(x) = \mathrm{argmax}_x \log f(x).
\] This turns the nasty exponent into a sum: \[
\hat{\mu}_\mathrm{ML} = \mathrm{argmax}_\mu \left\{ -\frac{n}{2} \left(2 \pi \sigma^2\right) - \frac{1}{2\sigma^2} \sum_i (x_i-\mu)^2 \right\}.
\] It's pretty clear that we'll need to do something about \(\sigma\) at
some point: if \(\sigma\) is too big, then the first term blows up, and
the whole thing goes negative, which is not great for the argmax. If
\(\sigma\) is too small, then second term blows up. Whatever we pick
\(\sigma\) to be, it's clear that that choice is independent of the one
we should make for \(\mu\), since we should just pick a \(\mu\) that
minimizes \(\sum_i (x_i - \mu)^2\).

Is this starting to look familiar? The \(\mu\) that minimizes this sum
is the one that solves: \[
0 = \frac{d}{d\mu} \sum_i (x_i - \mu)^2 = \sum_i -2 (x_i - \mu) = -2 \left( \sum_i x_i - \sum_i \mu \right),
\] which implies that \(\sum_i x_i = n\mu\), that is,
\(\mu = \tfrac{1}{n} \sum_i x_i\). It's just our old, dear friend, the
arithmetic mean.

\subsubsection{Gauss-Laplace synthesis}\label{gauss-laplace-synthesis}

If you're not shocked by what just happened, you have no heart. If you
\emph{are} shocked, then you're in good company: the realization that
the normal distribution is the distribution for which the arithmetic
mean is the maximum likelihood estimator was part of the ``Gauss-Laplace
synthesis''. In 1809, Gauss published a book about ``least squares
estimation'', a method for picking a ``best fit'' estimate to data by
minimizing the squares of the deviations from the arithmetic mean (i.e.,
minimizing \(\sum_i (x_i - \mu)^2\)), was a ``good'' guess for the
``true'' value of the mean, where ``good'' meant that it was relatively
easy to do my hand and seemed to work right.

In the same book, Gauss worked with the normal distribution, which had
previously been cooked up by Laplace, and showed how you could use it as
a model for measurement errors. (Gauss's work with the normal
distribution so overshadowed Laplace's original description that we now
call the normal distribution ``Gaussian'', and ``Laplacian'' refers to a
different distribution.)

In 1810, Laplace showed that the normal distribution arises as the sum
of many random variables \textbf{see somewhere}. Later that year, he
read Gauss's book from 1809, and he was \textbf{get the quote}. In that
moment, Laplace realized that the reason that the arithmetic mean always
seemed so nice, and the reason that least squares seemed to work so
well, was because the normal distribution's fundamental property causes
it to arise in so many places, and \textbf{clean up this section}.

\subsubsection{End of historical note}\label{end-of-historical-note}

As you might guess, the arithmetic mean is also the unbiased estimator
for the population mean: \[
\mathbb{E}[\mathrm{\mu}] = \mathbb{E}\left[\frac{1}{n} \sum_i X_i \right] = \frac{1}{n} \sum_i \mathbb{E}[X_i] = \mu.
\]

\textbf{Is this really all that special, then?}


\subsection{Estimators about
estimators}\label{estimators-about-estimators}

\subsubsection{Jackknife}\label{jackknife}

You have \(n\) data points and compute an estimator \(\hat{\theta}\) for
some population parameter \(\theta\). If you don't know how the
population is structured, then it's not clear what you expect the
variance of \(\hat{\theta}\) to be. How sure can you be of this value?
In terms of inference, can you make any inference with it?

Compute the \emph{jackknife replicates}\footnote{The ``jackknife''
  method is so called because Tukey compared the method, which is
  ``rough-and-ready'', to another rough-and-ready tool, the pocket
  knife, also known as a jackknife. Although this name has the
  disadvantage of giving you no clue what it is about, it had the
  advantage of having more brevity and vivacity than ``delete-1
  resampling'', which is probably the more accurate name.}
\(\hat{\theta}_j\), which are the estimators computed using all the data
points except the \(j\)-th one.

That seems like a weird thing to have done, but you can use them to
compute two handy things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  An estimate of the variance of the estimator. This can help you for
  description---by giving a confidence interval(?)---and for
  inference---by giving you a sense of the ``random'' ranges you would
  expect from two samples.
\item
  An estimate of the bias in the estimator. This is helpful if you don't
  want want your estimator to be biased but you don't know how to fix
  it.
\end{enumerate}

\paragraph{Jackknife variance
estimator}\label{jackknife-variance-estimator}

The variance estimator is \[
\widehat{\mathrm{Var}}_\mathrm{jk}[\hat{\theta}] := \frac{n-1}{n}  \sum_j \left( \hat{\theta}_j - \hat{\theta}_{(\cdot)} \right)^2,
\] where \(\hat{\theta}_{(\cdot)}\) is the average of the jackknife
replicates: \[
\hat{\theta}_{(\cdot)} := \frac{1}{n} \sum_j \hat{\theta}_j.
\] In other words, it's the variance of the jackknife replicates with
some rescaling: \[
\mathrm{Var}[\hat{\theta}_j] = \frac{1}{n-1} \sum_j \left( \hat{\theta}_j - \hat{\theta}_{(\cdot)} \right)^2 \implies
  \widehat{\mathrm{Var}}_\mathrm{jk}[\hat{\theta}] = \frac{(n-1)^2}{n} \mathrm{Var}[\hat{\theta}_j].
\]

The reason for that scaling factor is beyond the scope of this book
(Efron \& Stein 1981?), but the exercise gives you a sense of why it has
to be true for a specific case.

Some other work, also beyond the scope of this book, shows that the
jackknife estimate of variance is biased: it tends to overestimate the
true variance. This makes the jackknife a conservative tool.

\textbf{Exercise}. Let \(\theta\) be the mean. Show that the scaling
factor is what we think. Hints:

\begin{itemize}
\tightlist
\item
  Show that \(\hat{\theta}_{(\cdot)}\) is the sample mean.
\item
  Show that
  \(\hat{\theta}_j - \hat{\theta}_{(\cdot)} = (n \overline{x} - x_j) / (n - 1)\).
\item
  Show that that value is equal to \((\overline{x} - x_j) / (n - 1)\).
\end{itemize}

That exercise is from McIntosh's bioRxiv about jackknife resampling.

\paragraph{Jackknife bias estimator}\label{jackknife-bias-estimator}

The jackknife estimate of bias is
\((n-1) \left( \hat{\theta}_{(\cdot)} - \theta \right)\). This is the
sum of the deviations of the jackknife replicates from the observed
value \(\hat{\theta}\). Again, the reason that you would take the
average deviation and scale it up to the sum is beyond the scope.

However, if you have an expectation about the bias in an estimator, you
can make an unbiased estimator by subtracting out that bias: \[
\hat{\theta}_\mathrm{jk} := \hat{\theta} - \widehat{\mathrm{Bias}}_\mathrm{jk}[\theta].
\]

\textbf{Exercise}. Show that the jackknife estimate of bias for the
variance gives you the familiar unbiased variance estimator.

\textbf{Exercise}. Something about the maximum estimator?

\paragraph{Pros and cons of the
jackknife}\label{pros-and-cons-of-the-jackknife}

It's a piece of cake to implement. There are only \(n\) replicates to
do, so it's tractable. Those replicates are deterministic, so you only
run it once.

The cons are that it doesn't always work. For example, a jackknife
estimate of the variance of a median (\textbf{swo check Knight}) is not
consistent. It's also overly conservative: it's biased toward higher
variances. You can rescue some properties if you move to a delete-\(d\)
resampling and pick \(d\) from the correct range.

\subsubsection{Bootstrap}\label{bootstrap}

\section{Example from Efron, ``Thinking the unthinkable''}

There's some true distribution $f_X(x)$, and you're approximating it with
$\hat{f}_X(x)$, which is a pmf. If you took $N$ data points, then bootstrapping
means that you're picking a vector $\vec{c}$, where $c_i$ is the number of
times that the $i$-th data point makes it into the bootstrap sample. This
begs the question, how is $\vec{c}$ behaved? It's just a multinomial, with
probability $1/N$ for each of the $N$ cells.

Normally you compute a statistic $T(\vec{x})$ of the data. Instead, formulate
this in terms of a function $g(\vec{c})$ If you can write $T(\vec{x}) = \sum_i t(x_i)$,
then $g(\vec{c}) = \sum_i (c_i/N) t(x_i)$. In a Taylor expansion around
$\vec{c}_\mathrm{ML} = (1, 1, \ldots, 1)$:
$$
g(\vec{c}) = g(\vec{c}_\mathrm{ML}) + \sum_i \frac{dg}{dc_i} (c_i - 1) + \mathcal{O}(c_i^2)
$$
So the variance of the values $g(\vec{c})$ that you will get from
bootstrapping is approximately
\begin{align}
\expect{\left[g(\vec{c}) - g(\vec{c}_\mathrm{ML})\right]^2}
  &= \expect{\left(\sum_i \frac{dg}{dc_i} (c_i-1)\right)^2} + \mathcal{O}(c_i^2) \\
  &= \expect{\sum_i \left( \frac{dg}{dc_i} (c_i-1)\right)^2} + \mathcal{O}(c_i^2) \\
  &= \sum_i \left(\frac{dg}{dc_i}\right)^2 \expect{(c_i-1)^2} + \mathcal{O}(c_i^2) \\
\end{align}

And then an $n^2$ comes out? The point is that the jackknife is basically
doing a finite estimation of the gradient, by leaving out a single point at a
time.

\section{others}

Bootstrapping means that you guess that the observed samples are the true
distribution, and you work forward from there. It's like a maximum likelihood,
frequentist thing.

You need to draw $n$ samples, because that's the same number you drew originally.

What do you do when you want to compute the variance of some statistic
that's not easy to compute? Or you don't know what distribution you're
sampling from? Then you permute your own data. How do we relate:

\begin{itemize}
\tightlist
\item
  permutation tests
\item
  bootstrapping (and jack-knifing, etc.)
\item
  nonparametric (which I know is different)
\end{itemize}

\section{\texorpdfstring{What does it mean to
``sample''?}{What does it mean to sample?}}\label{what-does-it-mean-to-sample}

Does it make sense to compute a confidence interval when you're sampled
all the 50 United States?

\textbf{Finite correction factor} to point out that there's a difference
between simple random sampling and something else. Then need to explain
what simple random sampling is!

\section{Confidence intervals}\label{confidence-intervals}

Confidence intervals are very slippery things. It's tempting to say that
``I am 95\% confident'' that the true value of a quantity lies within
the 95\% confidence interval. In frequentist statistics, ``I am
\emph{X}\% confident'' has no meaning. The probability that the true
value lies within an interval is either 0 or 1, since if you repeat the
world-experiment many times, the true value will always be the same. In
other words, ``confidence'' is a Bayesian notion of probability. The
interval that you are 95\% confident that something falls in is
therefore a Bayesian concept (and it gets the confusing name of
``credible interval''). So forget that ``confidence interval'' has
anything to do with confidence.

Here's how things actually work: before you collect any data, you
develop a \emph{method} for generating the upper and lower confidence
intervals, which are a pair of statistics, that is, functions of the
data. This method has the property that the statistics it generates an
interval that, in 95\% of cases, contains the true value.

Here's the slippery part: the confidence interval is generated so that,
in 95\% of cases, they are ``correct'' in that they contain the true
value. In the other 5\% of cases, they don't include the true value.
Strictly speaking, they can't tell you anything about the probability
that the value is in the range.

\textbf{What's the easy way to explain the Bayesian link?} The typical
frequentist answer is really pedantic.

\subsection{Binomial test example}\label{binomial-test-example}

We set \(N\) and observe \(X\) to guess \(p\): \[
P[p \in \mathrm{CI} | X] \propto P[X | p \in \mathrm{CI}] \times P[p \in \mathrm{CI}].
\]

Confidence intervals don't actually do any of these. E.g.,
Clopper-Pearson guarantees that, given any \(p\), 95\% of the outcomes
will include the true value. The actual ranges included depend on \(p\)!
This isn't the case for the normal distribution, because it has all
these amazing properties about scaling and so forth. I think this is the
best example.

For example, start with the super dorpy confidence interval \([0, 1]\).
This is always true, but it's way too conservative. Then say something
else dorpy like a constant range, and show that this won't work if \(N\)
is too small.

Let's look at a real example. The Clopper-Pearson interval are the
limits of the range of values of \(p\) such that
\(P[x < X | p] > 2.5\%\) and \(P[x > X | p] > 2.5\%\). The first
inequality is fulfilled by smaller values of \(p\) (e.g., if \(p\) is
zero, then \(x\) has to be zero), so the \emph{upper} confidence limit
is determined by the greatest \(p\) that satisfies that inequality. The
second inequality is fulfilled by larger \(p\) (e.g., if \(p\) is 1 then
\(x=N\)), so the \emph{lower} confidence limit is determined by the
smallest \(p\) that satisfies this inequality.

How can we tell that this is a confidence interval? Given any true \(p\)
and \(N\), if I make draws \(x\) from \(\mathrm{Bin}(p, N)\), will this
confidence interval include \(p\) in 95\% of cases?

\subsection{\texorpdfstring{Comparison to
\(p\)-value}{Comparison to p-value}}\label{comparison-to-p-value}

Given \(N\), and having observed \(X\), we want to test the hypothesis
that \(p\) equals some \(p_0\) (typically \(\tfrac{1}{2}\)).

The nonrejection interval is the range of \(x\) such that \(x N\) is
close enough to \(p_0\) that the probability that \(x\) arose from
\(p_0\) is above some threshold. The lower limit is the lowest \(x\)
that would have arisen from \(p_0\) with some probability \(\alpha/2\),
that is, the smallest \(x\) such that \(P[x | p_0] > (1-\alpha)/2\).
This means that we take the state of nature \(p_0\) as given and scan
over the possible data values, comparing our data to those values.

In contrast, in a confidence interval, we take the data as given and
scan over the possible states of nature \textbf{in some way}. For the
binomial, it's obvious that the confidence intervals and the
nonrejection intervals are not the same thing, since it the one is
discrete and the other is continuous. Only in the normal (\textbf{???})
can we assume they are the same thing.

\subsection{Returning}\label{returning}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  You get some data that has some parameters (e.g., a sample mean and
  sample variance).
\item
  You \emph{guess} that your observed sample mean and variance are the
  \emph{true} mean and variance.
\item
  You ask, ``If someone else sampled from this true distribution many
  times, and they got all kinds of sample means and variances, what
  method could they use to construct, from the values they observed, an
  interval that would, in 95\% of cases, include the true value?''
\item
  Then you forget that you are omniscient and know the true value and
  instead use the methodology that these ignorant people would use, but
  you use it on your own data.
\end{enumerate}

Now the crazy Bayesian switch comes in: you conflate the frequency of
cases with your confidence that you are in the 95\% of cases.

\emph{N.B.}: For the \emph{t}-distribution, there's this ``pivotal
quantity'' thing, which means that the true \(\mu\) and \(\sigma\) drop
out, which is very luck, and it means that we \emph{don't} need to make
a parametric assumption about how things work.

\subsection{\texorpdfstring{\emph{t}-distribution}{t-distribution}}\label{t-distribution}

Let's think about how to construct that method. Say you knew the true
variance \(\sigma^2\). Then we know that the sample means are drawn from
\(\mathcal{N}(0, \sigma^2/n)\). So it's pretty easy to see that
\((\overline{x} - \mu) / (\sigma^2) \sim \mathcal{N}(0, 1)\), from which
the familiar \(1.96\), etc. come.

What if you \emph{don't} know the true variance? The means are still
drawn from \(\mathcal{N}(0, \sigma^2/n)\), but now the sample variance
is also a random variable.

We know the confidence interval is some function of the sample mean and
variance, and let's guess that it's symmetric about the sample mean and
is some linear function of sample variance: \[
\mathrm{CI}_\pm(\overline{x}, s) = \overline{x} \pm A s.
\] We want to find \(A\) such that \[
\mathbb{P}\left[ \mathrm{CI}_- < \mu < \mathrm{CI}_+ \right] = 95\%,
\] or, if we're willing to trust in symmetry, \[
2.5\% = \mathbb{P}\left[ \mathrm{CI}_- > \mu \right] = \mathbb{P}\left[ \frac{\overline{x} - \mu}{A} - s > 0 \right].
\] We know the distribution of the first thing: \[
(\overline{x}-\mu)/A \sim \mathcal{N}\left(0, \frac{\sigma^2}{n A^2}\right).
\] Some math shows that \[
\frac{(n-1) s^2}{\sigma^2} \sim \chi^2(n-1).
\]

Call the first thing \(K\) and the second \(L\). We're interested in the
distribution of \(M \equiv K - L\): \[
f_M(m) = \int_0^\infty f_K(m + l) f_L(l) \,\mathrm{d}l,
\]

where the limits come from the fact that variance is positive. You're
probably not excited to do this integral, which was considered a major
achievement (well, it was the thought leading up to the integral, which
we've just outlined, but whatever). This major achievement was made by
William Sealy Gosset, who made it while he was a researcher for Guinness
ensuring the quality of their beer. Guinness had a policy of not
allowing its employee to publish their results, so Gosset signed his
paper ``a student'', so the result of that integral is now called
Student's \emph{t}-distribution: \[
f_t(x; \nu) = \frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi} \Gamma\left(\frac{\nu}{2}\right)}
  \left(1+ \frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}},
\] where the (badly named) ``degrees of freedom'' \(\nu\) is \(n-1\) for
our purposes. I write this out fully because it is one of the things we
will \emph{not} derive in this book.

\section{Contingency tables}\label{contingency-tables}

These are nice examples for how to do statistical thinking.

\subsection{Barnard's test}\label{barnards-test}

The classic example is whether a certain treatment causes more of the
outcome of interest than just doing nothing. In medicine, that means
splitting your participants into a placebo group and a treatment group
and asking what fraction of each gets well. In a biology experiment, you
might split your mice into a treatment group and a control group and ask
what proportion of the mice in each group get cancer.

In statistics jargon, this is called a \(2 \times 2\) contingency table:

\begin{longtable}[]{@{}llll@{}}
\toprule
Group & Outcome \(p\) & Outcome not-\(p\) & Row sums\tabularnewline
\midrule
\endhead
A & \(a\) & \(c\) & \(m\)\tabularnewline
B & \(b\) & \(d\) & \(n\)\tabularnewline
Column sums & \(r\) & \(s\) & \(N\)\tabularnewline
\bottomrule
\end{longtable}

Because we picked \(m\) and \(n\), the sizes of the two groups, those
are fixed parameters. The question is whether the way that \(m\) gets
distributed into \(a\) and \(c\) (and that way that the \(n\) get put
into the \(b\) and \(d\)) is consistent with there being a common
probability \(p\) of the outcome of interest.

So we might say that \(a\) is distributed like a binomial distribution
with \(m\) draws and probability \(p_a\) of success, and \(b\) is
distributed like a binomial with \(n\) draws and a probability of
\(p_b\) of success. The null hypothesis is that \(p_a = p_b\). What's
the likelihood of the data given the null?

If we didn't assume the null, and gave the two binomials their own
probabilities, the likelihood of the data would be: \[
P(a, b | p_a, p_b) = \mathrm{Bin}(a; m, p_a) \times \mathrm{Bin}(b; n, p_b).
\] But, given that the probabilities are the same, we can collapse it:
\[
\begin{aligned}
\mathcal{P}[a, b | p_a = p_b = p] &= \mathrm{Bin}(a; m, p) \times \mathrm{Bin}(b; n, p) \\
  &= \binom{m}{a} p^a (1-p)^{m-a} \times \binom{n}{b} p^b (1-p)^{n-b} \\
  &= \binom{m}{a} \binom{n}{b} p^{a+b} (1-p)^{m+n-(a+b)} \\
  &= \frac{m! \, n!}{a! \, b! \, c! \, d!} p^r (1-p)^s.
\end{aligned}
\]

This result is a little confusing\footnote{I trotted out this test
  because these two confusions are actually great learning
  opportunities.}, for two reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The probability \(p\) of the outcome of interest might be interesting
  to design a later experiment, but it's \emph{not} interesting for
  designing a test. We certainly don't want to deliver a result like,
  ``Well, if the null hypothesis is true, \emph{and} \(p\) happens to be
  exactly such-and-such, then your \(p\)-value is so-and-so.'' The value
  \(p\) is called a \emph{nuisance parameter} since we don't actually
  care about its value.
\item
  We're usually not interested in the likelihood of exactly this data,
  but rather in the likelihood of data \emph{at least this extreme}. We
  usually measure ``extremeness'' using a statistic---a single
  number---so it's clear that ``more extreme'' means ``bigger'' (or
  ``smaller'' or ``bigger or smaller'', depending on if it's a one-sided
  or two-sided test). Here, we have two numbers, \(a\) and \(b\), so
  there aren't two ``sides'' to the distribution: there are four!
\end{enumerate}

To resolve the first point, we say that the null hypothesis
\(p_a = p_b = p\) doesn't restrict us to a particular value of \(p\). In
other words, the null hypothesis, which functions as a sort of Annoying
Skeptic, is free to pick \(p\) to make our results as uninteresting as
possible. Mathematically, this means that, when computing the
\(p\)-value, we should optimize over all values of \(p\), choosing the
one that makes our results as uninteresting as possible (i.e., which
maximizes the \(p\)-value).

We can't really ``resolve'' the second point, since it demonstrates that
our previous way of thinking about extremeness was not sufficient for
all cases. As Barnard notes in his original paper\footnote{Barnard
  conceived of the \((a, b)\) as points ``in a plane lattice diagram of
  points with integer co-ordinates'', that is, that \(a\) is like the
  \(x\)-axis and \(b\) is like the \(y\)-axis. Then the possible
  outcomes of the experiment are the points in the rectangle bounded by
  the horizontal lines \(a = 0\) and \(a = m\) and the vertical lines
  \(b = 0\) and \(b = n\). He then said that you should pick the
  non-extremal points (i.e., the values of \((a, b)\) for which you
  would not reject the null) such that they ``consist of as many points
  as possible, and should like away from that diagonal of the rectangle
  which passes through the origin. Formulated mathematically, these
  latter requirements mean that the {[}points for which you would reject
  the null{]} must in a certain sense be convex, symmetrical and
  minimal.''}, there are actually many ways to choose the pairs
\((a, b)\) that produce a \(p\)-value more than our threshold. This gets
into some fancy footwork to articulate exactly how you should pick this
area, but the basic results are pretty intuitive: when \(a/m\) and
\(b/n\) are similar, you tend to be under the rejection threshold; when
they are far apart, you tend to be over.

The interesting point here is that, whatever fancy footwork you pick to
choose that region, and no matter how ``reasonable'' your footwork is,
it's still footwork that doesn't obviously follow from the simple
definition of a hypothesis test. We'll encounter this problem again in
Bayesian statistics, when we find that the Bayesian analog of a
confidence interval is not unique: there are many ranges of values that
are compatible with our ignorance.

\subsection{Fisher's test to the
rescue(?)}\label{fishers-test-to-the-rescue}

If you've worked with contingency tables, you're probably saying, ``I've
never heard of this crazy Bernard's test, with its weird multi-sided
rejection space and its requirement to maximize over \(p\). We have
Fisher's exact test, which is the exactly right test to use here!''

Looking at the same contingency table, Fisher's test asks, given the row
marginals \(m\) and \(n\), the first column marginal \(r\), and the
grand total \(N\), what is the probability of a table at least this
extreme?

This is just a combinatoric problem: if you're as likely to assign items
in \(m\) to \(a\) as to \(c\) (and, analogously, to assign items from
\(n\) to \(b\) or \(d\)), then ``what's the probability of this table''
is equivalent to asking ``given the marginals, how many ways are there
to choose this table?''. More specifically, how many ways are there to
choose \(a\) items from a bank of \(m\) items and \(b\) items from a
bank of \(n\), given that we chose \(r = a + b\) items from the total
\(N\)? Mathematically: \[
\mathbb{P}[a | m, n, r, s] = \frac{\binom{m}{a} \binom{n}{b}}{\binom{N}{r}} = \frac{m! \, n! \, r! \, s!}{N! \, a! \, b! \, c! \, d!}.
\]

Computing the \(p\)-value is easier here than with Barnard's test
because we need to keep the row \emph{and column} marginals the same. In
Barnard's test, we just kept the row marginals constant, because we
considered those as fixed parameters, corresponding to things like the
number of patients we assigned to each of the placebo and treatment
groups. It doesn't make sense to allow the Annoying Skeptic to fiddle
with those values.

In Banard's test, we \emph{did} allow the Annoying Skeptic to fiddle
with the column marginals, since it wasn't clear, before the experiment
began, that \(r\) would have the outcome of interest. In other words, we
didn't know that \(r\) people in both the placebo and treatment groups
would get well.

Fisher's test, however, \emph{does} keep the column marginal constant.
This makes it a lot easier to compute the \(p\)-value. First, the
nuisance parameter \(p\) doesn't appear in the likelihood, so we don't
need to do the weird maximization. Second, we only need to vary one
value, \(a\) (or, equivalently, \(b\)), since, if you know the
marginals, there is only one axis along which to change the values in
the table. In other words, if you know \[
\begin{aligned}
a + c &= m \\
b + d &= n \\
a + b &= r,
\end{aligned}
\] then that's three equations with four unknowns (\(a\), \(b\), \(c\),
\(d\)), so specifying any one of \(a\), \(b\), \(c\), or \(d\) specifies
all the others. (You might be looking for a fourth equation
\(c + d = s\), but you can get that by adding the first two equations
and subtracting the third.)

Here's an example:

\begin{longtable}[]{@{}llll@{}}
\toprule
Group & Success & Failure & Row sums\tabularnewline
\midrule
\endhead
A & 1 & 9 & 10\tabularnewline
B & 11 & 3 & 14\tabularnewline
Column sums & 12 & 12 & 14\tabularnewline
\bottomrule
\end{longtable}

There's only one way to make this table more ``extreme'' without
changing the marginals: you can take the one group A success and make it
a group A failure and simultaneously make a group B failure into a group
B success. Similarly, there's only one way to make this table less
extreme: turn a group A failure into success, and turn a group B success
into failure.

So keeping the column sums constant made it way easier to compute the
\(p\)-value: count this table and all the tables with a more extreme
upper-left or bottom-right and see if your summed probability hits the
rejection threshold.

However, this simplicity came at a cost, which you may have noticed:
does it make sense to keep the columns constant? Experimentally, this
means that you're restricting the Annoying Skeptic to only consider
cases in which, say, the number of patients who got well \emph{in both
groups} is equal to the experimentally observed value. This is a little
weird. It suggest that your experimental design was like this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Pick \(m\), \(n\), and \(r\).
\item
  Assign \(m\) patients to placebo and \(n\) to treatment.
\item
  Wait until \(r\) patients \emph{across both groups} have gotten well.
\item
  Stop the experiment.
\end{enumerate}

This is almost certainly not reflective of how typical experiments are
run\footnote{It is, however, the way the famous ``lady tea tasting''
  experiment was designed. The myth is that Fisher didn't believe it
  when a high-class lady told him that she could detect whether tea was
  added to a cup with milk in it or whether the milk was added to the
  tea. He designed an experiment with \(m\) cups prepared one way, \(n\)
  prepared the other, and told her to detect the \(r = m\) cups that
  were prepared the first way. A Barnard-style experiment, in which the
  same \(m\) and \(n\) cups}.

\textbf{Fisherian small data}

\textbf{What happens if I use the ``wrong'' test? Chi-square as an
example of wrongness}

\section{Regression}\label{regression}

Hardin pages 56-57 talks about the difference between the normal
(subject-specific) and the generalized (population-average) estimating
equations. E.g., second-hand smoking: what's the odds of a kid having a
respiratory illness given that mom smokes? SS parameters give the OR for
\emph{each individual child} having the illness, so it's what we would
expect would happen if particular moms stopped smoking. PA parameters
give the OR \emph{across the population}, so it explains the difference
in prevalence we expect in the two groups. The first one is: \[
\mathrm{OR}^\mathrm{SS} = \frac{P(Y_{it}=1 | X_{it}=1, \nu_i) / P(Y_{it}=0 | X_{it}=1, \nu_i)}{P(Y_{it}=1 | X_{it}=0, \nu_i) / P(Y_{it}=0 | X_{it}=0, \nu_i)}
\] and the second is \[
\mathrm{OR}^\mathrm{PA} = \frac{P(Y_{it}=1 | X_{it}=1) / P(Y_{it}=0 | X_{it}=1)}{P(Y_{it}=1 | X_{it}=0) / P(Y_{it}=0 | X_{it}=0)}.
\] The difference, in case you didn't catch it, is whether you condition
on the random effect \(\nu_i\). The SS estimate is for these particular
people; the PA estimate marginalizes over the random effects.

\section{Bayesian}\label{bayesian}

One of the biggest sticking points about a Bayesian analysis is that it
requires specification of a prior. It can be thought of as an advantage
or a disadvantage, but I think it's better to think of it as a
responsibility. Let me tell you an allegory.

Once, a young statistician lived in her parents' house. She paid no rent
and simply never considered her orientation in the world. Some year
later, she left the home and had to do statistics in the wide world.
Where would she live? How would she pay rent? These decisions brought
power, since she was free to do things she couldn't do at home. She
could live a life that was more accurate to the real world. This
allegory is too long and rambly. But there's something in here.

Frequentist statistics is correct so long as you use it exactly for what
it is designed to do. The trouble is that we \emph{want} statistics to
answer the kinds of questions that \emph{only} Bayesian statistics can
answer. For example, how likely is it that this hypothesis is true? If
you perfectly adhere to the frequentist interpretation, then you are in
good shape. But if you deviate, if you start to say, ``Oh, the p-value
is kind-of like the probability my hypothesis is false.'' Then you have
SCREWED UP son.

\section{Appendix}\label{appendix}

\subsection{Random number generation}\label{random-number-generation}

In many places in this book, we've relied on the ability to generate
``random'' numbers. However, computers (in the sense of logical
machines) have no way to generate truly random numbers. Instead, we have
clever methods that get us something that's a pretty good approximation
of random numbers.

It's worth noting that a lot of the historical, conceptual directions in
statistics are due to the fact that doing any kind of Monte Carlo
methodology without computers is really onerous. Before we had today's
technology (pseudorandom number generators, to be discussed below), we
had tables of random numbers, the most notable being the hefty \emph{A
Million Random Digits with 100,000 Normal Deviates} (where ``normal
deviate'' means ``random number drawn from a standard normal
distribution), published by the (coincidentally-named) RAND
Corporation.\footnote{You can still get this book
  \href{https://www.rand.org/pubs/monograph_reports/MR1418.html}{on
  RAND's website} or
  \href{https://www.amazon.com/Million-Random-Digits-Normal-Deviates/dp/0833030477/}{in
  paperback}. The numbers were generated using an electronic device,
  specifically designed to shuffle a sort-of random table of numbers,
  attached to a computer. See the text about hardware random number
  generators.} RAND published this book because they had a lot of
engineers and researchers using Monte Carlo methods. Before the tables
of random numbers, you had to generate the random numbers yourself,
which was basically infeasible.\footnote{In 1777, Georges-Louis Leclerc,
  Comte de Buffon posed a math problem that included probability and
  geometry: if you throw needles (or matchsticks) onto a surface with
  parallel stripes whose widths are equal to the length of the needles,
  what fraction of the needles touch two stripes? It turns out that the
  answer has \(\pi\) in it. So in 1901, Mario Lazzarini published that
  he had tossed a needle 3,408 times and, using the analytical solution,
  back-calculated \(\pi\) as \(\tfrac{355}{113}\). This estimate of
  \(\pi\) was already known, and the fact that Lazzarini came up with
  exactly that value is taken as strong evidence that he faked the
  experiment. In other words, we have a pretty strong prior against
  believing that someone in 1901 even bothered to throw a needle 3,000
  times, much less the many more times than that that would be required
  for random number generation for more interesting Monte Carlo
  problems!}

As mentioned, now we have \emph{pseudorandom number
generators}\footnote{There is such a thing as a ``hardware'' random
  number generator, which is some kind of device that measures something
  that we think is truly noisy in the real world, like thermal noise or
  (what we believe are truly random) quantum phenomena like
  beamsplitting.}. These rely on some input \emph{seed}, which is the
(hopefully) truly random thing, and from that seed it generates a
deterministic list of numbers that, in the absence of knowing the seed,
appear random.\footnote{It may seem weird that, given a seed, you get a
  deterministic set of numbers. Most software with (pseudo)random number
  generators pick a seed using whatever entropy they have access to when
  you boot up the program, so you never notice that the seed is
  different each time you run a simulation. You can, however, always
  \emph{pick} the seed, which is nice, because it lets you repeat code
  with a Monte Carlo method in it and always get the same result, which
  is nice for testing and debugging.} Usually this seed comes from one
of the many ``entropy sources'' that a computer has access to, things
like the time between keystrokes, the time at which a process was
started, time between network pings, etc.

The pseudorandom number generator in most software now is the Mersenne
Twister. This algorithm is remarkable for having a long \emph{period} of
\(2^{19937} - 1\). (All pseudorandom number generators, started with
with some seed, will eventually end up repeating their output. The
period is the number of outputs you get before closing the loop.) The
random things produced by the generators are typically mapped into
uniformly distribution varibles over \([0, 1]\).

\subsubsection{Generating non-uniformly-distributed
numbers}\label{generating-non-uniformly-distributed-numbers}

Drawing numbers from \([0, 1]\) usually isn't that interesting. We want
to draw numbers from other distributions. There are two main approaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Clever transformations
\item
  Various forms of \emph{rejection sampling}
\end{enumerate}

The idea with clever transformations is to generate random numbers from
the uniform distribution and somehow turn them into random numbers
distributed according to some other distribution.

Rejection sampling is a big class of approaches, including such notables
as ``Metropolis-Hastings'' and ``Markov chain Monte Carlo'' (MCMC). They
are very useful for the practical scientist.

\textbf{inverse transform, ziggurat, rejection, Metropolis-Hastings and
other Monte carlo mcmc stuff}

Clever transformations are nice when you can do them, but it's unlikely
you'll derive one for yourself. Basically, if a run-of-the-mill random
number generation function in some software purports to be able to
sample numbers from some distribution, it's doing this transformation. I
don't think there's anything really practical to be gained from knowing
these transformations, but they're fun, so I put them here.

\paragraph{Clever transformations}\label{clever-transformations}

If you can write the cdf of your distribution of interest, say
\(F_X(x)\) and you invert it (i.e., solve for \(x\) in terms of
\(F_X\)), then you can use a nice trick called \emph{inverse transform
sampling}.

\subparagraph{Normal distribution: Box-Muller
transformation}\label{normal-distribution-box-muller-transformation}

To generate normally-distributed numbers from uniformly distributed
numbers, consider this trick.

Think about a pair of independent, normally-distributed variables
\(Z_1\) and \(Z_2\). Their joint pdf will be \[
\begin{aligned}
f_{Z_1,Z_2}(z_1, z_2) &=
  \frac{1}{\sqrt{2\pi}} \exp\left\{ -\frac{z_1^2}{2} \right\} \times
  \text{same thing for $z_2$} \\
  &= \frac{1}{2\pi} \exp\left\{ -\frac{1}{2} \left( z_1^2 + z_2^2 \right) \right\}.
\end{aligned}
\] The trick is to think of \(z_1\) and \(z_2\) as Cartesian coordinates
like \(x\) and \(y\), from which it's very natural to replace
\(z_1^2 + z_2^2\) with \(r^2\) and define some \(\theta\) such that
\(z_1 = r \sin \theta\) and \(z_2 = r \cos \theta\). My claim is that
we'll be able to generate \(r\) and \(\theta\) from independent, uniform
random variables.

Because \(f_{Z_1,Z_2}\) is symmetric with respect to \(z_1\) and \(z_2\)
(i.e., you could swap that subscripts and come out with the same
expression), it must be that there isn't anything special about having
sine versus cosine. In other words, there mustn't be anything special
about \(\theta = 0\) versus \(\theta = \pi\). The origin can't matter.
Thus, it must be that \(\theta\) is uniformly distributed over
\([0, 2\pi]\). Any other distribution would end up treating \(z_1\) and
\(z_2\) differently, which would break their independence.

Generating \(r\) is a little more tricky. Let's look at the cumulative
distribution function of the random variable \(R\): \[
\begin{aligned}
\mathbb{P}[R < r] &= \int_0^{2\pi} \int_0^r f_{Z_1, Z_2}
    \,\mathrm{d}z_1 \, \mathrm{d}z_2 \\
  &= \int_0^{2\pi} \int_0^r \frac{1}{2\pi} \exp\left\{-\frac{1}{2} r'^2\right\}
    r' \,\mathrm{d}{r'} \,\mathrm{d}\theta \\
  &= \int_0^r \exp\left\{-\frac{1}{2} r'^2\right\}
    r' \,\mathrm{d}{r'} \,\mathrm{d}\theta \\
  &= \int_0^{\tfrac{1}{2} r^2} \exp\left\{-s\right\} \,\mathrm{d}s,
    \text{where $s = \tfrac{1}{2} r'^2$} \\
  &= 1 - \exp\left\{ -\frac{1}{2} r^2 \right\}.
\end{aligned}
\] If we define \(r = \sqrt{-2 \log u}\), then
\(\mathbb{P}[R < r] = 1 - u\), which is just the cdf for a uniformly
distributed variable \(U\) on \([0, 1]\). So we generate \(r\) using
that formula.

It may seem a little strange that we can generate independent \(z_1\)
and \(z_2\) using \(r\) and \(\theta\). You might think that if I know
\(z_1\), then I can guess something about \(r\) or \(\theta\) and use
that information to make a guess about the value of \(z_2\). However,
because the Cartesian and polar coordinate systems encode exactly the
same information, that argument is like saying that, because I told you
\(x\), you might be able to guess \(y\), which is clearly impossible.

\section{Unplaced}\label{unplaced}

\subsection{Chebyshev's inequality}\label{chebyshevs-inequality}

What's the relationship between confidence intervals and variance? We
all know the relationship for the normal distribution.

As a lemma, consider a random variable \(X\) that only takes on
nonnegative values. Then \[
\mathbb{E}[X] = \sum_{k=0}^\infty k \, f_X(k) \geq \sum_{k=1}^\infty f_X(k) = \mathbb{P}[X \geq 1].
\] (To see the middle inequality, note that you can drop \(k=0\), and
then, for all the \(k \geq 1\), you can replace \(k\) with \(1\), which
makes that term in the sum smaller than it might be.) We'll use the
reversed version: \(\mathbb{P}[X \geq 1] \leq \mathbb{E}[X]\).

Now, Chebyshev's inequality is easy. Use the \[
\begin{aligned}
\mathbb{P}\left[|X - \mu| \geq k \sigma\right]
  &= \mathbb{P}\left[ \frac{(X - \mu)^2}{k^2 \sigma^2} \geq 1 \right] \\
  &\leq \mathbb{E}\left[ \frac{(X - \mu)^2}{k^2 \sigma^2} \right]
    \quad \text{(by the lemma)} \\
  &= \frac{1}{k^2 \sigma^2} \mathbb{E}\left[(X-\mu)^2\right] \\
  &= \frac{1}{k^2}. \quad \text{(since that expected value is $\sigma^2$ by definition)}
\end{aligned}
\]

For \(k=1\), we result is trivial: at most 100\% of values fall outside
1 standard deviation from the mean. (An upper bound of 100\% tells us
nothing.) For \(k=2\), at most \(1/4 = 25\%\) of values fall outside 2
standard deviations. That is, 75\% fall inside. For \(k=3\), 89\% fall
inside. These are much more conservative results than for the normal
distribution, for which 95\% of values fall within 2 standard deviations
and 99.7\% fall within 3.

I'm not sure of the practical utility of this inequality. It requires
knowing the true variance, which already requires a whole bunch of data.

\subsection{\texorpdfstring{Visualiztion of how \(\hat{p}\) changes with
\(n\)}{Visualiztion of how \textbackslash{}hat\{p\} changes with n}}\label{visualiztion-of-how-hatp-changes-with-n}

\begin{verbatim}
n = 1000
p = 0.3
x = rbinom(n, 1, p)
cumx = cumsum(x)
cumn = cumsum(rep(1, n))
phat = cumx / cumn
cil = mapply(function(x, n) binom.test(x, n)$conf.int[1], cumx, cumn)
ciu = mapply(function(x, n) binom.test(x, n)$conf.int[2], cumx, cumn)

data_frame(flip=x, phat, cil, ciu) %>%
  mutate(x=1:n()) %>%
  ggplot(aes(x, phat)) +
  geom_ribbon(aes(ymin=cil, ymax=ciu), fill='grey80') +
  geom_line() +
  geom_hline(yintercept=p, linetype=2)
\end{verbatim}

\section{$\chi^2$ test}

Say you have $k$ iid standard normal random variables:
\begin{equation}
X_i \stackrel{\text{iid}}{\sim} \mathcal{N}(0, 1).
\end{equation}
Then $Y = \sum_{i=1}^k$ is $\chi^2$-distributed with $k$ degrees of freedom.

Let's start with a simple case where you have a table with two cells with
expected probabilities $p_1$ or $p_2 = 1-p_1$. We got $n$ total observations,
with $O_1$ in the first cell and $O_2 = n - O_1$ in the second. You probably
remember how to compute the test statistic from Stats 101:
\begin{equation}
\chi^2 = \sum_{k=1}^2 \frac{(O_i - E_i)^2}{E_i} = \frac{(O_1 - np_1)^2}{np_1} + \frac{(O_2 - np_2)^2}{np_2},
\end{equation}
where $E_i$ is the ``expected'' number of counts in each cell.

Consider the numerator of the second term:
\begin{equation*}
(O_2 - np_2)^2 = \left[(n - O_1) - n(1 - p_1)\right]^2 = (-O_1 + np_1)^2 = (O_1 - np_1)^2.
\end{equation*}
Handy, that's the same as numerator of the first term! That means we can re-write things:
\begin{equation*}
\chi^2 = \frac{(O_1 - np_1)^2}{n}\left( \frac{1}{p_1} + \frac{1}{p_2}\right).
\end{equation*}
A little algebra shows that $1/p_1 + 1/p_2 = 1/p_1(1-p_1)$, so that
\begin{equation}
\chi^2 = \frac{(O_1 - np_1)^2}{np_1(1-p_1)} = \left( \frac{O_1-np_1}{\sqrt{np_1(1-p_1)}} \right)^2.
\end{equation}
That might look terrible, but it's actually pretty cool. Here's why: $O_1$ is the observed value, $np_1$ is the expected mean, and $\sqrt{np_1(1-p_1)}$ is the standard deviation of the binomial distribution. I'll re-write that last equation with more suggestive notation:
\begin{equation}
\chi^2 = \left( \frac{x_1 - \mu_1}{\sigma_1} \right)^2
\end{equation}

This certainly \emph{looks} like a $\mathcal{N}(0, 1)$ variable, although we
said previously that the counts in the two cells follow a binomial
distribution. This is where the central limit theorem comes in: the sum of any
large set of (well-behaved) iid random variables approaches a normal
distribution. The binomial distribution approaches the normal distribution
particularly quickly such that (if the distribution is not highly skewed) you
only need about 5 counts for the normal approximation to be pretty
good.\footnote{The normal approximation to the binomial was proved long before
the central limit theorem. This special case, called the \emph{de Moive-Laplace
theorem}, was first published by de Moivre in 1738. Laplace published
the reverse result, that the binomial approximates the normal, 75 years later,
in 1812. The general central limit theorem was proven, more than 150 years
after de Moivre's original result only, in 1901 by Lyapunov.}

So, so long as each cell has (ish) 5 or more counts, then we can approximate
the binomial variables with normal variables, which means that the test
statistic $\chi^2$ that I wrote is actually just the square of a single,
standard normal variable, which happens to be $\chi^2$-square distributed with
1 degree of freedom. Two cells in the table ($k=2$) meant $k-1=1$ degrees of
for the $\chi^2$ distribution.

The same result holds, that the sum of the $(O_i - E_i)^2/E_i$ values follows
a $\chi^2$ distribution with $k-1$ degrees of freedom, for $k>2$. The math is
a lot more involved because the $k$ cells in the table are distributed
according to a multinomial distribution. In other words, conditioned on the
total number $n$ of counts, the values in the different cells are not
independent: if cell 1 has a lot of counts, cells 2, 3, etc. can't have that
many cells. Like we've seen before, covariance makes the calculations hard!
Nevertheless, the same restrictions apply: you can only count on the normal
approximation working if you have enough counts in every cell.


\end{document}
