%!TEX root=main.tex

\chapter{Introduction to statistical testing}

\section{Statistical testing is young}

Statistical hypothesis testing, or significance testing, is one of the most
widespread methods in science. If you pick up an article of \textit{Science} or
\textit{Nature} and read any scientific article, there will be a $p$-value in
it. I put statistical tests after the incredibly fundamental concepts of
observation, quantitation, and experimentation as the building blocks of
contemporary science.

Statistical testing is the youngest of these fundamental concepts. In the
Western scientific tradition, observation as a method goes back to Aristotle
(b. 384 BC). The ancient Greeks were also using quantitative measurements to
determine things like the diameter of the Earth. Experimentation may as we now
think of it was championed by Francis Bacon (b. 1561 AD) but probably has much
older roots. Our earliest examples of statistical hypothesis testing date to
the 1700s ---one of the early examples will be explored in a later chapter---
but testing as we now understand it was formalized by Ronald Fisher (b. 1890
AD), Jerzy Neyman (b. 1894 AD), and Egon Pearson (b. 1895 AD).

The fact that a young method, whose formalization is less than 100 years old,
has permeated nearly all of science speaks to its intellectual appeal. I think
it also clarifies why statistical testing and interpretation of $p$-values is
such a controversial issue in contemporary science: we as a scientific
community simply have not had enough time to fully digest the idea. Not only is
it not fully refined, we have also not found the best ways to explain it to one
another.

\section{Statistical inference is philosophically confusing}

Aside from being a relatively young idea, statistical testing and
statistical inference run into some of the critical philosophical
foundations of probability theory. My experience is that my colleagues
stumble on the philosophical problems of statistical inference as much as
on the mathematical aspects! Rather than sweep those under the rug, I want
to lay them out, to avoid confusion later.

\emph{Probability} is, mathematically speaking, a well-defined concept: it
is a function (more technically, a ``measure'') that links elements in
a set of possible outcomes of an experiment with numbers between $0$ and
$1$. For example, if I show the outcome ``coin flip comes up heads'' to
the probability function, it will return to me the number $\tfrac{1}{2}$.

\subsection{Frequentist and Bayesian probability}

Philosophically, probability has two main interpretations:
\emph{frequentist} and \emph{Bayesian}. When doing math and science,
frequentist probability statistics is the default: when people say
``statistics'', they almost always mean frequentist statistics, unless
they specifically say ``Bayesian statistics''. The principal mistake made
in interpreting statistical tests is that, although the test was
frequentist in construction, it is interpreted in a Bayesian way.

It is therefore important to understand the two different definitions of
probability and their implications for inference:
\begin{itemize}

\item \emph{Frequentist probability is a proportion.} ``The probability of
outcome $X$ from this experiment is $p$'' means that, as the number of 

\end{itemize}
