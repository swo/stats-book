{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>Welcome to the stats book!</p>"},{"location":"#why-this-book","title":"Why this book?","text":"<p>As a graduate student and postdoc in the life sciences, I saw that many of my colleauges had substantial training and experience in quantitative methods. They were very able to hack together sensible ways to approach their statistical problems. I found, however, that there was often a steep drop-off in their ability to apply statistical rigor to these ad hoc methods.</p> <p>I think this gap between the ability to hack something together and the understanding to make something rigorous is partly born out of a gap in educational materials. There are plenty of introductory statistics textbooks that explain what a mean is. There are also plenty of statistical test cookbooks that well you what assumptions are made when using a \\(t\\)-test. And finally, there are plenty of books and articles on statistics meants for people with a graduate-level education in statistics or math. However, there are few resources for people who are mature and shrewd quantitative thinkers but who do not have a half dozen statistics courses under their belt.</p> <p>To me, this situation is analogous to when I tried to learn a foreign language as an adult. It was easy for me to find books written for children. These books are at my reading level in terms of grammar and vocabulary, but they are thematically boring. On the other hand, books for adults are thematically interesting but completely intractable in terms of vocabulary and grammar. I wanted to write a book about statistics that is thematically and \"gramatically\" appropriate.</p> <p>I am not a statistician, and this is not a book for people who want to push the boundaries of what statisticians think are interesting problems. It is also not a cookbook that tells you what statistical test to run on your data. There are plenty of those already. Instead, my goal is to give you the ability to reason about why to do a statistical test and how to formulate your own. Rather than telling you which steps to generate a \\(p\\)-value in a specific case, I show how \\(p\\)-values come about in general and how to derive them for specific cases.</p> <p>I hope you find it useful!</p>"},{"location":"appendix/","title":"Appendix","text":"<p>Before diving into that equation, note that the \\(X_i\\) and \\(e{X}\\) are random variables, and note that they are not independent: the value of \\(\\overline{X}\\) certainly depends on each of the \\(X_i\\). So first let's imagine a simpler case, where we're in a universe where happen to know the expected value of the distribution we're trying to determine the variance of. To make the equations simpler to read, I'll use the standard notation \\(\\mu \\equiv \\mathbb{E}[X]\\).[^1] In this case, having known expected value (\"kEV\"), our estimator will be a little simpler: \\(\\(\\hat{\\mathbb{V}}_{X,\\mathrm{kEV}} = \\frac{1}{N} \\sum_i ( X_i - \\mu )^2\\)\\)</p>"},{"location":"appendix/#bias-in-estimators-of-variance","title":"Bias in estimators of variance","text":"<p>The known-expected-value estimator is unbiased: \\((\\begin{aligned} \\mathbb{E}[\\hat{\\mathbb{V]}_{X,\\mathrm{kEV}}}   &amp;= \\mathbb{E}[\\frac{1]{N} \\sum_i ( X_i - \\mu )^2} \\   &amp;= \\frac{1}{N} \\sum_i \\mathbb{E}[X_i^2 - 2\\mu X_i + \\mu^2] \\   &amp;= \\mathbb{E}[X^2] - 2 \\mu \\mathbb{E}[X] + \\mu^2 \\quad\\text{(since \\(X_i\\) are identic. distrib.)} \\   &amp;= \\mathbb{E}[X^2] - \\mu^2 \\   &amp;= \\mathbb{V}[X]. \\quad\\text{(since \\(\\mathbb{V}[X] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\))} \\end{aligned}\\)\\) So if we happen to know the true expected value \\(\\mu\\), then we can compute variance in the naive way, and it's exactly correct.</p> <p>Now we can use this result to pull a little mathematical trick. Even if we don't know \\(\\mu\\) and \\(\\mathbb{V}[X]\\), we do know they exist, so we can manipulate [eq:estimator-vx1]{reference-type=\"eqref\" reference=\"eq:estimator-vx1\"} to make it easier to work with. Specifically, I'm going to write \"standardized\"[^2] random variables: \\(\\(Z_i = \\frac{X_i - \\mu}{\\sqrt{\\mathbb{V}[X]}} \\implies X_i = \\sqrt{\\mathbb{V}[X]} Z_i + \\mu\\)\\) It should be easy to see that \\(Z_i\\) has expected value 0 and variance \\(\\mathbb{E}[Z_i^2] = 1\\), and \\(\\overline{Z}\\) has expected value 0 and variance \\(\\mathbb{E}[\\overline{Z]^2} = 1/n\\). Now I'll rewrite [eq:estimator-vx1]{reference-type=\"eqref\" reference=\"eq:estimator-vx1\"} so it has \\(Z_i\\) instead of \\(X_i\\):</p> <p>$$ \\begin{aligned} \\hat{\\mathbb{V}}_X   &amp;= \\frac{1}{n} \\sum_i \\left( X_i - \\overline{X} \\right)^2 \\   &amp;= \\frac{1}{n} \\sum_i \\left( \\left[\\sqrt{\\mathbb{V}[X]} Z_i + \\mu\\right] - \\left[ \\sqrt{\\mathbb{V}[X]} \\overline{Z} + \\mu \\right] \\right)^2 \\   &amp;= \\frac{\\mathbb{V}[X]}{n} \\sum_i \\left( Z_i - \\overline{Z} \\right)^2 \\end{aligned}$$ Analogous to how we showed that \\(\\mathbb{V}[X] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\), some algebra shows that \\(\\(\\sum_i \\left(Z_i - \\overline{Z}\\right)^2 = \\sum_i Z_i^2 - n \\overline{Z}^2.\\)\\) Thus, the expected value of this estimator is \\(\\(\\begin{aligned} \\mathbb{E}[\\hat{\\mathbb{V]}_X}   &amp;= \\mathbb{E}[\\frac{\\mathbb{V}[X]]{n} \\sum_i \\left( Z_i - \\overline{Z} \\right)^2} \\\\   &amp;= \\frac{\\mathbb{V}[X]}{n} \\mathbb{E}[\\sum_i Z_i^2 - n \\overline{Z]^2} \\\\   &amp;= \\frac{\\mathbb{V}[X]}{n} \\left( \\sum_i \\mathbb{E}[Z_i^2] - n \\mathbb{E}[\\overline{Z]^2} \\right)\\\\   &amp;= \\frac{\\mathbb{V}[X]}{n} (n - 1) \\quad\\text{(using the little identities)} \\\\   &amp;= \\frac{n-1}{n} \\mathbb{V}[X]. \\end{aligned}\\)\\) Note that the expected value of our estimator is not equal to the thing we're trying to estimate, so this estimator is biased! It systematically underestimate the true variance. Interestingly, like when we tried to cook up an estimator for the upper limit of a uniform distribution, we ended up with something that was off by a multiplicative factor. The solution is to make a new estimator that has the inverse, cancelling factor out front: \\(\\(\\hat{\\mathbb{V}}_{X,\\mathrm{unbiased}}   = \\frac{n}{n-1} \\times \\frac{1}{n} \\sum_i \\left(X_i - \\overline{X}\\right)^2   = \\frac{1}{n-1} \\sum_i \\left(X_i - \\overline{X}\\right)^2.\\)\\) Using \\(n-1\\) instead of \\(n\\) in the denominator is known as Bessel's correction.[^3]</p> <p>This equation may look familiar as the \"sample variance\", typically written \\(\\(s^2 = \\frac{1}{n-1} \\sum_i \\left(x_i - \\overline{x}\\right)^2.\\)\\) I always wondered why, in Stats 101, I was told that the mean had \\(n\\) in the denominator but variance had \\(n-1\\).[^4] This is why! The point is to make an unbiased estimator.</p> <p>[^1]: Do remember, however, that \\(\\overline{X}\\) is a random variable---a     function of the data---while \\(\\mathbb{E}[X]\\) is a function of the     distribution, and therefore just a single matter-of-fact number.</p> <p>[^2]: You're probably used to seeing standardized normal variables     written this way, but note that I haven't assumed that the \\(X_i\\) are     normally distributed. This logic holds for any random variable that     has a well-defined expected value and variance.</p> <p>[^3]: Gauss was using the correction before Bessel discovered it, as     early as 1823. I hope the late date, 1823, impresses upon you how     subtle this reasoning must be. The ancient Greeks were using the     arithmetic mean, but an unbiased estimator for standard deviation     took thousands of years.</p> <p>[^4]: The traditional answer says that it's about \"degrees of freedom\":     you're \"using up\" one \"degree of freedom\" when computing     \\(\\overline{x}\\), so you only have \\(n-1\\) to compute \\(s^2\\). \"Degrees of     freedom\" is an ill-defined concept that I don't think it useful.     Explaining one mystery in terms of another mystery is not good     pedagogy!</p>"},{"location":"appendix/#things-to-include","title":"Things to include","text":"<ul> <li>Gauss: a minimum-variance, mean-unbiased estimator minimizes the   squared-error loss function. Laplace: among median-unbiased   estimators, a minimum-average-absolute-deviation estimator minimizes   the absolute loss function. Maybe it's better to allow some bias so   you can get less variance. That's the domain of statistical theory.   [Move to MLE section?]{.mark}</li> </ul> <ul> <li>Fisher's crazy sum test is the same thing as is used in TRANSIT   (DeJesus et al.): they treat TA sites in the same gene as   independent; the statistic is the difference in the sum of the   (normalized) number of insertions in two treatments; the null   distribution is generated by shuffling the values across the two   datasets. OK, it's not exactly like Fisher's test, since it's not   paired, but it's pretty close. Fisher probably wouldn't have wanted to   to the \\(\\binom{n}{2}\\) options, compared to the \\(2^n\\) that he did.   [Example of how people come up with tests?]{.mark}</li> </ul>"},{"location":"appendix/#estimators-about-estimators","title":"Estimators about estimators","text":"<p>[Move this up, into the descriptive section]{.mark}</p>"},{"location":"appendix/#jackknife","title":"Jackknife","text":"<p>You have \\(n\\) data points and compute an estimator \\(\\hat{\\theta}\\) for some population parameter \\(\\theta\\). If you don't know how the population is structured, then it's not clear what you expect the variance of \\(\\hat{\\theta}\\) to be. How sure can you be of this value? In terms of inference, can you make any inference with it?</p> <p>Compute the jackknife replicates[^1] \\(\\hat{\\theta}_j\\), which are the estimators computed using all the data points except the \\(j\\)-th one.</p> <p>That seems like a weird thing to have done, but you can use them to compute two handy things:</p> <ol> <li> <p>An estimate of the variance of the estimator. This can help you for     description---by giving a confidence interval(?)---and for     inference---by giving you a sense of the \"random\" ranges you would     expect from two samples.</p> </li> <li> <p>An estimate of the bias in the estimator. This is helpful if you     don't want want your estimator to be biased but you don't know how     to fix it.</p> </li> </ol>"},{"location":"appendix/#jackknife-variance-estimator","title":"Jackknife variance estimator","text":"<p>The variance estimator is \\(\\(\\widehat{\\mathrm{Var}}_\\mathrm{jk}[\\hat{\\theta}] := \\frac{n-1}{n}  \\sum_j \\left( \\hat{\\theta}_j - \\hat{\\theta}_{(\\cdot)} \\right)^2,\\)\\) where \\(\\hat{\\theta}_{(\\cdot)}\\) is the average of the jackknife replicates: \\(\\(\\hat{\\theta}_{(\\cdot)} := \\frac{1}{n} \\sum_j \\hat{\\theta}_j.\\)\\) In other words, it's the variance of the jackknife replicates with some rescaling: $$\\mathrm{Var}[\\hat{\\theta}j] = \\frac{1}{n-1} \\sum_j \\left( \\hat{\\theta}_j - \\hat{\\theta}{(\\cdot)} \\right)^2 \\implies   \\widehat{\\mathrm{Var}}_\\mathrm{jk}[\\hat{\\theta}] = \\frac{(n-1)^2}{n} \\mathrm{Var}[\\hat{\\theta}_j]. $$</p> <p>The reason for that scaling factor is beyond the scope of this book (Efron &amp; Stein 1981?), but the exercise gives you a sense of why it has to be true for a specific case.</p> <p>Some other work, also beyond the scope of this book, shows that the jackknife estimate of variance is biased: it tends to overestimate the true variance. This makes the jackknife a conservative tool.</p> <p>Exercise. Let \\(\\theta\\) be the mean. Show that the scaling factor is what we think. Hints:</p> <ul> <li>Show that \\(\\hat{\\theta}_{(\\cdot)}\\) is the sample mean.</li> </ul> <ul> <li>Show that \\(\\hat{\\theta}_j - \\hat{\\theta}_{(\\cdot)} = (n \\overline{x} - x_j) / (n - 1)\\).</li> </ul> <ul> <li>Show that that value is equal to \\((\\overline{x} - x_j) / (n - 1)\\).</li> </ul> <p>That exercise is from McIntosh's bioRxiv about jackknife resampling.</p>"},{"location":"appendix/#jackknife-bias-estimator","title":"Jackknife bias estimator","text":"<p>The jackknife estimate of bias is \\((n-1) \\left( \\hat{\\theta}_{(\\cdot)} - \\theta \\right)\\). This is the sum of the deviations of the jackknife replicates from the observed value \\(\\hat{\\theta}\\). Again, the reason that you would take the average deviation and scale it up to the sum is beyond the scope.</p> <p>However, if you have an expectation about the bias in an estimator, you can make an unbiased estimator by subtracting out that bias: \\(\\(\\hat{\\theta}_\\mathrm{jk} := \\hat{\\theta} - \\widehat{\\mathrm{Bias}}_\\mathrm{jk}[\\theta].\\)\\)</p> <p>Exercise. Show that the jackknife estimate of bias for the variance gives you the familiar unbiased variance estimator.</p> <p>Exercise. Something about the maximum estimator?</p>"},{"location":"appendix/#pros-and-cons-of-the-jackknife","title":"Pros and cons of the jackknife","text":"<p>It's a piece of cake to implement. There are only \\(n\\) replicates to do, so it's tractable. Those replicates are deterministic, so you only run it once.</p> <p>The cons are that it doesn't always work. For example, a jackknife estimate of the variance of a median (swo check Knight) is not consistent. It's also overly conservative: it's biased toward higher variances. You can rescue some properties if you move to a delete-\\(d\\) resampling and pick \\(d\\) from the correct range.</p>"},{"location":"appendix/#bootstrap","title":"Bootstrap","text":""},{"location":"appendix/#example-from-efron-thinking-the-unthinkable","title":"Example from Efron, \"Thinking the unthinkable\"","text":"<p>There's some true distribution \\(f_X(x)\\), and you're approximating it with \\(\\hat{f}_X(x)\\), which is a pmf. If you took \\(N\\) data points, then bootstrapping means that you're picking a vector \\(\\vec{c}\\), where \\(c_i\\) is the number of times that the \\(i\\)-th data point makes it into the bootstrap sample. This begs the question, how is \\(\\vec{c}\\) behaved? It's just a multinomial, with probability \\(1/N\\) for each of the \\(N\\) cells.</p> <p>Normally you compute a statistic \\(T(\\vec{x})\\) of the data. Instead, formulate this in terms of a function \\(g(\\vec{c})\\) If you can write \\(T(\\vec{x}) = \\sum_i t(x_i)\\), then \\(g(\\vec{c}) = \\sum_i (c_i/N) t(x_i)\\). In a Taylor expansion around \\(\\vec{c}_\\mathrm{ML} = (1, 1, \\ldots, 1)\\): \\(\\(g(\\vec{c}) = g(\\vec{c}_\\mathrm{ML}) + \\sum_i \\frac{dg}{dc_i} (c_i - 1) + \\mathcal{O}(c_i^2)\\)\\) So the variance of the values \\(g(\\vec{c})\\) that you will get from bootstrapping is approximately \\(\\(\\begin{aligned} \\mathbb{E}[\\left[g(\\vec{c]) - g(\\vec{c}_\\mathrm{ML})\\right]^2}   &amp;= \\mathbb{E}[\\left(\\sum_i \\frac{dg]{dc_i} (c_i-1)\\right)^2} + \\mathcal{O}(c_i^2) \\\\   &amp;= \\mathbb{E}[\\sum_i \\left( \\frac{dg]{dc_i} (c_i-1)\\right)^2} + \\mathcal{O}(c_i^2) \\\\   &amp;= \\sum_i \\left(\\frac{dg}{dc_i}\\right)^2 \\mathbb{E}[(c_i-1)^2] + \\mathcal{O}(c_i^2) \\\\ \\end{aligned}\\)\\)</p> <p>And then an \\(n^2\\) comes out? The point is that the jackknife is basically doing a finite estimation of the gradient, by leaving out a single point at a time.</p>"},{"location":"appendix/#what-does-it-mean-to-sample","title":"What does it mean to \"sample\"?","text":"<p>[Earlier in text, get clearer about RVs and their \"realizations\". and \"samples\"]{.mark}</p> <p>Does it make sense to compute a confidence interval when you're sampled all the 50 United States?</p> <p>Finite correction factor to point out that there's a difference between simple random sampling and something else. Then need to explain what simple random sampling is!</p>"},{"location":"appendix/#t-distribution","title":"t-distribution","text":"<p>[Maybe just mention in passing how difficult the math gets when you want to estimate many quantities simultaneously? Contrast t- and z-tests]{.mark}</p> <p>Let's think about how to construct that method. Say you knew the true variance \\(\\sigma^2\\). Then we know that the sample means are drawn from \\(\\mathcal{N}(0, \\sigma^2/n)\\). So it's pretty easy to see that \\((\\overline{x} - \\mu) / (\\sigma^2) \\sim \\mathcal{N}(0, 1)\\), from which the familiar \\(1.96\\), etc. come.</p> <p>What if you don't know the true variance? The means are still drawn from \\(\\mathcal{N}(0, \\sigma^2/n)\\), but now the sample variance is also a random variable.</p> <p>We know the confidence interval is some function of the sample mean and variance, and let's guess that it's symmetric about the sample mean and is some linear function of sample variance: \\(\\(\\mathrm{CI}_\\pm(\\overline{x}, s) = \\overline{x} \\pm A s.\\)\\) We want to find \\(A\\) such that \\(\\(\\mathbb{P}\\left[ \\mathrm{CI}_- &lt; \\mu &lt; \\mathrm{CI}_+ \\right] = 95\\%,\\)\\) or, if we're willing to trust in symmetry, \\(\\(2.5\\% = \\mathbb{P}\\left[ \\mathrm{CI}_- &gt; \\mu \\right] = \\mathbb{P}\\left[ \\frac{\\overline{x} - \\mu}{A} - s &gt; 0 \\right].\\)\\) We know the distribution of the first thing: \\(\\((\\overline{x}-\\mu)/A \\sim \\mathcal{N}\\left(0, \\frac{\\sigma^2}{n A^2}\\right).\\)\\) Some math shows that \\(\\(\\frac{(n-1) s^2}{\\sigma^2} \\sim \\chi^2(n-1).\\)\\)</p> <p>Call the first thing \\(K\\) and the second \\(L\\). We're interested in the distribution of \\(M \\equiv K - L\\): \\(\\(f_M(m) = \\int_0^\\infty f_K(m + l) f_L(l) \\,\\mathrm{d}l,\\)\\)</p> <p>where the limits come from the fact that variance is positive. You're probably not excited to do this integral, which was considered a major achievement (well, it was the thought leading up to the integral, which we've just outlined, but whatever). This major achievement was made by William Sealy Gosset, who made it while he was a researcher for Guinness ensuring the quality of their beer. Guinness had a policy of not allowing its employee to publish their results, so Gosset signed his paper \"a student\", so the result of that integral is now called Student's t-distribution:</p> <p>$$ f_t(x; \\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\sqrt{\\nu\\pi} \\Gamma\\left(\\frac{\\nu}{2}\\right)}   \\left(1+ \\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}},$$ where the (badly named) \"degrees of freedom\" \\(\\nu\\) is \\(n-1\\) for our purposes. I write this out fully because it is one of the things we will not derive in this book.</p>"},{"location":"appendix/#contingency-tables","title":"Contingency tables","text":"<p>[Move Fisher's exact, Barnard, chi-square up, into a section on p-values? Or maybe statistical power?]{.mark}</p> <p>These are nice examples for how to do statistical thinking.</p>"},{"location":"appendix/#barnards-test","title":"Barnard's test","text":"<p>The classic example is whether a certain treatment causes more of the outcome of interest than just doing nothing. In medicine, that means splitting your participants into a placebo group and a treatment group and asking what fraction of each gets well. In a biology experiment, you might split your mice into a treatment group and a control group and ask what proportion of the mice in each group get cancer.</p> <p>In statistics jargon, this is called a \\(2 \\times 2\\) contingency table:</p> <p>Group         Outcome \\(p\\)   Outcome not-\\(p\\)   Row sums</p> <p>A             \\(a\\) \\(c\\) \\(m\\)   B             \\(b\\) \\(d\\) \\(n\\)   Column sums   \\(r\\) \\(s\\) \\(N\\)</p> <p>Because we picked \\(m\\) and \\(n\\), the sizes of the two groups, those are fixed parameters. The question is whether the way that \\(m\\) gets distributed into \\(a\\) and \\(c\\) (and that way that the \\(n\\) get put into the \\(b\\) and \\(d\\)) is consistent with there being a common probability \\(p\\) of the outcome of interest.</p> <p>So we might say that \\(a\\) is distributed like a binomial distribution with \\(m\\) draws and probability \\(p_a\\) of success, and \\(b\\) is distributed like a binomial with \\(n\\) draws and a probability of \\(p_b\\) of success. The null hypothesis is that \\(p_a = p_b\\). What's the likelihood of the data given the null?</p> <p>If we didn't assume the null, and gave the two binomials their own probabilities, the likelihood of the data would be: \\(\\(P(a, b | p_a, p_b) = \\mathrm{Bin}(a; m, p_a) \\times \\mathrm{Bin}(b; n, p_b).\\)\\) But, given that the probabilities are the same, we can collapse it: $$\\begin{aligned} \\mathcal{P}[a, b | p_a = p_b = p] &amp;= \\mathrm{Bin}(a; m, p) \\times \\mathrm{Bin}(b; n, p) \\   &amp;= \\binom{m}{a} p^a (1-p)^{m-a} \\times \\binom{n}{b} p^b (1-p)^{n-b} \\   &amp;= \\binom{m}{a} \\binom{n}{b} p^{a+b} (1-p)^{m+n-(a+b)} \\   &amp;= \\frac{m! \\, n!}{a! \\, b! \\, c! \\, d!} p^r (1-p)^s. \\end{aligned} $$</p> <p>This result is a little confusing[^2], for two reasons:</p> <ol> <li> <p>The probability \\(p\\) of the outcome of interest might be interesting to design a later experiment, but it's not interesting for designing a test. We certainly don't want to deliver a result like, \"Well, if the null hypothesis is true, and \\(p\\) happens to be exactly such-and-such, then your \\(p\\)-value is so-and-so.\" The value \\(p\\) is called a nuisance parameter since we don't actually care about its value.</p> </li> <li> <p>We're usually not interested in the likelihood of exactly this data, but rather in the likelihood of data at least this extreme. We usually measure \"extremeness\" using a statistic---a single number---so it's clear that \"more extreme\" means \"bigger\" (or \"smaller\" or \"bigger or smaller\", depending on if it's a one-sided or two-sided test). Here, we have two numbers, \\(a\\) and \\(b\\), so there aren't two \"sides\" to the distribution: there are four!</p> </li> </ol> <p>To resolve the first point, we say that the null hypothesis \\(p_a = p_b = p\\) doesn't restrict us to a particular value of \\(p\\). In other words, the null hypothesis, which functions as a sort of Annoying Skeptic, is free to pick \\(p\\) to make our results as uninteresting as possible. Mathematically, this means that, when computing the \\(p\\)-value, we should optimize over all values of \\(p\\), choosing the one that makes our results as uninteresting as possible (i.e., which maximizes the \\(p\\)-value).</p> <p>We can't really \"resolve\" the second point, since it demonstrates that our previous way of thinking about extremeness was not sufficient for all cases. As Barnard notes in his original paper[^3], there are actually many ways to choose the pairs \\((a, b)\\) that produce a \\(p\\)-value more than our threshold. This gets into some fancy footwork to articulate exactly how you should pick this area, but the basic results are pretty intuitive: when \\(a/m\\) and \\(b/n\\) are similar, you tend to be under the rejection threshold; when they are far apart, you tend to be over.</p> <p>The interesting point here is that, whatever fancy footwork you pick to choose that region, and no matter how \"reasonable\" your footwork is, it's still footwork that doesn't obviously follow from the simple definition of a hypothesis test. We'll encounter this problem again in Bayesian statistics, when we find that the Bayesian analog of a confidence interval is not unique: there are many ranges of values that are compatible with our ignorance.</p>"},{"location":"appendix/#fishers-test-to-the-rescue","title":"Fisher's test to the rescue(?)","text":"<p>If you've worked with contingency tables, you're probably saying, \"I've never heard of this crazy Bernard's test, with its weird multi-sided rejection space and its requirement to maximize over \\(p\\). We have Fisher's exact test, which is the exactly right test to use here!\"</p> <p>Looking at the same contingency table, Fisher's test asks, given the row marginals \\(m\\) and \\(n\\), the first column marginal \\(r\\), and the grand total \\(N\\), what is the probability of a table at least this extreme?</p> <p>This is just a combinatoric problem: if you're as likely to assign items in \\(m\\) to \\(a\\) as to \\(c\\) (and, analogously, to assign items from \\(n\\) to \\(b\\) or \\(d\\)), then \"what's the probability of this table\" is equivalent to asking \"given the marginals, how many ways are there to choose this table?\". More specifically, how many ways are there to choose \\(a\\) items from a bank of \\(m\\) items and \\(b\\) items from a bank of \\(n\\), given that we chose \\(r = a + b\\) items from the total \\(N\\)? Mathematically: \\(\\(\\mathbb{P}[a | m, n, r, s] = \\frac{\\binom{m}{a} \\binom{n}{b}}{\\binom{N}{r}} = \\frac{m! \\, n! \\, r! \\, s!}{N! \\, a! \\, b! \\, c! \\, d!}.\\)\\)</p> <p>Computing the \\(p\\)-value is easier here than with Barnard's test because we need to keep the row and column marginals the same. In Barnard's test, we just kept the row marginals constant, because we considered those as fixed parameters, corresponding to things like the number of patients we assigned to each of the placebo and treatment groups. It doesn't make sense to allow the Annoying Skeptic to fiddle with those values.</p> <p>In Banard's test, we did allow the Annoying Skeptic to fiddle with the column marginals, since it wasn't clear, before the experiment began, that \\(r\\) would have the outcome of interest. In other words, we didn't know that \\(r\\) people in both the placebo and treatment groups would get well.</p> <p>Fisher's test, however, does keep the column marginal constant. This makes it a lot easier to compute the \\(p\\)-value. First, the nuisance parameter \\(p\\) doesn't appear in the likelihood, so we don't need to do the weird maximization. Second, we only need to vary one value, \\(a\\) (or, equivalently, \\(b\\)), since, if you know the marginals, there is only one axis along which to change the values in the table. In other words, if you know \\(\\(\\begin{aligned} a + c &amp;= m \\\\ b + d &amp;= n \\\\ a + b &amp;= r, \\end{aligned}\\)\\) then that's three equations with four unknowns (\\(a\\), \\(b\\), \\(c\\), \\(d\\)), so specifying any one of \\(a\\), \\(b\\), \\(c\\), or \\(d\\) specifies all the others. (You might be looking for a fourth equation \\(c + d = s\\), but you can get that by adding the first two equations and subtracting the third.)</p> <p>Here's an example:</p> <p>Group Success Failure Row sums</p> <p>A 1 9 10 B 11 3 14 Column sums 12 12 14</p> <p>There's only one way to make this table more \"extreme\" without changing the marginals: you can take the one group A success and make it a group A failure and simultaneously make a group B failure into a group B success. Similarly, there's only one way to make this table less extreme: turn a group A failure into success, and turn a group B success into failure.</p> <p>So keeping the column sums constant made it way easier to compute the \\(p\\)-value: count this table and all the tables with a more extreme upper-left or bottom-right and see if your summed probability hits the rejection threshold.</p> <p>However, this simplicity came at a cost, which you may have noticed: does it make sense to keep the columns constant? Experimentally, this means that you're restricting the Annoying Skeptic to only consider cases in which, say, the number of patients who got well in both groups is equal to the experimentally observed value. This is a little weird. It suggest that your experimental design was like this:</p> <ol> <li> <p>Pick \\(m\\), \\(n\\), and \\(r\\).</p> </li> <li> <p>Assign \\(m\\) patients to placebo and \\(n\\) to treatment.</p> </li> <li> <p>Wait until \\(r\\) patients across both groups have gotten well.</p> </li> <li> <p>Stop the experiment.</p> </li> </ol> <p>This is almost certainly not reflective of how typical experiments are run[^4].</p> <p>Fisherian small data</p> <p>What happens if I use the \"wrong\" test? Chi-square as an example of wrongness</p>"},{"location":"appendix/#regression","title":"Regression","text":"<p>[Make this a chapter? In descriptive statistics? Or just say this regression is an ML problem? Link and error distributions.]{.mark}</p>"},{"location":"appendix/#chi2-test","title":"\\(\\chi^2\\) test","text":"<p>[Merge with section above on chi-square]{.mark}</p> <p>Say you have \\(k\\) iid standard normal random variables: \\(\\(X_i \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, 1).\\)\\) Then \\(Y = \\sum_{i=1}^k x^2\\) (??) is \\(\\chi^2\\)-distributed with \\(k\\) degrees of freedom.</p> <p>Let's start with a simple case where you have a table with two cells with expected probabilities \\(p_1\\) or \\(p_2 = 1-p_1\\). We got \\(n\\) total observations, with \\(O_1\\) in the first cell and \\(O_2 = n - O_1\\) in the second. You probably remember how to compute the test statistic from Stats 101: \\(\\(\\chi^2 = \\sum_{k=1}^2 \\frac{(O_i - E_i)^2}{E_i} = \\frac{(O_1 - np_1)^2}{np_1} + \\frac{(O_2 - np_2)^2}{np_2},\\)\\) where \\(E_i\\) is the \"expected\" number of counts in each cell.</p> <p>Consider the numerator of the second term: \\(\\((O_2 - np_2)^2 = \\left[(n - O_1) - n(1 - p_1)\\right]^2 = (-O_1 + np_1)^2 = (O_1 - np_1)^2.\\)\\) Handy, that's the same as numerator of the first term! That means we can re-write things: \\(\\(\\chi^2 = \\frac{(O_1 - np_1)^2}{n}\\left( \\frac{1}{p_1} + \\frac{1}{p_2}\\right).\\)\\) A little algebra shows that \\(1/p_1 + 1/p_2 = 1/p_1(1-p_1)\\), so that \\(\\(\\chi^2 = \\frac{(O_1 - np_1)^2}{np_1(1-p_1)} = \\left( \\frac{O_1-np_1}{\\sqrt{np_1(1-p_1)}} \\right)^2.\\)\\) That might look terrible, but it's actually pretty cool. Here's why: \\(O_1\\) is the observed value, \\(np_1\\) is the expected mean, and \\(\\sqrt{np_1(1-p_1)}\\) is the standard deviation of the binomial distribution. I'll re-write that last equation with more suggestive notation: \\(\\(\\chi^2 = \\left( \\frac{x_1 - \\mu_1}{\\sigma_1} \\right)^2\\)\\)</p> <p>This certainly looks like a \\(\\mathcal{N}(0, 1)\\) variable, although we said previously that the counts in the two cells follow a binomial distribution. This is where the central limit theorem comes in: the sum of any large set of (well-behaved) iid random variables approaches a normal distribution. The binomial distribution approaches the normal distribution particularly quickly such that (if the distribution is not highly skewed) you only need about 5 counts for the normal approximation to be pretty good.[^5]</p> <p>So, so long as each cell has (ish) 5 or more counts, then we can approximate the binomial variables with normal variables, which means that the test statistic \\(\\chi^2\\) that I wrote is actually just the square of a single, standard normal variable, which happens to be \\(\\chi^2\\)-square distributed with 1 degree of freedom. Two cells in the table (\\(k=2\\)) meant \\(k-1=1\\) degrees of for the \\(\\chi^2\\) distribution.</p> <p>The same result holds, that the sum of the \\((O_i - E_i)^2/E_i\\) values follows a \\(\\chi^2\\) distribution with \\(k-1\\) degrees of freedom, for \\(k&gt;2\\). The math is a lot more involved because the \\(k\\) cells in the table are distributed according to a multinomial distribution. In other words, conditioned on the total number \\(n\\) of counts, the values in the different cells are not independent: if cell 1 has a lot of counts, cells 2, 3, etc. can't have that many cells. Like we've seen before, covariance makes the calculations hard! Nevertheless, the same restrictions apply: you can only count on the normal approximation working if you have enough counts in every cell.</p>"},{"location":"appendix/#coda","title":"Coda","text":"<p>[New material]{.mark}</p> <p>- Bayes - Bayes sampling? - MCMC for copmlex models? - Nature Biotech Bayes example - Optimization for MLE - Regression and mixed models - Random variable neq variable with a random value - When discussing RVs, note that cdf defines it. Then don't ever talk about events, just look at joint cdfs, etc. Make this a whole section unto itself. - Do joint pdf's so it's easier to talk about independence</p> <p>Tony's ideas:</p> <p>- Neyman Pearson lemma - Likelihood ratio tests - Why is frequentist so good? CLT, convergence, etc. - Information theory, model simplicity, AIC?</p> <p>[^1]: The \"jackknife\" method is so called because Tukey compared the method, which is \"rough-and-ready\", to another rough-and-ready tool, the pocket knife, also known as a jackknife. Although this name has the disadvantage of giving you no clue what it is about, it had the advantage of having more brevity and vivacity than \"delete-1 resampling\", which is probably the more accurate name. [^2]: I trotted out this test because these two confusions are actually great learning opportunities. [^3]: Barnard conceived of the \\((a, b)\\) as points \"in a plane lattice diagram of points with integer co-ordinates\", that is, that \\(a\\) is like the \\(x\\)-axis and \\(b\\) is like the \\(y\\)-axis. Then the possible outcomes of the experiment are the points in the rectangle bounded by the horizontal lines \\(a = 0\\) and \\(a = m\\) and the vertical lines \\(b = 0\\) and \\(b = n\\). He then said that you should pick the non-extremal points (i.e., the values of \\((a, b)\\) for which you would not reject the null) such that they \"consist of as many points as possible, and should like away from that diagonal of the rectangle which passes through the origin. Formulated mathematically, these latter requirements mean that the [points for which you would reject the null] must in a certain sense be convex, symmetrical and minimal.\" [^4]: It is, however, the way the famous \"lady tea tasting\" experiment was designed. The myth is that Fisher didn't believe it when a high-class lady told him that she could detect whether tea was added to a cup with milk in it or whether the milk was added to the tea. He designed an experiment with \\(m\\) cups prepared one way, \\(n\\) prepared the other, and told her to detect the \\(r = m\\) cups that were prepared the first way. A Barnard-style experiment, in which the same \\(m\\) and \\(n\\) cups [^5]: The normal approximation to the binomial was proved long before the central limit theorem. This special case, called the de Moive-Laplace theorem, was first published by de Moivre in 1738. Laplace published the reverse result, that the binomial approximates the normal, 75 years later, in 1812. The general central limit theorem was proven, more than 150 years after de Moivre's original result only, in 1901 by Lyapunov. [Put CLT in with regression? Or z-test?]{.mark}</p>"},{"location":"appendix/#from-the-readme-material-to-fix","title":"From the Readme : Material to fix","text":""},{"location":"appendix/#introduction","title":"Introduction","text":"<ul> <li>Universality of statistics<ul> <li>Aside from the scientific method in general, and numbers, there is no other thing as widespread in the sciences</li> <li>Scientific questions eventually boil down to numbers, and so down to statistics. Your beautiful fascinating theory about frog evolution and flight paths or whatever eventually comes down to whether this heap of numbers is bigger than that heap, or whether this function is a better fit to this data than that function.</li> </ul> </li> <li>linear mixed models for GWAS is a good example of when biology becomes numbers</li> <li>mixing around counts for TnSeq is another good example</li> <li>Combination of observations<ul> <li>historical challenge of what to do with multiple observations</li> <li>temptation that persists to simply pick the best one</li> <li>crowdsourcing, poll the audience (who wants $1 million)</li> </ul> </li> </ul>"},{"location":"appendix/#probability","title":"Probability","text":"<ul> <li>Probability as counting techniques, which assumes that all events are equiprobable<ul> <li>law of ignorance</li> </ul> </li> <li>also need some more discussion earlier to say that thinking about \\(\\Omega\\) is really confusing. What sample space would give rise to a normally-distributed variable, for example? And where do we draw the liens around the universe we're interested in?<ul> <li>eg \"heads\" is an event, not an outcome. Exactly where the coin lands, etc. is the outcome. But we're always thinking at some level of abstraction. The theory is robust to that, thankfully.</li> </ul> </li> <li>Joint probability distributions, and how for iid, you get products</li> <li>Venn in 1866 saying that probability should be interpreted as frequency, not as educated guesses<ul> <li>If that seems backwards, recall that we're trying to understand what probability is, and in this case we're following the math to the world, not the other way around!</li> </ul> </li> </ul>"},{"location":"appendix/#random-variables","title":"Random variables","text":"<ul> <li>How you make RVs that are functions of other RV(s)<ul> <li>The sum as a simple example, which gives you the sample mean</li> <li>Then the difference</li> <li>And then the most general case</li> </ul> </li> <li>There's a <code>sample'' probability distribution $\\hat{f}_X$. It's hard to say whether this is a maximum likelihood estimator or something. What does</code>sample'' mean? Like \"sample mean\"?</li> <li>cdf is also the quantile function</li> </ul>"},{"location":"appendix/#underlying-distribution-vs-sample-distribution","title":"Underlying distribution vs. sample distribution","text":"<p>Up to this point, I've used the word <code>distribution'' in the mathematical probability sense: it's the cdf $F_X$ or the pmf/pdf $f_X$. It it now crucial to distinguish between these mathematical probability objects and, on the other hand, the mass of data that we get from an experiment, which might also be called the</code>distribution'' of the data. (As we'll see later, this confusion of language is not totally unwarranted, because the mass of data is itself a complicated estimator of the mathematical distribution.)</p> <p>The distinction between mathematical distributions and data distributions is usually made in the jargon of the social sciences: from some \\emph{population}, you draw a \\emph{sample}. The sample refers to the numbers you collected; the population refers to the larger universe of numbers your sample was drawn from. We use the sample to estimate properties of the population. For example, if you're interested in the height of men versus women in the US, then your target populations will be, say, all American males and all American females. The sample is whatever heights you actually measure. We then try to make some inference about all Americans based on our sample.</p> <p>Improve the formalism: A sample is the realization of a set of iid RVs, e.g. And then the statistic is itself an RV, that can have a realization.</p> <p>In the heights example, the target population is finite. There are only 300 million Americans. It is therefore theoretically possible to make a list of the heights of all Americans. In a repeatable experiment, on the other hand, the target population is infinite: you could theoretically repeat an experiment infinitely many times, drawing an infinite number of data points. In most cases, statistics proceeds as if the target population were infinite, although there are corrections you can make when your sample size starts to approach the size of the target population.</p> <p>Many statistical methods also assume that the target population follows some known distribution. The classic example is that the \\(t\\)-test assumes that the target populations are normally distributed. This can make the language a little confusing. If you are using the \\(t\\)-test to compare the heights of males and females, is the <code>true'' population the actual distribution of heights among the 300 million Americans, or is the</code>true'' population the normal distribution? In most cases, ``true'' is used to refer to the abstract mathematical distribution rather than the actual distribution of the target population.</p> <p>The important point is that the <code>sample'' distribution refers to the data you collected, while the</code>true'' or ``theoretical'' distribution refers to the theoretical approximation of the target population your data came from.</p> <p>In the <code>hard'' experimental sciences, this language about</code>population'' may sound weird. If I'm trying to measure the speed of light, I repeat my experiment many times and get many different values. Those values are the sample, but what is the population? We think there is a single, true speed of light, not a distribution of speeds of lights. In this case, the ``population'' refers, somewhat tautologically, to the distribution of the values that would come from an infinite number of repetitions of our experiment. In other words, the sample defines the population, not the other way around.</p> <p>In the social sciences example, the shape of the distribution of measured values says something about the variation of the values in the target population. We get a range of measured values because people are different heights. In the physics example, the shape of the distribution has everything lot to do with the precision in our experiment. We get a range of measured values because of all sorts of error that arise the in the process of experimentation.</p> <p>In biology, the situation is somewhere in between. We think there is true variation in, say, the physiology of the cells used in an experiment. We might wish there weren't. It would be easier to do some experiments if all cells started in exactly the same state and responded exactly the same way to an experimental condition. The meaning of the range of values we get is then something of a philosophical one. To what degree do we treat the range of values as due to error, because we couldn't make all the cells the same, and to what degree is it an honest report about the heterogeneity of behavior in some ``target population'' of cells? Multilevel models, which will come up later, are an attempt to deal with multiple sources of variation.</p>"},{"location":"appendix/#distributions","title":"Distributions","text":"<ul> <li>descriptions of the different distributions<ul> <li>mixture distributions</li> <li>Poisson as horse kicks, or cells in droplets</li> </ul> </li> <li>How these relate to each other</li> <li>Mean, variance, skew, kurtosis: legacy of Pearson's biometrics, thinking that all distributions could be described this way.<ul> <li>Something about moments in general</li> </ul> </li> <li>Gumbel, Tippet, Weibull for extreme events (maximum of distribution?)</li> </ul>"},{"location":"appendix/#estimators","title":"Estimators","text":"<ul> <li>efficiency and cramer rao for talking about the variance of estimators<ul> <li>probably bring this up but don't go too much into it, except to say that it's definitely going to be important for doing inferences.</li> </ul> </li> <li>Robustness using Efron's example: if you flip a coin 100 times, and get \\(p=0.3\\), then throwing away any one particular data point will have a small effect. But Mariner space probe bacteria swabs are a different story, with two huge outliers.</li> <li>consistency of the arithmetic mean using Chebyshev's identity. That's called the weak law of large numbers. Bernoulli proved a special case long ago.</li> <li>Tukey's stuff as true stuff for descriptive statistics</li> </ul>"},{"location":"appendix/#confidence-intervals","title":"Confidence intervals","text":"<ul> <li>Bowley's confidence trick: \"I am not at all sure that the 'confidence' is not a 'confidence trick'.\" \"Does it really lead us towards what we need--the chance that in the universe which we are sampling the proportion is within these certain limits?\" (1934, on Neyman)</li> </ul>"},{"location":"appendix/#tests","title":"Tests","text":"<ul> <li>Neyman-Pearson hypothesis testing, which they never did, as decision-making, and as one of the most broadly-used tools in science</li> <li>multiple hypothesis correction should always start with examining the distribution of pvalues</li> <li>Fisher's p-value examples: 10^-4 was too small, 0.3 was too large, 0.04 meant that you probalby needed to do some more experiments</li> <li>Significance vs. hypothesis testing</li> </ul>"},{"location":"appendix/#constructing-the-t-test","title":"Constructing the t test","text":"<ul> <li>Gosset's t-test because you couldn't expect to have hundreds or thousands of samples as Pearson assumed</li> </ul>"},{"location":"appendix/#cookbook","title":"Cookbook","text":"<ul> <li>Note about biostats handbook. I don't want to be as \"cookbook\"y.</li> </ul>"},{"location":"appendix/#the-normal-distribution","title":"The normal distribution","text":"<ul> <li>normal distribution as arising from the poisson, or whatever you want</li> </ul>"},{"location":"appendix/#jackknife-and-bootstrap","title":"Jackknife and bootstrap","text":"<ul> <li>So if there are more and more \\(X_i\\) with iid \\(F_X\\), then the idea is that the estimator of the \\(\\hat{\\theta}(\\vec{x})\\) should approach \\(\\theta(F_X)\\).</li> <li>Glivenko-Cantelli theorem about why you can (asymptotically) use the empirical distribution function to approximate the real one</li> </ul>"},{"location":"appendix/#permutations","title":"Permutations","text":"<ul> <li>Cohen's kappa as example for permutation?</li> <li>cross validation vs. the jackknife. CV is just a kind of resampling, like bootstrap and jackknife.</li> </ul>"},{"location":"appendix/#regression_1","title":"Regression","text":"<ul> <li>Censored data should be easy to understand if you do the MLE approach for talking about regression. Rather than allowing each point to be in particular places, you allow for it to be in many possible places.</li> <li>historical note about what \"regression means\"</li> <li>Simpson's paradox for need to have other variables</li> <li>Berkson's paradox for problems with including this that aren't causal</li> <li>Cook's distance \\(D_i = \\sum_{j=1}^n (\\hat{y}_j  - \\hat{y}_{j(i)})^2 / ps^2\\), where \\(p\\) is the number of parameters, \\(y_{j(i)}\\) is the predicted value for \\(y_j\\) when you remove point \\(i\\). \\(n\\) must be the number of data points minus one. \\(s^2\\) is the mean squared error of the regression model. Some people say \\(D_i &gt; 1\\) means a point is an outlier. Note the similarity to the jackknife, which will also do estimates with a leave-one-out kind of procedure.</li> <li>probit analysis<ul> <li>logit makes sense if you're thinking about odds ratios (well, probably risk ratios, but there are some nice things about odds ratios)</li> <li>probit analysis makes sense if you're thinking aobut the proportion that's going to hit some mark; which is just what you'd want for an LD-50 experiment</li> </ul> </li> </ul>"},{"location":"appendix/#miscellany","title":"Miscellany","text":"<ul> <li>Expectation maximization for fitting, e.g., mixed models, data clustering, etc.</li> </ul>"},{"location":"appendix/#bayesian","title":"Bayesian","text":"<ul> <li>include a bayesian introduction before any inferential?</li> <li>introduce priors with the beta binomial</li> <li>credible intervals, which align with confidence intervals in very particular cases</li> </ul>"},{"location":"appendix/#bayesian-inference","title":"Bayesian inference","text":"<ul> <li>Bayes factor as alternative to pvalue (if you have two models)</li> <li>MCMC as example for complex SIR-like models. It's really clear how to turn the crank, but it's not easy to say how the inputs should relate to the outputs. Or my funny hierarchical thing.<ul> <li>how does ABC compare for this?</li> </ul> </li> </ul>"},{"location":"appendix/#random-number-generation","title":"Random number generation","text":"<ul> <li>How RNGs work and maintain state</li> <li>Where seeds come from</li> </ul>"},{"location":"confidence_intervals/","title":"Confidence intervals","text":""},{"location":"confidence_intervals/#definition","title":"Definition","text":"<p>So far we have worked with point estimates, that is, guesses for population parameters that are just a single number. This presents a problem, because statistics and probability are all about uncertainty, and we know that the point estimate we make for, say, \\(B\\) in the German tank problem will likely never be exactly correct, even if the estimator is consistent, unbiased, and efficient.</p> <p>In the frequentist framework, the approach for expressing uncertainty is using confidence intervals. A confidence interval for a parameter \\(\\theta\\) with estimator \\(\\hat{\\theta}\\) at level \\(1-\\alpha\\) is a pair of estimators \\(\\hat{\\theta}_-\\) and \\(\\hat{\\theta}_+\\) defined such that, for any parameter value \\(\\theta\\), it holds that: \\(\\(\\mathbb{P}[\\hat{\\theta]_- \\leq \\theta \\leq \\hat{\\theta}_+} \\geq 1 - \\alpha.\\)\\) A typical value for \\(\\alpha\\) is 5%, or \\(0.05\\), which yields 95% confidence intervals. A higher \\(\\alpha\\) corresponds to a lower probability of error, and thus wider confidence intervals, for which we are more sure that the interval actually contains the true value. Thus a 99% confidence interval is wide and a 90% confidence interval is narrow.</p> <p>This equation is typically read as saying \"the probability that the value of the unknown parameter \\(\\theta\\) is between \\(\\hat{\\theta}_-\\) and \\(\\hat{\\theta}_+\\) is 95%\". The typical conceptualization is then to imagine that \\(\\theta\\) is varying, bopping around inside the confidence interval set by \\(\\hat{\\theta}_-\\) and \\(\\hat{\\theta}_+\\). In fact, however, it is the intervals that are the random variables, and the parameter is the constant fixed point around which those random variables are bopping.</p> <p>Note that, in the frequentist approach, we develop two statistics ---functions of the observed data--- such that their corresponding estimators will, with some probability, enclose the true value, for every possible true value. Strictly speaking, you must select a method for constructing confidence intervals such that, for any parameter \\(\\theta\\) I choose, the proportion of infinitely many repeated trials will produce realizations of the confidence interval that enclose \\(\\theta\\).</p>"},{"location":"confidence_intervals/#meaning-and-interpretation","title":"Meaning and interpretation","text":"<p>If this introduction has been a bit befuddling, then you are in good company. Confidence intervals are a very new concept. They were first introduced in the statistical literature in 1937 and become commonplace in scientific work only in the later 20th century. It is perhaps no surprise, given its youth, that the concept of the confidence interval is very confusing.</p> <p>The most common misconception about confidence intervals is that they represent confidence. (One might argue that \"confidence\" was a poor word to use to name this thing!) In this misconception, we can have 95% confidence that the true parameter \\(\\theta\\) lies inside the confidence interval constructed after the experimental data has been gathered. Strictly speaking, this is incorrect. In a frequentist framework, the true value is either inside the realized confidence interval, or it is not; our ignorance of the true value has nothing to do with probability. In fact, the interval that encloses the true value with some \"confidence\" is a Bayesian concept, the credible interval. \"Confidence\" is part of the Bayesian definition of probability; it is foreign to the frequentist construction.</p> <p>To a practicing scientist, this distinction might appear entirely semantic. But if anyone tells you there is a 95% probability that the true value of a parameter falls within some fairly specific range, you should be very skeptical. Would you, for example, bet twenty-to-one odds that they were correct? I would not, not because they had miscomputed their statistics, but because their experimental approach is very likely imperfect.</p>"},{"location":"confidence_intervals/#constructing-intervals","title":"Constructing intervals","text":"<p>Regardless of what confidence intervals really mean, they are useful for producing some quantification of uncertainty.</p> <p>To show how confidence intervals are constructed, consider the German tank problem again. Our unbiased point estimate is \\(\\hat{B} = \\tfrac{n+1}{n} \\max_i X_i\\). We will design a symmetric confidence interval, which means that: \\(\\(\\mathbb{P}[\\hat{B]_- \\leq B} = \\mathbb{P}[\\hat{B]_+ \\geq B} = 1 - \\frac{\\alpha}{2}\\)\\) This confidence interval is symmetric because the probability that the confidence interval will not include \\(B\\) because it is too low is equal to the probability that it will not include \\(B\\) because it is too high. For a 95% confidence interval, the probability of the confidence interval being wrong on either side is \\(2.5\\%\\), for a 5% total probability of error.</p> <p>To construct \\(\\hat{B}_-\\) and \\(\\hat{B}_+\\), we will find the range of values in which the point estimator \\(\\hat{B}\\) is to expected fall, given \\(B\\). We found that the cdf for the biased estimator \\(\\max_i X_i\\) was \\((x/B)^n\\), so the cdf for the unbiased point estimator is: \\(\\(\\mathbb{P}[\\hat{B] \\leq x} = \\frac{n+1}{n} \\left(\\frac{x}{B}\\right)^n\\)\\) First, we use this cdf to find the value of \\(x\\) such that \\(\\hat{B}\\) will fall below it with probability \\(1-\\frac{\\alpha}{2}\\): \\(\\(\\mathbb{P}[\\hat{B] \\leq x} = 1 - \\frac{\\alpha}{2} \\implies x = \\left( \\frac{\\alpha}{2} \\frac{n}{n+1}\\right)^{1/n} B\\)\\) Next, we \"invert\" this relationship: \\(\\(1 - \\frac{\\alpha}{2}     = \\mathbb{P}[\\hat{B] \\leq \\left( \\frac{\\alpha}{2} \\frac{n}{n+1}\\right)^{1/n} B}     = \\mathbb{P}[\\left( \\frac{\\alpha]{2} \\frac{n}{n+1}\\right)^{-1/n} \\hat{B} \\leq B}\\)\\) And thus, we have found our lower confidence interval: \\(\\(\\hat{B}_- = \\left( \\frac{\\alpha}{2} \\frac{n}{n+1}\\right)^{-1/n} \\hat{B}\\)\\) A similar exercise shows that: \\(\\(\\hat{B}_+ = \\left[ \\left(1- \\frac{\\alpha}{2}\\right) \\frac{n}{n+1}\\right]^{-1/n} \\hat{B}\\)\\)</p> <p>As an example, for \\(n=5\\), \\(\\max x_i = 1\\), and \\(\\alpha = 5\\%\\), we have \\(\\hat{B} = 1.2\\), \\(\\hat{B}_- = 1.04\\) and \\(\\hat{B}_+ = 2.17\\). For \\(n=100\\) and the same observed maximum \\(1\\), we have \\(\\hat{B} = 1.01\\), \\(\\hat{B}_- = 1.0004\\), and \\(\\hat{B}_+ = 1.04\\). In both these cases, we do not say what \\(B\\) actually is, so we cannot say whether or not the confidence intervals contain the true value or not. We only know that, regardless of what \\(B\\) is, there is a 95% probability that the resulting data will produce, according to our definitions, values of \\(\\hat{B}_-\\) and \\(\\hat{B}_+\\) that contain \\(B\\).</p>"},{"location":"confidence_intervals/#properties-of-confidence-intervals","title":"Properties of confidence intervals","text":"<p>We could have been much lazier with our confidence intervals. For example, I could have defined \\(\\hat{B}_- = 0\\). Because we know a priori that \\(B&gt;0\\), this lower end of the confidence interval will always be correct. In fact, it means that the confidence intervals will be contain \\(B\\) with a probability greater than \\(1-\\alpha\\). This is an undesirable feature. Confidence intervals with the desirable property that they have a error probability of exactly \\(\\alpha\\) and no more are termed valid confidence intervals.</p> <p>You might imagine that we could have constructed different confidence intervals. For example, if we relaxed the symmetry requirement, then we might be able to produce confidence intervals that are overall narrower. For certain situations, there is a well-acknowledged single best method for constructing a confidence interval. In other situations, there are multiple very reasonable ways to construct confidence intervals. For example, for the binomial distribution, there are pros and cons to the Wilson method and the Clopper-Pearson methods for computing confidence intervals.</p>"},{"location":"confidence_intervals/#resampling-methods","title":"Resampling methods","text":"<p>In the previous example, we used our knowledge about the distribution of the data to develop confidence intervals on the estimator \\(\\hat{B}\\). What would we do if we had an estimator ---that is, we knew what property of the data we wanted to measure--- but we did not know how the data were distributed? In these situations, we turn to resampling methods like the bootstrap or the jackknife.</p>"},{"location":"confidence_intervals/#bootstrap","title":"Bootstrap","text":"<p>In the bootstrap approach, we do no assert that we know the statistical distribution that the data were drawn from. Instead, we assume that the data we collected is the best approximation we have for the true, underlying distribution of the data. In statistics jargon, this means we use the sampling distribution as if it were the population distribution. In other words, we sampled some number \\(n\\) of data points \\(x_i\\) from the true distribution \\(F_X\\) of some i.i.d.\u00a0random variables \\(X\\), and from those observed data points we construct a new set of i.d.d.\u00a0random variables \\(\\hat{X}\\), which are discrete random variables with probability mass function: \\(\\(f_{\\hat{X}}(x') = \\frac{\\#\\{x' \\in x_i\\}}{n},\\)\\) The probability of drawing the value \\(x'\\) is equal to the proportion of the data points \\(x_i\\) equal to that value. If none of the \\(x_i\\) are the same (i.e., every value is unique), then: \\(\\(f_{\\hat{X}}(x') = \\begin{cases}         \\tfrac{1}{n} &amp;\\text{ if } x' \\in x_i \\\\         0 &amp;\\text{ otherwise}     \\end{cases}\\)\\)</p> <p>Strictly speaking, there are a finite number of possible draws from this population, and so the probabilities \\(f_{\\hat{X}}(x')\\) can be computed exactly. For example, say you ran an experiment and drew three values \\(x_1, x_2, x_3\\). Then \\(f_{\\hat{X}}(x')\\) is \\(\\tfrac{1}{3}\\) for \\(x' \\in \\{x_1,x_2,x_3\\}\\) and zero otherwise. There are only 10 possible data sets that can be drawn: \\(\\{x_1,x_1,x_1\\}\\) with probability \\(\\tfrac{1}{27}\\), \\(\\{x_1,x_1,x_3\\}\\) with probability \\(\\tfrac{1}{9}\\), and so on, finishing with \\(\\{x_3,x_3,x_3\\}\\) with probability \\(\\tfrac{1}{27}\\).</p> <p>Another way to say this is that bootstrapping represents resampling with replacement. From the \\(n\\) data points in our sample, we take a \"resample\" of \\(n\\) data points, allowing replacement. One of the possible resamples is the original sample. In the example above, this is the resample \\(\\{x_1,x_2,x_3\\}\\). In all the other cases, we draw at least one data point multiple times. For example, in the resample \\(\\{x_1,x_1,x_2\\}\\), we drew \\(x_1\\) twice.</p> <p>Some combinatorics shows that there are \\(\\binom{2n-1}{n}\\) possible resamples. For \\(n=3\\), this yields 10 possible resamples. For \\(n=10\\), this yields \\(92,378\\) possible resamples. For \\(n=25\\), this yields more than \\(10^{13}\\) possible resamples. Thus, for all but the smallest data sets, it is not feasible to consider all possible resamples. A scientific study might perform \\(1,000\\) or 1 million bootstraps, randomly resampling the original data \\(1,000\\) or 1 million times, and then stop. Very few studies exhaustively considering all the possible combinations. This means there is some stochasticity in practical applications of the bootstrap. In practice, the number of bootstraps is chosen to be quite large in order to minimize any of these stochastic effects.</p> <p>Bootstraps can be used to construct confidence intervals on some estimator. There are a few different mathematical approaches, the simplest of which is called the percentile method. Given a statistic \\(t\\) and data points \\(x_i\\), draw many bootstraps \\(x^{(j)}\\), and compute \\(t(x^{(j)})\\) for each bootstrap. The lower confidence interval \\(\\hat{T}_-\\) is the \\(\\tfrac{\\alpha}{2}\\) quantile of the bootstrapped statistics \\(t(x^{(j)})\\), and the upper confidence interval \\(\\hat{T}_+\\) is the \\(1-\\tfrac{\\alpha}{2}\\) quantile. In other words, the confidence intervals for the estimator are equal to the quantiles of the statistic computed on many bootstraps of the data.</p>"},{"location":"confidence_intervals/#jackknife","title":"Jackknife","text":"<p>In the bootstrap, we draw \\(n\\) data points with replacement, yielding possible \\(\\binom{2n-1}{n}\\) resamples, and we select a subset at random to analyze. The jackknife, by contrast, involves taking just \\(n\\) resamples, each of size \\(n-1\\). In each resample, all the original data points are used, except one, which is left out. The jackknife is therefore sometimes called \"delete-1\" resampling. The advantage of the jackknife is that evaluating \\(n\\) resamples is computationally feasible, so it ceases to be a Monte Carlo method.</p> <p>One might be tempted to proceed with the jackknife in the same way we did the bootstrap: for a statistic \\(t\\), enumerate the \\(n\\) resamples \\(x^{(j)}\\), compute the jackknifed statistics \\(t(x^{(j)})\\), and then look at the distribution of those jackknife statistics to compute the confidence interval. This approach would be mistaken because the resamples are all quite similar to one another, having \\(n-2\\) of \\(n-1\\) data points in common. Thus, the distribution of the jackknifed statistics \\(t(x^{(j)})\\) is much narrower than the distribution of the bootstrapped statistics would be.</p> <p>The process of correctly accounting for the narrowness of these values is:</p> <ol> <li> <p>Compute the jackknifed replicates \\(t^{(j)} \\equiv t(x^{(j)})\\)</p> </li> <li> <p>Compute the jackknifed statistic, which is the mean of the replicates: \\(t_\\mathrm{jack} = \\tfrac{1}{n} \\sum_j t^{(j)}\\)</p> </li> <li> <p>Compute the jackknife estimate of the variance of \\(T\\) as \\(\\frac{n-1}{n} \\sum_j \\left( t^{(j)} - t_\\mathrm{jack} \\right)^2\\)</p> </li> <li> <p>Get the confidence intervals from the Student's \\(t\\)-distribution with \\(n-1\\) degrees of freedom and multiply them by the jackknife estimate of the variance</p> </li> </ol> <p>Those familiar with Taylor series may appreciate that the jackknife is the first-order approximation for the bootstrap in the space of the observed data \\(x_i\\)^1. Starting from the observed data, the jackknife moves a little bit in every \"direction\" by deleting a single data point and seeing what effect that has on the computed statistic. The bootstrap, on the other hand, moves about in every direction at once, wildly exploring the large but ultimately finite space of possible resamples.</p> <p>The jackknife has some notable flaws. For example, the jackknife performs badly when estimating used to estimate confidence intervals for the sample median. In these cases, one needs to resort to the bootstrap or to other resampling methods, such as delete-\\(d\\) resampling, which yields \\(\\binom{n}{d}\\) possible resamples.</p>"},{"location":"confidence_intervals/#profile-method","title":"Profile method","text":"<p>Digest this: <code>https://personal.psu.edu/abs12/stat504/Lecture/lec3_4up.pdf</code></p>"},{"location":"estimators/","title":"Estimators","text":""},{"location":"estimators/#parameters-populations-and-samples","title":"Parameters, populations, and samples","text":"<p>Some people refer to statistics (the functions) as sample statistics to emphasize that they are functions of a sample of data that was drawn from some larger population, which has some fixed an unknowable parameters that describe it. For example, if you draw many data points \\(x_i\\) from a distribution and compute the mean of the drawn data points, you do not expect that the sample mean that you compute will be exactly equal to the true population mean.</p> <p>In mathematical terms, we say that a random variable \\(X\\) has some expected value \\(\\mathbb{E}[X]\\) that is fixed. A random variable is a function, and the expected value is another function that links the random variable with a single number. There is nothing \"random\" about this so far. The analogy to the random sample are iid random variables \\(X_i\\), so the sample mean is analogous to the estimator \\(t(X)\\).</p> <p>Ideally, the estimator \\(t(X)\\) will actually reflect the parameter \\(\\mathbb{E}[X]\\) that we wanted to learn about. The rest of this section will discuss what makes a \"good\" estimator.</p>"},{"location":"estimators/#a-useful-example-the-german-tank-problem","title":"A useful example: the \"German tank\" problem","text":"<p>To examine the properties of estimators and what makes an estimator a \"good\" one, I will use an example that is mathematically tractable but just unfamiliar enough to make you think.</p> <p>During World War II, it was important to the Allies to know how many tanks Germany was producing. The traditional approach was to use spies and aerial reconnaissance. The new approach was to use statistics on serial numbers, which turned out to be much more accurate.</p> <p>A serial number is a unique number written on a manufactured part. Serial numbers are usually assigned in sequential order, so that older parts have lower serial numbers and newer parts have higher numbers. German tanks had serial numbers. When the Allies captured German tanks, they took note of those numbers, which gave them a clue about how may thanks there were. For example, if you captured three tanks and found serial numbers 1, 3, and 5, you know there are at least 5 tanks total, and there probably aren't more than 10 or so. If you find serial numbers 100, 300, and 500, then you know there are at least 500 tanks, and there are probably more like 1,000.</p> <p>The Allies had a fairly complex problem, because they wanted to estimate the rate of production, and they had many serial numbers on many different parts of the tank, some of which were not exactly sequential. Let's instead consider an abstracted, simplified version of this problem. Say you drew \\(n\\) numbers from uniform distribution ranging from \\(A\\) to some unknown upper limit \\(B\\). You want to estimate \\(B\\), which is analogous to the number of German tanks out there.</p> <p>Mathematically, we are interested in iid random variables \\(X\\) that follow the uniform distribution:</p> \\[ \\begin{gathered} f_X(x) = \\frac{1}{B-A} \\text{ for } A \\leq x \\leq B \\\\ F_X(x) = \\frac{x-A}{B-A} \\text{ for } A \\leq x \\leq B \\end{gathered} \\] <p>For simplicity, let's say that \\(A=0\\) so that \\(f_X(x) = 1/B\\) and \\(f_X(x) = x/B\\) for \\(0 \\leq x \\leq B\\).</p> <p>Our challenge is to create a statistic, a function of the data, that estimates the parameter \\(B\\) and then examine the mathematical properties of the corresponding estimator. It's typical to denote estimators with a \"hat\", so we will write our estimators like \\(\\hat{B}\\). It's also typical to denote statistics with lower-case letters reminiscent of the true value, so we'll use names like \\(b\\). To be clear, \\(b\\) is a function of numbers, although we also write \\(\\hat{B} = b(X)\\), where now \"\\(b\\)\" means some manipulation of a set of iid random variables to create a new random variable.</p>"},{"location":"estimators/#consistent-estimators","title":"Consistent estimators","text":"<p>Let me start with what will seem like a very inane choice of statistic. Maybe my favorite number is 3, so I will say that, regardless of what data I collect, I will just always guess that \\(B\\) is 3:</p> \\[ b(x_1, \\ldots, x_n) \\equiv 3. \\] <p>The corresponding estimator \\(\\hat{B}\\) is very simple. It takes on the value 3 with 100% probability:</p> \\[ f_{\\hat{B}}(x) = \\begin{cases} 1 &amp;\\text{if $x = 3$} \\\\ 0 &amp;\\text{otherwise} \\end{cases} \\] <p>This is clearly a bad estimator. No matter how much data I accumulate, my estimate doesn't improve. I will only be \"right\" if \\(B\\), by some miracle, happens to be exactly \\(3\\). By contrast, my expectation would be that, in the limit of collecting a lot of data, the statistic \\(b\\) would be almost guaranteed to be very close to \\(B\\). This requirement is called consistency.</p> <p>Mathematically, I want the probability that \\(\\hat{B}\\) takes on a value far from \\(B\\) to get close to zero as I collect more and more data. This requires defining what a \"limit\" is for a series of random variables. For a sequence of numbers, a limit means that, for any a priori, fixed threshold \\(\\varepsilon &gt; 0\\), there is some integer \\(n\\) such that every value in the sequence after the \\(n\\)-th one is within \\(\\varepsilon\\) of the true value:</p> \\[ \\lim_{n\\to\\infty} a_n = L \\implies \\text{for any $\\varepsilon &gt; 0$ there exists some $n$ such that } |a_i - L| &lt; \\varepsilon \\text{ for all $i \\geq n$}. \\] <p>For example, given the geometric series \\(1, \\tfrac{1}{2}, \\tfrac{1}{4}, \\ldots\\), if you pick the threshold \\(\\varepsilon = \\tfrac{1}{100}\\), I can pick \\(n=7\\), since the terms in the series are equal to \\(\\tfrac{1}{2^n}\\), and \\(\\tfrac{1}{2^7} = \\tfrac{1}{128}\\). All subsequent terms in the series will be even closer to zero than that term.</p> <p>In probability, if you give me \\(\\varepsilon\\), in most cases I cannot pick an \\(n\\) such that the estimator maps all values outside that threshold to zero probability. Casually speaking, even after a trillion data points, there is no guarantee that a series of random data points won't stray from their limit. Instead, in probability, we say that a sequence of random variables \\(X_n\\) converges toward a number \\(a\\) if</p> \\[ \\lim_{n \\to \\infty} \\mathbb{P}[|X_n - a| &gt; \\varepsilon] = 0, \\] <p>that is, if the probability that \\(X_n\\) takes on a value more than \\(\\varepsilon\\) away from \\(a\\) approaches zero as \\(n\\) increases. An estimator is consistent if the series of random variables corresponding to collecting more and more data converge toward the true value.</p> <p>In our example, \\(\\hat{B}_1\\) corresponds to the estimator when we only collect 1 data point, \\(\\hat{B}_2\\) when we collect 2, and so on. In the dumb example where \\(\\hat{B}\\) only takes on the value 3, the \\(\\hat{B}_n\\) converge to 3, but not to \\(B\\), except in the unlikely case that \\(B\\) just happens to be 3.</p> <p>For our second guess, let's use a more reasonable statistic. Say that \\(b\\) is the maximum of whatever data points we collected:</p> \\[ b(x_1, \\ldots, x_n) = \\max_i x_i. \\] <p>I know I can write out an analogous equation to define the estimator:</p> \\[ \\hat{B} = \\max_i X_i. \\] <p>But what does it mean to take the maximum of two random variables? Recall that, we computed the distribution of a sum of two random variables \\(Z = X + Y\\) as \\(f_Z(z) = \\int f_X(x) f_Y(z-x) \\,dx\\). For a maximum, it is easier to express the new variable's distribution using its cdf. To say \\(Z = \\max[X, Y]\\) means that, if \\(X\\) and \\(Y\\) are independent,</p> \\[ \\begin{aligned} F_Z(z) &amp;= \\mathbb{P}[Z \\leq z] \\\\ &amp;= \\mathbb{P}[(X \\leq x) \\cap (Y \\leq y)] \\\\ &amp;= \\mathbb{P}[X \\leq x] \\, \\mathbb{P}[Y \\leq y] \\\\ &amp;= F_X(x) F_Y(y) \\end{aligned} \\] <p>So if \\(F_X(x) = x/B\\), then \\(F_{\\hat{B}_n} = (x/B)^n\\). Note that \\(\\hat{B}_n\\) can only take on values between \\(0\\) and \\(B\\), so if it diverges from \\(B\\), it will do so by falling short:</p> \\[ \\begin{aligned} \\mathbb{P}\\left[ |\\hat{B}_n - B| &gt; \\varepsilon \\right] &amp;= \\mathbb{P}[\\hat{B}_n \\leq B - \\varepsilon] \\\\ &amp;= F_{\\hat{B}_n}(B - \\varepsilon) \\\\ &amp;= \\left(\\frac{B - \\varepsilon}{B}\\right)^n \\\\ &amp;= \\left(1 - \\frac{\\varepsilon}{B}\\right)^n. \\end{aligned} \\] <p>For \\(\\varepsilon &gt; 0\\), this limit of this number, as \\(n \\to \\infty\\), is zero, and thus \\(\\hat{B}_n\\) converges to \\(B\\), so \\(\\hat{B}\\) is a consistent estimator.</p>"},{"location":"estimators/#unbiased-estimators","title":"Unbiased estimators","text":"<p>It is a nice thing that our estimator \\(\\hat{B}\\) is consistent: as we get more and more data, the statistic \\(b\\) will end up closer and closer to the true value \\(B\\). It is disappointing, though, that \\(b\\) is always an underestimate. If the data points \\(x_i\\) are randomly distributed between \\(0\\) and \\(B\\), and we take \\(b = \\max_i x_i\\), it is necessarily the case that \\(b &lt; B\\). Because \\(b\\) is not \"centered around\" \\(B\\), we say that \\(\\hat{B}\\) is a biased estimator.</p> <p>Consider the extreme case where \\(n=1\\): we draw only one data point \\(x\\), which is somewhere between \\(0\\) and \\(B\\). Intuitively, I might expect \\(x\\) to be about halfway between \\(0\\) and \\(B\\), that is, that \\(x \\approx \\tfrac{1}{2}B\\), so that \\(b\\) underestimates \\(B\\) by a factor of \\(\\tfrac{1}{2}\\). And if \\(n=2\\), then I might expect those points, roughly speaking, to land somewhere like \\(\\tfrac{1}{3}B\\) and \\(\\tfrac{2}{3}B\\), so that \\(b\\) underestimates \\(B\\) by a factor of about \\(\\tfrac{1}{3}\\). These factors appear to shrink as \\(n\\) increases, which makes sense, since \\(\\hat{B}\\) is a consistent estimator. But can we do something to figure out, mathematically, what that factor should be?</p> <p>We do this by examining the expected value of the estimator:</p> \\[ \\mathbb{E}[\\hat{B]} = \\int_0^B x \\, f_{\\hat{B}}(x) \\, dx. \\] <p>We said above that \\(F_{\\hat{B}} = (x/B)^n\\), from which it follows that</p> \\[ f_{\\hat{B}}(x) = \\frac{d}{dx} F_{\\hat{B}} = \\frac{n}{B^n} x^{n-1}. \\] <p>Plugging this definition of \\(f_{\\hat{B}}\\) into the integral gives</p> \\[ \\mathbb{E}[\\hat{B]} = \\int_0^B \\frac{n}{B^n} x^n \\, dx = \\frac{n}{n+1} B. \\] <p>This result accords exactly with our intuition above: for \\(n=1\\), the expected value of \\(\\hat{B}\\) is \\(\\tfrac{1}{2}B\\). For \\(n=2\\), it is \\(\\tfrac{2}{3}B\\), and so forth. Based on this finding, we can define a new estimator:</p> \\[ \\hat{B}_\\mathrm{unbiased} \\equiv \\frac{n+1}{n} \\max_i X_i. \\] <p>This ensures that our estimates for \\(B\\) are \"centered\" around \\(B\\), because \\(\\mathbb{E}[\\hat{B}_\\mathrm{unbiased}] = B\\). More generally, we say that an estimator is unbiased if its expected value is equal to the parameter it is estimating.</p> <p>Note the subtlety in what we did: without actually knowing what \\(B\\) is, we found a way to adjust the way that we estimate \\(B\\) to ensure that we come up with a more accurate estimate. I was mystified in introductory statistics that, while we computed variance as \\(\\tfrac{1}{n} \\sum_i (x_i - \\mu)^2\\), we computed sample variance as \\(\\tfrac{1}{n-1} \\sum_i (x_i - \\mu)^2\\). The standard explanation has something to do with \"degrees of freedom\", which I never understood. There is, I think, a more straightforward explanation, which you can now understand, which is that the \\(n-1\\) is there in the sample variance, instead of \\(n\\), because it corrects for a bias.</p>"},{"location":"estimators/#efficient-estimators","title":"Efficient estimators","text":"<p>Consistency ensures that, in the limit of infinite data, our estimate approaches the true value. How can we be sure that we are getting the best estimate for our data, given the fact that we can't collect infinite data? This question relates to the concept of efficiency. One unbiased estimator \\(\\hat{X}_1\\) is more efficient than another unbiased estimator \\(\\hat{X}_2\\) if it has lower variance:</p> \\[ \\mathbb{V}[\\hat{X}_1] &lt; \\mathbb{V}[\\hat{X}_2]. \\] <p>As we will see, confidence intervals are related to the variance of an estimator, so a more efficient estimator means that we can get more narrow confidence intervals for the same amount of data, which is clearly a desirable thing. I don't want to appear more ignorant than I have to be, just because I picked a poor estimator!</p> <p>It turns out that there is a theoretical lower limit to the variance of estimators called the Cram\u00e9r-Rao bound, and many estimators actually hit this limit. Therefore, rather than saying that one estimator is more efficient than another, we usually just say that an estimator is \"efficient\", meaning that it hits the lower bound on variance and is maximally efficient.</p> <p>The math behind efficiency is somewhat complex, so I will simply tell you that a whole class of estimators, maximum likelihood estimators, are efficient.</p>"},{"location":"estimators/#maximum-likelihood-estimators","title":"Maximum likelihood estimators","text":"<p>The German tank problem has a useful toy example, but it's hard to imagine deriving estimators by hand for the kind of complex data analysis that most scientists do. Fortunately, modern computing means that we can address all sorts of complex data using a maximum likelihood (ML) approach. In a maximum likelihood approach, you specify a theoretical model for how your data are generated, and then you use optimization to estimate values for the parameters that best \"fit\" the data.</p> <p>The word likelihood is confusing because, in common speech, \"probability\" and \"likelihood\" are synonymous. In statistics jargon, the likelihood function is just the probability density function, except that it treats the parameters as the varying input and the data as fixed. Say we have some data \\(x\\) that was drawn from a probability distribution with density function \\(f_X\\). We often think about families of probability distributions and name them by their parameters. For example, we say that a normal distribution is defined by two parameters, its mean and its variance. So say that the density function \\(f_X\\) changes depending on some parameter (or parameters) \\(\\theta\\). We write the density function as \\(f_X(x; \\theta)\\) to emphasize that, while the parameters \\(\\theta\\) are relevant, we think of them as fixed. By contrast, we write the likelihood function \\(L\\) as \\(L(\\theta; x) \\equiv f_X(x; \\theta)\\). For likelihood, we consider the data fixed and vary the parameters.</p> <p>To see how this plays out, let's return to the German tank problem. Given some data points \\(x_i\\), what is the probability density of drawing that data, for some varying parameter \\(B\\)? We look for the value of \\(B\\) that maximizes the probability density function for all the measured data points \\(x_i\\). This kind of optimization is called argmax. While max asks for the maximum value of some function \\(f(x)\\) when varying \\(x\\), argmax asks for the value of \\(x\\) that leads to \\(f(x)\\) being maximized. Thus:</p> \\[ \\begin{aligned}     \\hat{B}_\\mathrm{ML}     &amp;= \\argmax_B \\prod_i f_X(x_i; B) \\\\     &amp;= \\argmax_B \\prod_i \\begin{cases}       1/B &amp; 0 \\leq x_i \\leq B \\\\       0 &amp; x_i &gt; B     \\end{cases} \\\\     &amp;= \\argmax_B \\begin{cases}       (1/B)^n &amp; x_i &gt; B \\text{ for any $i$} \\\\       0 &amp; \\text{otherwise}      \\end{cases} \\\\     &amp;= \\max_i x_i \\end{aligned} \\] <p>Note that, in the maximum likelihood approach, when computing \\(f_X(x_i; B)\\), we assume that we know \\(B\\), because it is the parameter we are optimizing over. This makes the maximum likelihood computation relatively simple: we cannot choose \\(\\hat{B}_\\mathrm{ML}\\) to be smaller than the maximum of the observed values \\(x_i\\), because that would make the observed data impossible, and we set \\(\\hat{B}_\\mathrm{ML}\\) at exactly \\(\\max_i x_i\\) because choosing any greater value would make the observed data seem unusual. If \\(B\\) were actually much, much higher than \\(\\max_i x_i\\), the observed data would be rare, being clustered very close to zero, relative to that very large \\(B\\).</p> <p>Note also that the maximum likelihood approach for the German tank problem gave us a consistent estimator, but not an unbiased one. Given certain mathematical necessities that are probably true for your applications, maximum likelihood estimators are consistent, but they are in general not biased.</p> <p>Again, a key practical advantage of maximum likelihood estimation is that it requires only a computer and an articulation of how the data are generated.</p>"},{"location":"estimators/#the-arithmetic-mean-as-an-estimator","title":"The arithmetic mean as an estimator","text":"<p>The German tank problem has simple math, but it is not a familiar application. Let us consider instead a very familiar estimator, the arithmetic mean, that is, the simple average of numbers. Just as the mean of some set of numbers \\(x_i\\) is \\((1/n) \\sum_i x_i\\), we analogously define the estimator \\(\\hat{\\mathbb{E}}_X \\equiv (1/n) \\sum_i X_i\\) of the mean of i.i.d. random variables \\(X_i\\).</p> <p>It is immediately clear that \\(\\hat{\\mathbb{E}}_X\\) is an unbiased estimator of \\(\\mathbb{E}[X]\\):</p> \\[ \\mathbb{E}[\\hat{\\mathbb{E}}_X] = \\mathbb{E}\\left[\\frac{1}{n} \\sum_i X_i \\right] = \\frac{1}{n} \\mathbb{E}[X_i] = \\mathbb{E}[X]. \\] <p>But is \\(\\hat{\\mathbb{E}}_X\\) as consistent estimator of \\(\\mathbb{E}[X]\\)? The mathematician Jacob Bernoulli published the first proof of this fact for a special case of random variables (i.e., those following the Bernoulli distribution) in 1713. It took him 20 years to develop a rigorous proof for what he proudly called the \"Golden Theorem\". Now the proof is considerably simpler and broader, and we call it the law of large numbers.</p> <p>The modern proof of the law of large numbers follows from an observation about the variance of the estimator \\(\\hat{\\mathbb{E}}_X\\) that was surprising to many early statisticians:</p> \\[ \\mathbb{V}\\left[\\hat{\\mathbb{E}}_X \\right] = \\mathbb{V}\\left[\\frac{1}{n} \\sum_i X_i \\right] = \\frac{1}{n^2} \\sum_i \\mathbb{V}[X_i] = \\frac{1}{n} \\mathbb{V}[X] \\] <p>In words, the variance of the estimator is \\(1/n\\) of the variance of the underlying random variable it is estimating. Intuitively, this means that, as you take more data, you are ever more likely to get an estimate near to the true mean of the underlying population you are sampling. This result makes it clear that \\(\\hat{\\mathbb{E}}_X\\) is a consistent estimator, because for arbitrarily high \\(n\\), it has arbitrarily low variance.</p> <p>Because the standard deviation, the square root of the variance, is more intuitive, one can say that the standard deviation of the estimator decreases with the square root of \\(n\\). To distinguish the standard deviation of the estimator from the standard deviation of the underlying population, \\(\\mathbb{V}[\\hat{\\mathbb{E}}_X]\\) is also called the standard error of the mean. With more data points \\(n\\), you can get an ever more precise estimate of the mean, so the standard error of the mean declines, but the standard error of the underlying population \\(\\mathbb{V}[X]\\) is fixed.</p> <p>This result was surprising because it was expected that the standard error would scale like \\(1/n\\) rather than \\(1/\\sqrt{n}\\). In other words, it was expected that, if you collected double the data, you would get double the precision. In fact, when you double the data, you get only an improvement of \\(\\sqrt{2}\\), of 41%, in standard error. More starkly, if you take one hundred times the data, you get only a ten-fold increase in precision.</p>"},{"location":"functions/","title":"Functions","text":"<p>It may seem strange to start a book about statistics with something as abstract as functions, but many of the important objects in statistics are actually functions. If you are not entirely comfortable with the formalism of mathematical functions, fundamental concepts in statistics may become very confusing.</p> <p>As an example, in statistics, you often see notation like \\(X=x\\). You might be tempted to think that \\(X\\) is a variable representing a number, and \\(x\\) is another variable representing a number, and the equal sign means that the two variables represent the same number. However, in probability and statistics, the notation \\(X=x\\) refers to something entirely different: \\(X\\) represents a kind of function called a random variable, \\(x\\) is a number, and the whole statement is not an assertion that the two sides of the equation are equal but instead a reference to the set of events that the function \\(X\\) maps to the value \\(x\\). If this sounds incredibly bewildering, I agree, and helping unpack that is the purpose of the next few chapters.</p>"},{"location":"functions/#functions-map-things-to-other-things","title":"Functions map things to other things","text":"<p>In mathematical terms, a function is relationship between sets, linking each element of one set to an element of another set. For example, say I have a function \\(f\\) that links each number to that number plus one: \\(f(1) = 2\\), \\(f(2) = 3\\), and so on. Take careful note: the equation \\(f(1) = 2\\), when translated into words, means that the function \\(f\\) maps the input \\(1\\) to the output \\(2\\). It does not mean that the function equals \\(2\\). A function is a relationship between sets, not a number.</p> <p>In computer programming, a \"function\" refers to a sort of computational factory that takes some input and returns some output. This kind of \"function\" is not necessarily a mapping like a mathematical function is. For example, a random number generator \"function\" is designed to return a different, unpredictable number every time is it called. Some programming languages define pure functions, which are analogous to mathematical functions in that they are guaranteed to deliver the same output whenever given the same input, but this is unusual. Do not confuse mathematical functions with the conceptually distinct computer programming concept that happens to have the same name.</p>"},{"location":"functions/#functions-can-map-different-kinds-of-things-including-functions","title":"Functions can map different kinds of things, including functions","text":"<p>The function \\(f\\) above is relatively simple: it takes a single real number as input and returns another real number. We characterize the function \\(f\\) like this: \\(f : \\mathbb{R} \\to \\mathbb{R},\\) where \\(\\mathbb{R}\\) represents the set of real numbers. A slightly more complex function adds two numbers: \\(\\mathrm{add}(1, 2) = 3\\) Technically, this function still takes a single thing as input, only now that \"thing\" is an ordered pair of numbers. Ordered pairs of real numbers are also called the Cartesian product of the set of real numbers with itself, that is, \\(\\mathbb{R} \\times \\mathbb{R}\\), or just \\(\\mathbb{R}^2\\). We characterize this function like: \\(\\mathrm{add} : \\mathbb{R}^2 \\to \\mathbb{R}\\).</p> <p>Functions can map from things that aren't numbers. In linear algebra, every square matrix has a determinant, which is a real number. Thus, \"determinant\" is a function: \\(\\mathrm{det} : \\{\\text{square matrices}\\} \\to \\mathbb{R}\\). Functions can also map to things that aren't numbers. Again in linear algebra, one matrix has multiple eigenvectors. Thus, \"eigenvectors\" is a function: \\(\\mathrm{eigenvectors} : \\{\\text{matrices}\\} \\to \\{\\text{multiple vectors}\\}\\).</p> <p>Importantly, functions can map from functions and to functions. For example, the minimum of the function \\(f(x) = x^2\\) is zero. We characterize the \"minimum\" function like: \\(\\mathrm{min} : (\\mathbb{R} \\to \\mathbb{R}) \\to \\mathbb{R},\\) where \\((\\mathbb{R} \\to \\mathbb{R})\\) means the set of functions that map from real number to real numbers. Or, note that the mirror of some function \\(f(x)\\) around \\(x=0\\) is a new function \\(g(x) = f(-x)\\). Thus, \"mirror around \\(x=0\\)\" is a function: \\(\\mathrm{mirror} : (\\mathbb{R} \\to \\mathbb{R}) \\to (\\mathbb{R} \\to \\mathbb{R})\\).</p>"},{"location":"functions/#functions-are-written-in-different-ways","title":"Functions are written in different ways","text":"<p>We are used to writing functions using the prefix (or \"Eulerian\") style, like \\(f(1) = 2\\). In a few cases with well-known functions, we drop the parentheses, such as in \\(\\sin \\pi = 1\\).</p> <p>However, there are other styles of writing functions. In infix notation, the function (or \"operator\") goes between the two inputs, such as in \\(1 + 2\\), rather than \\(+(1, 2)\\). We use around-fix notation for the absolute value, like in \\(|x|\\), and we use postfix notation for the factorial function, such as \\(4!\\). And there are plenty more. The binomial \"choose\" function \\(\\binom{n}{k}\\) maps a pair of integers to an integer. Summation, such as \\(\\sum_{x=0}^n f(x)\\), can be thought of as mapping a function and two integers (the lower and upper limits) to a real number (i.e., the value of the sum).</p> <p>The important thing is to understand what each symbol on the page is. If it not a variable, then it is likely a function, or a part of the notation needed to encode a function. If it is a function, what is it mapping from and to?</p>"},{"location":"functions/#different-functions-are-written-in-the-same-way","title":"Different functions are written in the same way","text":"<p>In computer science, polymorphism refers to the fact that the same \"function\" (in the computer science sense) will, when handed inputs of different types, perform different computations. For example, in many programming languages, <code>+</code> is used to signal numeric addition as well as string concatenation: <code>1 + 2</code> returns <code>3</code>, but <code>\"Cui \" + \"bono?\"</code> gives <code>\"Cui bono?\"</code>. Somehow the plus \"function\" knew to add in one case and to concatenate in the other.</p> <p>In mathematics, polymorphism is mostly called \"different notation\". For example, the plus symbol (\\(+\\)) can be used to add two real numbers, but it can also be used to \"add\" two matrices. Adding two numbers and adding two matrices are analogous operations, but they are not identical. In statistics, we will encounter this situation often, where a series of symbols on the page could make perfect sense as an operation on numbers, but when in fact it means something very different.</p> <p>The most important example of this confusion comes from the meaning of the equal sign (\\(=\\)). If I write \\(x=2\\), it is most likely a definition: I'm asserting that the value of \\(x\\) is \\(2\\). But if I write \\(1=2\\), and ask you, \"Is this true?\", you will say no. Many programming languages distinguish between the assignment operator <code>=</code> and the equality test operator <code>==</code>, so that <code>x=1</code> is a command \"set \\(x\\) to \\(1\\)!\", while <code>x==2</code> is a question \"is \\(x\\) equal to 2?\", to which the program will response <code>TRUE</code> or <code>FALSE</code>.</p> <p>I'll use \\(\\equiv\\) to mean \"equals as a matter of definition\" and the normal \\(=\\) for \"equals as a matter of assertion.\"</p>"},{"location":"generating_functions/","title":"Generating functions","text":""},{"location":"generating_functions/#probability-generating-functions","title":"Probability generating functions","text":"<p>[TBD]</p>"},{"location":"generating_functions/#characteristic-functions","title":"Characteristic functions","text":""},{"location":"generating_functions/#application-size-bias","title":"Application: Size bias","text":"<p>We often talk about sampling from a distribution, thinking about i.i.d. variables \\(X\\) and their distribution. Sometimes, we don't directly sample from the distribution, but from a size-biased version of that distribution, in which the probability of drawing a value is proportional to the value itself.</p> <p>If a random variable \\(X \\geq 0\\) is distributed with pdf \\(f_X\\), then the pdf of the associated size-biased random variable \\(X^\\mathrm{s}\\) is:</p> <pre><code>f_{X^\\mathrm{s}}(x) = \\frac{x \\, f_X(x)}{\\mathbb{E}[X]}\n</code></pre> <p>The value of \\(\\mathbb{E}[X]\\) in the denominator is the normalization constant.</p> <p>As an abstract example, imagine there are many urns, each with an associated random variable \\(X \\geq 0\\). Each urn has a number of balls that depends on \\(X\\), such that the expected number of balls in the urn is \\(\\mathbb{E}[X]\\). Each ball has the same value as its urn. Then the distribution of values associated with a randomly selected ball is \\(X^\\mathrm{s}\\). The urns with larger values \\(X\\) will have more balls, and so we are more likely to draw a ball from one of those urns, so the values \\(X^\\mathrm{s}\\) will be biased toward higher values.</p> <p>More concretely, imagine that, in the context of an infectious disease, infected people are infectious for some period of time \\(X \\sim \\mathrm{Exp}(\\lambda)\\), during which time they have a constant infectiousness, so that the number of people they infect is proportional to \\(X\\), and the times of those secondary infections are uniformly distributed throughout the period \\(X\\). What is the average delay between the infector's being infected and an infectee's being infected?</p> <p>First, we consider the size-biased variable \\(X^\\mathrm{s}\\), which represents selecting an infectee at random and asking about their infector's infectious period. The expected value of this random variable is:</p> <pre><code>\\begin{align*}\n\\mathbb{E}[X^\\mathrm{s}] &amp;= \\int_0^\\infty x f_{X^\\mathrm{s}}(x) \\,dx \\\\\n&amp;= \\int_0^\\infty x \\frac{x f_X(x)}{\\mathbb{E}[X]} \\,dx \\\\\n&amp;= \\int_0^\\infty x \\frac{x \\lambda e^{-\\lambda x}}{1/\\lambda} \\,dx \\\\\n&amp;= \\int_0^\\infty (\\lambda x)^2 e^{-\\lambda x} \\,dx\n\\end{align*}\n</code></pre> <p>Make a chance of variables \\(y=\\lambda x\\) so that:</p> <pre><code>\\mathbb{E}[X^\\mathrm{s}] = \\frac{1}{\\lambda} \\int_0^\\infty y^2 e^{-y} \\,dy = \\frac{2}{\\lambda} = 2 \\mathbb{E}[X]\n</code></pre> <p>(This integral relied on multiple applications of integration by parts, which worked in this specific example, for the exponential distribution.)</p> <p>Because each infectee lands, on average, in the middle of the infector's infectious period, then the average time to infection is \\(\\mathbb{E}[X]\\), that is, equal to the average duration of infectiousness.</p>"},{"location":"generating_functions/#size-bias-with-cfs","title":"Size bias with CFs","text":"<p>The math worked out here for the exponential distribution, but in general integrals are hard and this sort of math won't work out for other distributions.</p> <p>Rather than working with pdf's, we can instead work with their transforms. Just as logarithms make difficult multiplication into easy addition, so these transformations can make things difficult things like adding random variables into simpler multiplication.</p> <p>Define the characteristic function of a random variable \\(X\\) as:</p> <pre><code>\\phi_X(u) = \\mathbb{E}[e^{iuX}]\n</code></pre> <p>This form may look strange, but it allows us to extract moments of \\(X\\), notably the expected value, from derivatives. Note that:</p> <pre><code>\\begin{align*}\n\\phi_X(u) &amp;= \\mathbb{E}[e^{iuX}] = \\int e^{iux} f_X(x) \\,dx \\\\\n\\phi_X'(u) &amp;= \\int ix e^{iux} f_X(x) \\,dx \\\\\n\\phi_X'(0) &amp;= i \\int x f_X(x) \\,dx = i \\mathbb{E}[X]\n\\end{align*}\n</code></pre> <p>Thus, the expected value of a random variable is \\(\\mathbb{E}[X] = \\frac{1}{i} \\phi_X'(0)\\).</p> <p>By a similar calculation, one can show that the characteristic of a size-biased random variable is closely related to the distribution of the associated unbiased variable:</p> \\[ \\phi_{X^\\mathrm{s}}(u) = \\frac{1}{i \\mathbb{E}[X]} \\phi_X'(u) \\] <p>Given the characteristic function for an exponential is \\(\\phi(u) = (1 - i \\lambda^{-1} u)^{-1}\\), it is relatively straigthforward to compute the mean of the size biased exponential:</p> \\[ \\begin{align*} \\mathbb{E}[X^\\mathrm{s}] &amp;= \\frac{1}{i} \\phi_{X^\\mathrm{s}}'(0) \\\\ &amp;= \\frac{1}{i^2 \\mathbb{E}[X]} \\phi_X''(0) \\\\ &amp;= \\frac{-1}{\\mathbb{E}[X]} \\phi_X''(0) \\end{align*} \\] <p>The derivatives are relatively straightforward:</p> \\[ \\begin{align*} \\phi_X'(u) &amp;= (-1)(-i \\lambda^{-1}) (1 - i \\lambda^{-1}u)^{-2} \\\\ &amp;= i \\lambda^{-1} (1 - i\\lambda^{-1}u)^{-2} \\\\ \\phi_X''(u) &amp;= (-2)(-i \\lambda^{-1})(i \\lambda^{-1})(1 - i \\lambda^{-1} u)^{-3} \\\\ &amp;= -2 \\lambda^{-2} (1-i\\lambda^{-1}u)^{-3} \\\\ \\phi_X''(0) &amp;= -2 \\lambda^{-2} \\end{align*} \\] <p>And thus \\(\\mathbb{E}[X^\\mathrm{s}] = 2 \\lambda^{-1} = 2 \\mathbb{E}[X]\\).</p> <p>This same procedure could be followed for other distributions, which would be impossible to integrate.</p>"},{"location":"generating_functions/#moment-generating-functions","title":"Moment generating functions","text":"<p>[TBD]</p>"},{"location":"inference/","title":"Statistical inference","text":"<p>The typical approach in statistics is to teach frequentist inference ---things like \\(t\\)-tests--- before Bayesian statistics. The reason is that Bayesian statistics is mathematically more complex, it was developed after frequentist statistics, and is more confusing. However, the reason Bayesian statistics is confusing is because inference is confusing, and pretending that it is not leads to some critical misunderstandings about statistical inference.</p> <p>Rather than hide the fact that inference is confusing, I introduce the concept of inference in this chapter, contrasting frequentist and Bayesian inference, and showing very little math.</p> <p>Having dealt with inference's limitations, I will go on to the nuts and bolts of \"doing\" statistical inference. But read this chapter first, so you don't hold on to some critical misconceptions.</p>"},{"location":"inference/#statistical-testing-is-young","title":"Statistical testing is young","text":"<p>Statistical hypothesis testing, or \"significance testing\", is one of the most widespread methods in science. If you pick up an article of Science or Nature and read any scientific article, there will almost certainly be a \\(p\\)-value in it. I put statistical tests after the incredibly fundamental concepts of observation, quantitation, and experimentation as the building blocks of contemporary science.</p> <p>Of these fundamental scientific concepts, statistical testing is by far the youngest. In the Western scientific tradition, observation as a method goes back to Aristotle (b. 384 BC). Other ancient Greeks were using quantitative measurements and geometry to determine things like the diameter of the Earth. Experimentation may as we now think of it was championed by Francis Bacon (b. 1561 AD) but probably has much older roots. Our earliest examples of statistical hypothesis testing date to the 1700s --one of the early examples will be explored in a later chapter-- but testing as we now understand it was formalized by Ronald Fisher (b. 1890), Jerzy Neyman (b. 1894), and Egon Pearson (b. 1895).</p> <p>The fact that a young method, whose formalization is less than 100 years old, has permeated nearly all of science speaks to its intellectual appeal. I think it also clarifies why statistical testing and interpretation of \\(p\\)-values is such a controversial issue in contemporary science: we as a scientific community simply have not had enough time to fully digest the idea. Not only is the conceptual basis of statistical testing not fully refined, we have also not found the best ways to explain this concept to one another.</p>"},{"location":"inference/#statistical-inference-is-philosophically-confusing","title":"Statistical inference is philosophically confusing","text":"<p>Not only is statistical testing a relatively young idea, but statistical inference as a conceptual approach is deeply entwined with some of the fundamental philosophical questions that underly probability theory. In my experience, the students and practitioners that are my target audience for this book are challenged by these philosophical problems of statistical inference just as much as they stuggle with the mathematical aspects of statistical testing!</p> <p>Rather than sweep those philosophical questions under the rug, I will lay them out, to avoid confusion later.</p>"},{"location":"inference/#inference-is-philosophically-confusing","title":"Inference is philosophically confusing","text":"<p>Inference means taking some concrete cases and making a broader conclusion. Inference is always contrasted with deduction, which does the opposite, reasoning from broad rules to particular circumstances. The classic example of deduction is: all humans are mortal, Socrates is human, therefore Socrates is mortal. We reason from a general rule about mortality to a particular case of Socrates's mortality.</p> <p>Inference goes from particular to general. For most of humanity's history, we did this without worrying. Plato felt confident asserting that all humans are mortal because he had never heard a counterexample: because every human he observed or had heard about was mortal, he reasoned that every human ---every one who ever lived and every one which will live--- is mortal.</p> <p>[inverse probability]{.mark}</p> <p>Thousands of years after Plato, the philosopher David Hume questioned this kind of logic. He said, \"Sure, every human so far has been mortal. But that's not proof positive that no human will ever be immortal.\" We today are very comfortable to say that \"correlation does not prove causation\", but Hume took that logic to its extreme: if correlation does not prove causation, then seeing that every single one of billions of human who lived is mortal does not prove that the next human is guaranteed to be mortal. If we can't even prove that humans are mortal, then how do we conclude anything?[^1]</p> <p>This may seem abstract, but I want to point out that inference is philosophically confusing so that you will not be surprised to hear that inferential statistics is confusing!</p>"},{"location":"inference/#null-hypotheses-are-our-imperfect-approach-to-frequentist-inference","title":"Null hypotheses are our imperfect approach to frequentist inference","text":"<p>[1. Freq. stats cannot adress states of nature. 2. But we want \\(P[\\theta|X]\\). Waht to do? 3. Trick is to use \\(P[X|\\theta]\\) to make p-values and supplement that with CIs.]{.mark}</p> <p>We resolve the paradox of using frequentist statistics to make inferences about the character of the universe ---things which do not have frequencies--- using a trick called a null hypothesis. Rather than asking, \"Given the data, what's the probability that my hypothesis is correct?\", we construct an uninteresting, null hypothesis and then reverse the question, \"Imagine this null hypothesis were true. Then what's the probability of getting data like mine?\"</p> <p>For example, if I think you're cheating in a card game, the natural question is to ask, \"Given the cards I've seen you draw, what's the probability that you're cheating?\" The point of a null hypothesis is to flip it around: \"Imagine that you were playing fair. What's the probability you would be winning this month?\"</p> <p>To emphasize how these two questions are different, I'll introduce the typical mathematical symbols for these concepts: \\(\\theta\\) is the event that the null hypothesis is correct, and \\(X\\) is the event that I observed the data I did. Our natural inclination is to ask about \\(\\mathbb{P}[\\theta | X]\\). But in frequentist statistics, \\(\\mathbb{P}[\\theta]\\) is nonsense, either you are cheating or you are not. So we flip the question, instead asking about \\(\\mathbb{P}[X | \\theta]\\).</p> <p>In the previous section, we replaced an induction question with a deduction question: \\(\\mathbb{P}[\\theta | X]\\) reasons from the specific \\(X\\) to the general \\(\\theta\\), while \\(\\mathbb{P}[X | \\theta]\\) reasons from the general \\(\\theta\\) to the specific \\(X\\). In other words, we replaced the hard, inference question (are you cheating? are all human mortal?) with the easier, deduction question: if you were not cheating, how likely are you to have gotten such lucky hands?</p> <p>It's tempting to equate \\(\\mathbb{P}[\\theta | X]\\) and \\(\\mathbb{P}[X | \\theta]\\). This is called the prosecutor's fallacy. Imagine there was a crime, and the perpetrator left type AB negative blood at the scene. A person with type AB negative blood is accused of the crime. The prosecutor will say, \"Only 1% of Americans have type AB negative blood. So if this person were innocent, there is only a 1% chance that type AB negative blood would be found at the crime site. Thus, there is a 1% chance that this person is innocent.\"</p> <p>The prosecutor did a bait-and-switch. The null hypothesis \\(\\theta\\) is that the accused is innocent, and some random American committed the crime. The data \\(X\\) is having found AB negative blood. The prosecutor accurately concluded that \\(\\mathbb{P}[X | \\theta] = 1\\%\\). But then they fallaciously concluded that \\(\\mathbb{P}[\\theta | X] = 1\\%\\).</p> <p>If it's not obvious that this argument is fallacious, consider the more general proposition that \\(\\mathbb{P}[A | B]\\) equals \\(\\mathbb{P}[B | A]\\). For example, if it is snowing, what's the probability that the temperature is below freezing? Basically 100%. But if it is below freezing, what is the probability that it is snowing? Clearly much less than 100%. But clearly the two concepts have something to do with one another, which we will examine in the next section.</p>"},{"location":"inference/#bayess-theorem-is-the-only-way-to-ask-about-whether-your-hypothesis-is-correct","title":"Bayes's theorem is the only way to ask about whether your hypothesis is correct","text":"<p>[Move this to a Part IV]{.mark}</p> <p>The relation between \\(\\mathbb{P}[X | \\theta]\\) and \\(\\mathbb{P}[\\theta | X]\\) is laid out in Bayes's theorem: \\(\\(\\mathbb{P}[\\theta | X] = \\frac{\\mathbb{P}[X | \\theta] \\mathbb{P}[\\theta]}{\\mathbb{P}[X]}.\\)\\) This result follows almost immediately from the definition of conditional probability, laid out in Probability.</p> <p>Note that there are four terms in Bayes's theorem:</p> <ul> <li>\\(\\mathbb{P}[\\theta | X]\\): Given the data, what's the probability that hypothesis is true? This is what the court is trying to figure out: given the evidence, what's the probability that the accused is innocent (or guilty)?</li> </ul> <ul> <li>\\(\\mathbb{P}[X | \\theta]\\): Given the hypothesis, or null hypothesis, what's the probability of observing the data we did?[^2]</li> </ul> <ul> <li>\\(\\mathbb{P}[X]\\): What's the probability of observing this data? In this case, this is essentially the same as \\(\\mathbb{P}[X | \\theta]\\).</li> </ul> <ul> <li>\\(\\mathbb{P}[\\theta]\\): What's the probability, not considering the evidence, that this person is innocent?</li> </ul> <p>In the courtroom example, \\(\\mathbb{P}[X]\\) is the probability that, if we walked into a random courtroom trying a case with blood evidence, that the blood would be of type AB negative. If 1% of Americans have type AB negative blood, and people of different blood types commit crimes at the same rates, then \\(\\mathbb{P}[X] = 1\\%\\).</p> <p>In this example, \\(\\mathbb{P}[X | \\theta]\\) is very similar to \\(\\mathbb{P}[X]\\). Say there are 300 million Americans, of whom 1%, or 3 million, have type AB negative blood. If the accused is innocent, there are 299,999,999 type AB negative people left, so the probability of observing type AB negative blood is \\((3\\text{ million} - 1) / (300\\text{ million}) = 1\\% - 1 / (300\\text{ million})\\), or just slightly smaller than 1%.</p> <p>So, plugging in \\(\\mathbb{P}[X]\\) and \\(\\mathbb{P}[X | \\theta]\\), which we computed based on our knowledge about blood types, we can conclude that \\(\\mathbb{P}[\\theta | X]\\) is just slightly smaller than \\(\\mathbb{P}[\\theta]\\). In other words, the probability that the accused is innocent given the blood evidence is only very slightly smaller than the probability that they are innocent when not considering that evidence. Without any evidence, the probability that some random accused person happens to be the perpetrator is small, say 1 in 300 million. In other words, the prosecutor, if they have only the blood evidence, has a very bad case.</p>"},{"location":"inference/#bayess-theorem-is-about-updating-not-defining-probabilities","title":"Bayes's theorem is about updating, not defining, probabilities","text":"<p>In the example from the previous section, I showed that we can use data to compute the relationship between \\(\\mathbb{P}[\\theta]\\), the probability that a hypothesis is correct not considering the data, and \\(\\mathbb{P}[\\theta | X]\\), the probability that a hypothesis is correct given the data. In the language of Bayesian statistics, \\(\\mathbb{P}[\\theta]\\) is called the prior probability of \\(\\theta\\), and \\(\\mathbb{P}[\\theta | X]\\) is the posterior probability. In other words, the data \"updated\" the prior, pre-data-collection probability.</p> <p>The blood type example worked because the choice of \\(\\mathbb{P}[\\theta]\\) was pretty clear: if we picked an Amercian at random, what's the chance they are innocent of this particular crime? (Or, conversely, what's the chance that a randomly-selected person is the perpretrator?)</p> <p>In other cases, when \\(\\theta\\) is a hypothesis, \\(\\mathbb{P}[\\theta]\\) becomes extremely problematic. Take the Socrates example: what's the probability, if you hadn't observed the lives and deaths of any human, that all humans are mortal? Or take a science example: if Einstein had showed you his general theory of relativity, but neither you nor he had ever observed any experimental evidence or astronomical data, what would you say is the prior, pre-data-collection probability that his data is correct?</p> <p>As I discussed above, it only makes sense to assign probability to hypotheses in a Bayesian context. It doesn't make sense to ask on what fraction of replicates of Earth humans are mortal; instead you ask how confident you are that humans are mortal.</p> <p>Again, Bayes's theorem, is a perfectly mathematically sound theorem, but it only updates probabilities. Given a prior probability, and the data, it gives a posterior probability. But it can never tell you the prior probability. This is why people don't like Bayesian statistics. How can you put a prior probability on a hypothesis? Do you make a guess? \"Well, Einstein was smart, so that's a plus, but his theory sounds complicated, so that's a minus, and so I think maybe fifty-fifty?\"</p> <p>There's no mathematically sound answer to this question because there is no philosophically sound answer to this question. In other words, frequentist statistics and Bayesian statistics are both rigorous mathematical systems, and there is clear link between experimental science and the proportion-probabilities of frequentist statistics, but the link betwen Bayesian degree-of-confidence probabilities and what we have in our heads is not so clear.</p> <p>Having filled your mind with this philosophical cloud, let's get down to the nuts and bolts of statistical inference.</p> <p>[^1]: This argument is usually couched as the [\"sunrise problem\"]{.mark}: given that the sun has risen every day, what's the probability it will rise tomorrow? Is that probability exactly 1? [^2]: You may recognize that this wording is the definition of the \\(p\\)-value. In a way, the point of this chapter is to conceptually prepare you to be critical of the \\(p\\)-value, and to understand that is is not \\(\\mathbb{P}[\\theta | X]\\).</p>"},{"location":"probability/","title":"Probability","text":"<p>To develop a robust understanding of statistics, it is critical to understand basic concepts in probability theory. Probability theory is an entire branch of mathematics, and a very complicated one. Here I will give only the essentials as they have a direct bearing on statistics. I will leave many things out; what you see here is not a complete theory of probability.</p>"},{"location":"probability/#probability-has-at-least-two-definitions","title":"Probability has at least two definitions","text":"<p>We think about probability every day, for example, for prediction. Given the weather report, is it worth it for me to carry an umbrella? As a practicing scientist, you might ask yourself a different kind of question every day: given the results of my experiments, how likely is it that my hypothesis is correct?</p> <p>For such a familiar concept, probability is actually very philosophically thorny, and there are multiple interpretations of the concept of probability. Our intuitive sense of probability mostly aligns with the Bayesian interpretation, under which probability refers to a sense of confidence about an unknown thing, often a future event. If I say the probability of an event is 1%, it means, in a practical sense, that I'm willing to bet a fair amount of money it won't happen. Philosophically, it's hard to say what \"1% confident\" really means, but the concept should be very intuitive.</p> <p>Unfortunately, Bayesian statistics is mathematically and technically challenging. It also requires an assertion of a prior probability: you must state the probability that your hypothesis was true before you started the experiment. The apparent subjectivity of prior probabilities was repugnant to some early, prominent statisticians, who mostly directed the field of statistics away from the Bayesian interpretation and toward the frequentist interpretation of probability.</p> <p>In the frequentist interpretation, the probability of an outcome is the proportion, or frequency, of a that outcome when a trial is repeatedly many times. To say there is a 50% probability of flipping a coin and seeing heads means that, as you flip the coin over and over again, the proportion of flips that come up heads will approach 50%.</p> <p>For technical, philosophical, and historical reasons, when we talked about \"statistics\" with no modifier, we mean frequentist statistics. For better or worse, frequentist is the default; Bayesian is the exception.</p>"},{"location":"probability/#a-frequentist-interpretation-cannot-assign-probabilities-to-states-of-nature","title":"A frequentist interpretation cannot assign probabilities to states of nature","text":"<p>A problem with the frequentist definition is that probabilities can only be assigned to experiments or situations that can be repeated infinitely many times. It does not make sense to ask about the probability that it will rain tomorrow any more than it makes sense to ask about the probability than it makes sense to ask about the probability that it rained yesterday: it either rained or it didn't, either it will rain or it won't.</p> <p>The fact that we live in just one universe means that, under the frequentist interpretation, you cannot ask about the probability of a state of nature. Critically, you cannot ask about the probability your hypothesis is correct. Your hypothesis is either correct or not, so the probability that it is correct is either zero or one. You just don't know which of those two possibilities is right!</p> <p>As a scientist, this is deeply dissatisying. The whole point of statistical inference is to figure out what's going on in the world. I don't want to feed my hard-won experimental data into a statistical algorithm that says, \"If your hypothesis is true, then it is; and if it's not, it's not.\"</p> <p>Imagine that I show you a coin, then put my hands behind my back, and tell you the coin is in one of my hands. I ask you, \"What is the probability the coin is in my left hand?\" In the frequentist interpretation, probability is a fixed, objective thing. The fact that you know less than me about where the coin's location is irrelevant; the fact that the coin truly is in one hand or the other is all the frequentist interpretation knows.</p> <p>The Bayesian approach is more appealing in that it does in fact allow you to ask about the probability of states of nature. There is such a thing as a Bayesian probability that it will rain tomorrow or that your scientific hypothesis is correct. In a Bayesian interpretation, where probability can be subjective, it is OK that I put the probability of the coin being in my left hand as 0% or 100%, even while you assign the probability as just 50%. Nevertheless, we will pursue a frequentist interpretation here.</p>"},{"location":"probability/#probability-is-a-function","title":"Probability is a function","text":"<p>The frequentist definition of probability has to do with the proportions of infinitely-repeatable trials that have some outcome. An outcome is any thing that can happen as a result of an infinitely-repeatable trial. For example, if you flip a coin, you will get heads, or you will get tails.</p> <p>The sample space, written \\(\\Omega\\), consists of all outcomes as well as all possible combinations of outcomes. A combination of outcomes is called an event and written \\(\\omega\\). In other words, outcomes are individual, real things that might happen, while events are abstract groupings of outcomes. The sample space is the set of all events, which includes outcomes.</p> <p>In the context of flipping a coin, \"flipped heads\" is an outcome and an event. The empty set \\(\\varnothing\\) (\"nothing happened\") and the set of all outcomes \\(\\Omega\\) event (\"something happened\") are both events, but they are not outcomes. Some other examples of outcomes, events, and sample spaces are in Table 1.1{reference-type=\"ref\" reference=\"tab:outcome-event\"}.</p> Situation Outcomes Events Flipping a coin heads (\\(H\\)), tails (\\(T\\)) \\(\\varnothing\\), \\(H\\), \\(T\\), \\(\\Omega\\) Rolling a die \\(1, 2, \\ldots, 6\\) \\(\\varnothing\\); the outcomes \\(1, 2, \\ldots, 6\\); \\(\\binom{6}{2}\\) 2-event outcomes (e.g., 1 or 2); \\(\\binom{6}{3}\\) 3-outcome events (e.g., 1, 2, or 3); \\(\\ldots\\); \\(\\binom{6}{5}\\) 5-outcome events (e.g., any except 1); \\(\\Omega\\) Drawing a card 2 of Clubs, \\(\\ldots\\), Ace of Hearts \\(\\varnothing\\); the 52 outcomes (e.g., 2 of Clubs); \\(\\binom{52}{2}\\) 2-outcome events (e.g., a black Ace); \\(\\ldots\\); \\(\\binom{52}{51}\\) 51-outcome events (e.g., any card except 2 of Clubs); \\(\\Omega\\) Pick a random decimal number between 0 and 1 All real numbers between 0 and 1 \\(\\varnothing\\); intervals like \\([0, \\tfrac{1}{2}]\\); \"all subsets\" of \\([0, 1]\\); \\(\\Omega\\) Your experiment Each possible configuration of atoms at the moment of measurement \"Cancer cell died\", \"Electron had energy between 1 and 2 eV\", etc. <p>As an aside, I note that defining \"all possible subsets of outcomes\" is straightforward for a discrete case like flipping a coin or drawing a card, but it is complicated for a continuous case like drawing a random real number between \\(0\\) and \\(1\\). In continuous cases, a rigorous definition for the event space \\(\\Omega\\) requires concepts from measure theory, including a \\(\\sigma\\)-algebra. I avoid these complexities in this chapter at the cost of presenting a mathematically incomplete theory of probability.</p> <p>The probability function \\(\\mathbb{P}\\) maps events to numbers between \\(0\\) and \\(1\\):</p> \\[ \\mathbb{P} : \\Omega \\to [0, 1] \\] <p>For example, if \\(H\\) is the event of flipping heads, then \\(\\mathbb{P}[H] = \\tfrac{1}{2}\\). I use the square brackets to emphasize that \\(\\mathbb{P}\\) is a function of something other than numbers: \\(H\\) is not a number like \\(5\\), it is an event, a distinct mathematical object.</p> <p>Basic axioms of probability theory require that, for any probability function, the probability that \"nothing happened\" is zero and the probability that \"something happened\" is one:</p> \\[ \\begin{align*} \\mathbb{P}[\\varnothing] &amp;= 0 \\\\ \\mathbb{P}[\\Omega] &amp;= 1 \\end{align*} \\]"},{"location":"probability/#or-adds-event-probabilities-and-multiplies","title":"\"Or\" adds event probabilities; \"and\" multiplies","text":"<p>The axioms about probability functions also mean that there are specific rules around manipulating probabilities. For example, we're often interested in the relationships between events. What is that probability that this or that happened? What is the probability that this and that happened?</p> <p>If \\(A\\) and \\(B\\) are two events that don't have any constituent outcomes in common, we call then disjoint, and their probabilities add. For example, the probability of flipping a heads or flipping a tails is the probability of heads plus the probability of tails. Mathematically we write this as:</p> \\[ \\text{if } A \\cap B = \\varnothing \\text{, then } \\mathbb{P}[A \\cup B] = \\mathbb{P}[A] + \\mathbb{P}[B], \\] <p>where the \"cap\" \\(\\cap\\) means intersection (\"and\") and the \"cup\" \\(\\cup\\) means union (\"or\"), so this reads \"if no outcomes are in both events \\(A\\) and \\(B\\), then the probability of \\(A\\) or \\(B\\) is the sum of their individual probabilities.\"</p> <p>If \\(A\\) and \\(B\\) do have some overlap, you need to subtract out the probability of the overlap. For example, consider drawing a card from a standard 52-card deck. What is the probability of drawing a Jack or a Diamond? If you add up the probability of drawing a Jack and the probability of drawing a Diamond, you end up double-counting the Jack of Diamonds event. The solution is to subtract out the double-counted event:</p> \\[ \\mathbb{P}[A \\cup B] = \\mathbb{P}[A] + \\mathbb{P}[B] - \\mathbb{P}[A \\cap B]. \\] <p>If \"or\" adds probability, how do we find the probability of \\(A\\) and \\(B\\)? Say I flip two coins. What's the probability that I flip heads on the first coin and tails on the second? If \\(A\\) and \\(B\\) are independent events, then their probabilities multiply. The probability of flipping a heads then a tails is \\(\\tfrac{1}{2} \\times \\tfrac{1}{2} = \\tfrac{1}{4}\\). (The probability that I flip heads and tails on the same flip is zero, since \\(\\mathbb{P}[H \\cap T] = \\mathbb{P}[\\varnothing] = 0\\).)</p>"},{"location":"probability/#example-warings-theorem","title":"Example: Waring's theorem","text":"<p>With these simple rules of \"and\" and \"or,\" you can compute the probability of all sorts of things, like particular poker hands.</p> <p>Beyond some simple examples, these sorts of calculations can become very tedious. For example, say there are \\(n\\) events \\(A_1, \\ldots, A_n\\) that are not necessarily disjoint. What is the probability that exactly \\(r\\) of these events occur?</p> <p>Consider first the simple case with \\(n=2\\) and \\(r=1\\). The result is:</p> \\[ \\mathbb{P}[A_1] + \\mathbb{P}[A_2] - \\mathbb{P}[A_1 \\cap A_2] \\] <p>In other words, the chance that one of \\(A_1\\) or \\(A_2\\) happens is the probability that either or both of them happen, minus the probability that both happen.</p> <p>If you had \\(n=3\\) and \\(r=1\\), then you would need to start with the probabilities like \\(\\mathbb{P}[A_1]\\) that any of the three events happened, then subtract the three 2-way overlaps like \\(\\mathbb{P}[A_1 \\cap A_2]\\), but then add back in the single 3-way overlap \\(\\mathbb{P}[A_1 \\cap A_2 \\cap A_3]\\).</p> <p>Extending this logic arrives Waring's theorem, which gives the generalized probability of exactly \\(r\\) events occurring as:</p> \\[ \\sum_{t=0}^{n-r} (-1)^t \\binom{r+t}{t} S_{r+t} \\] <p>where</p> \\[ S_k = \\sum_{i_1 &lt; i_2 &lt; \\ldots &lt; i_k} \\mathbb{P}[A_{i_1} \\cap A_{i_2} \\cap \\ldots \\cap A_{i_k}] \\] <p>This sort of result is powerful, but only for these limited kind of counting problems.</p>"},{"location":"probability/#independence-and-conditional-probability","title":"Independence and conditional probability","text":"<p>Mathematically, \\(A\\) and \\(B\\) are independent if and only if their probabilities multiply. This might feel circular, so I will show you the logic.</p> <p>Intuitively, two events are independent if they do not depend on each other. If I flip two separate coins, or I flip the same coin twice, the one flip doesn't affect the other flip.</p> <p>It turns out that the simple phrase \"depends on\" produces deep philosophical questions. Just as \"probability\" had frequentist and Bayesian definitions, so there are multiple definitions of conditional probability. The easiest definition is:</p> \\[ \\mathbb{P}[A | B] \\equiv \\frac{\\mathbb{P}[A \\cap B]}{\\mathbb{P}[B]}, \\] <p>where \\(\\mathbb{P}[A | B]\\) is pronounced \"the probability of \\(A\\) given \\(B\\)\".</p> <p>Although the definition of conditional probability is a mathematical axiom that cannot be proven or disproven, it is easy to get an intuitive picture of why it is chosen as an axiom. In a frequentist interpretation of probability, \\(\\mathbb{P}[A | B]\\) is, on a denominator of the trials in which \\(B\\) happened, the proportion of trials in which \\(A\\) also happened. Say \\(n_B\\) is the number of trials in which \\(B\\) happened, \\(n_{AB}\\) is the number of trials in which \\(A\\) and \\(B\\) happened, and \\(n\\) is the total number of trials. Then the proportion we're talking about is \\(n_{AB} / n_B\\), which corresponds to \\(\\mathbb{P}[A \\cap B] / \\mathbb{P}[B]\\).</p> <p>I like to think of conditional probability as \"zooming in\" to a smaller sample space: rather than thinking about all of \\(\\Omega\\), we are thinking only about the subset of outcomes \\(\\omega\\) where \\(B\\) happened. To find \\(\\mathbb{P}[A|B]\\), we imagine that \\(B\\) is the entire sample space and consider the probability of \\(A \\cap B\\) in that space.</p> <p></p> <p>If \\(A\\) and \\(B\\) are independent, then \\(A\\) does not \"depend on\" \\(B\\): the probability of \\(A\\) given that \\(B\\) happened is equal to the probability of \\(A\\) without knowing anything about \\(B\\): if \\(A\\) and \\(B\\) are independent, then \\(\\mathbb{P}[A | B] = \\mathbb{P}[A]\\), and thus \\(\\mathbb{P}[A \\cap B] = \\mathbb{P}[A] \\times \\mathbb{P}[B]\\).</p>"},{"location":"probability/#partitions","title":"Partitions","text":"<p>Many problems become much easier with partitions. A partition is a set of mutually-disjoint events \\(B_1, \\ldots, B_n\\) that cover the whole space of events:</p> \\[ \\begin{gather*} B_i \\cap B_j = \\varnothing \\text{ for } i \\neq j \\\\ \\cup_{i=1}^n B_n = \\Omega \\end{gather*} \\] <p>Then you can break down any event based on this partition:</p> \\[ \\mathbb{P}[A] = \\sum_{i=1}^n \\mathbb{P}[A|B_i] P[B_i] \\] <p>The proof is elementary, but the impacts are very useful.</p>"},{"location":"probability/#example-monty-hall-problem","title":"Example: Monty Hall problem","text":"<p>You are on a gameshow. There are three closed doors. Behind one door is a brand new card; behind the other two doors are \"zonks,\" nominally worthless things like a live goat or an old sweater. You pick a door at random but, to create tension, the host does not open the door you picked, but instead opens one of the other two doors, revealing a zonk. You are given a choice: do you stick with the door you initially picked, or switch to the other unopened door?</p> <p>It's clear that, initially, you have a 1 in 3 chance of picking the car. Naively, it might seem like, after the other door is opened, you have a 1 in 2 chance of picking the car either way, so that it doesn't matter if you switch.</p> <p>In fact, it is much better to switch. To see why, let \\(S\\) be the event that you get the car upon switching doors, and then partition on the events \\(C\\) and \\(Z\\) that you a priori picked the car or a zonk. Then:</p> \\[ \\mathbb{P}[S] = \\mathbb{P}[S|C] \\mathbb{P}[C] + \\mathbb{P}[S|Z] \\mathbb{P}[Z] = 0 \\cdot \\tfrac{1}{3} + 1 \\cdot \\tfrac{2}{3} = \\tfrac{2}{3} \\] <p>In other words, you are twice as likely to get the car if you switch.</p> <p>Another way to think about the problem is that: if you commit to not switching, then you keep that original 1 in 3 chance of getting the car, so it must be that switching has the remaining 2 in 3 chance.</p>"},{"location":"probability/#example-simpsons-paradox","title":"Example: Simpson's paradox","text":"<p>You run a clinical trial comparing two ways to treat kidney stones. Of the 350 patients who get treatment A, 273 have a good outcome. Of the 350 who get treatment B, 289 have a good outcome. Naively, it looks like treatment B is better.</p> <p>But now stratify the analysis by whether the kidney stones are large or small:</p> Stone size Treatment A Treatment B Small 81/87 (93%) 234/270 (87%) Large 192/263 (73%) 55/80 (68%) Total 273/350 (78%) 289/350 (83%) <p>Note that treatment A outperforms treatment B for each of the two stone sizes but appears to performs worse overall.</p> <p>To show how this relates to partitions, write \\(A\\) for having a good outcome from treatment A, and \\(S\\) and \\(L\\) for having a large or small stone:</p> \\[ \\mathbb{P}[A] = \\mathbb{P}[A|S] \\mathbb{P}[S] + \\mathbb{P}[A|L] \\mathbb{P}[L] \\] <p>and similarly for \\(B\\).</p> <p>Clearly, as a patient, I have either a small stone or a large stone, so I would seem to prefer A. Then how is it that B performs better overall?</p> <p>The answer is that stone size and treatment are not independent: more of the small stones, which have overall better outcomes, are treated with treatment B. Although B has a lower success rate for small stones, it appears to win out overall, since it gets used on more of the easy cases. (In reality, this is because treatment A was more invasive, and so doctors and patients opted for the less invasive but less effective treatment when the stone was smaller.)</p>"},{"location":"random_variables/","title":"Random variables","text":"<p>The math in the last chapter is perfectly good for computing the probability of flipping a coin and getting a certain number of heads in a row, or for computing the probability of a winning poker hand, but not much more.</p> <p>This chapter introduces random variables. Mathematically, random variables link outcomes with numbers. Intuitively, this link mirrors scientific measurement, the act of associating an outcome of an experiment with a number. Moving away from counting individual outcomes and events to looking at numbers will provide both useful mathematical abstraction as well as a more intuitive way to think about a scientific experiment.</p>"},{"location":"random_variables/#a-random-variable-is-a-function","title":"A random variable is a function","text":"<p>A random variable is a function, common written \\(X\\), that maps from the sample space, the set of all possible outcomes of an experiment, to the real numbers:</p> \\[ X : \\Omega \\to \\mathbb{R} \\] <p>As mentioned above, I think of a random variable as a mathematical analog to scientific measurement: outcomes are the actual experimental outcomes, and a random variable maps that outcome to the number you would see on your measurement device.</p> <p>Before learning probability, I thought of \"random variables\" as random number generators: I might \"draw\" ten values from a \"normally-distributed random variable\". However, as discussed in Functions, a random number generator is not a function in the mathematical sense. It is important to remember that, technically speaking, a random variable is a function.</p> <p>It is also important to remember that \"random variable\" does not mean \"a numerical variable that takes on a random value\". It would be very hard to prove any theorems about random variables if \"random\" meant that, every time you looked at the page, \"\\(X\\)\" meant something different! In common speech we might say that a random variable \"takes on\" some value, but I advise you to shy away from that during this book, because it encourages you to think about \"random variables\" not as functions but instead as random number generators.</p>"},{"location":"random_variables/#random-variables-are-classified-by-how-many-values-they-map-to","title":"Random variables are classified by how many values they map to","text":"<p>Random variables come in two major flavors, discrete and continuous. A discrete random variable \\(X\\) associates outcomes \\(\\omega \\in \\Omega\\) with a finite set of numbers \\(x_i\\), while a continuous random variable maps events to an infinite number of numbers.</p> <p>Note that the distinction between discrete and continuous random variables only has to do with the values they map to, not with the sample space they map from. If the sample space is discrete, we can clearly only have a discrete random variable. For example, when flipping a coin, there are only two outcomes, heads \\(H\\) or tails \\(T\\). You could imagine a random variable \\(X\\) encoding \"number of heads flipped\":</p> \\[ \\begin{aligned} X[H] &amp;= 1 \\\\ X[T] &amp;= 0 \\end{aligned} \\] <p>There are other random variables you could define from this, say this silly example:</p> \\[ \\begin{aligned} X[H] &amp;= 5\\pi \\\\ X[T] &amp;= -33 \\end{aligned} \\] <p>But you clearly cannot map from a finite number of inputs to an infinite number of outputs.</p> <p>In contrast, you can have an infinite space of outcomes and map it to a discrete set of values. For example, imagine I'm measuring the time required for a radioactive atom to decay. The outcomes are all possible times required for decay, which is an infinite number of possibilities. But I could define a simpler, discrete random variable that asks, \"Did it take less than 1 second, between 1 and 2 seconds, or more than 2 seconds?\" and then map those three swaths of outcomes to three, but only three, different numbers.</p>"},{"location":"random_variables/#random-variables-abstract-over-outcomes","title":"Random variables abstract over outcomes","text":"<p>The whole point of random variables is that we don't need to talk about individual outcomes. Enumerating the outcomes of a trial is easy when talking about flipping coins, or rolling a die, or drawing a card from a deck, but it's basically impossible to enumerate an infinite set of outcomes without referring to them by means of numbers. In other words, the only sensible way to talk about infinite sets of outcomes is to instead talk about the values that a random variable maps those outcomes to!</p> <p>From this point forward, we will only refer to events by the values that a random variable maps those events to. For example, if \\(X\\) represents the number of heads flipped on one coin toss, then \\(X = 0\\) refers to the event where the coin landed tails, and \\(X = 1\\) refers to the event where the coin landed heads.</p> <p>Tip</p> <p>Take careful note that \\(X=1\\) is an event, a probability theory object. Reading probability equations can become very confusing if you don't know what is a number and what is random variable!</p> <p>Because the probability function maps events to numbers, we can now write:</p> \\[ \\mathbb{P}[X = 0] = \\tfrac{1}{2} \\] <p>Take careful note of all the pieces in this equation:</p> <ul> <li>\\(X\\) is a function mapping events to numbers</li> <li>\\(X = 0\\) is the event composed of all the outcomes for which \\(X\\) takes on the value zero</li> <li>\\(\\mathbb{P}\\) is a function mapping events to numbers between zero and one</li> </ul> <p>Note that the same symbol \\(=\\) is used to mean two different things in this equation! In the first case, \\(=\\) turns a random variable (which is itself a function) and a number (0) to produce an event, while the second equals is an assertion that the two sides of the equation are the same number. To further emphasize that, consider this equation:</p> \\[ (X = 5) = \\varnothing \\] <p>It is impossible to flip five heads on a single coin flip, so the set of events that \\(X\\) maps to the number five is the empty set. You will never see anyone else write an equation like this, but I put it there to scare you, to make you less sure of what an equal sign means.</p> <p>Again, the nice thing is that, although the probabilities of the outcomes heads and tails both remain \\(\\tfrac{1}{2}\\), we no longer need to think about exactly what outcomes there are, what probabilities they have, and to what number the random variable maps each outcome. Instead, we can just examine the random variable, which has abstracted over everything else. A very pedantic person might have argued that, when you flip a coin, \"heads\" is actually a grouping of an infinite number of distinct outcomes: the coin might fall with the face on the coin pointing north, or west, or north-north-west; or the coin might have fallen into between the cushions of the couch and actually been standing almost straight up. Discrete sample spaces are clearly a mathematical fiction. Nevertheless, discrete random variables allow me to deal with a finite number of interesting events, and continuous random variables allow me to abstract over whatever was happening in nature and simply say, I guess this or that number with some probability.</p> <p>The previous chapter was not a waste, however, because the rules for manipulating probabilities work just as well for events like \"\\(X = 1\\)\" as they did for outcomes like \"flipped heads\". The philosophical and technical definition of probability still holds. For example, \\(\\mathbb{P}[(X = 1) \\cup (X = 0)]\\), the probability of the events that \\(X\\) maps to either zero or one, is the sum of the probabilities the two constituent, disjoint events \\(\\mathbb{P}[X=1]\\) and \\(\\mathbb{P}[X=0]\\).</p>"},{"location":"random_variables/#cumulative-distribution-functions","title":"Cumulative distribution functions","text":"<p>Now that we have random variables, we can ask how they are \"distributed\". If you are like me in days of yore, you think of random variables as random number generators that produce values to be visualized as a histogram, whose mathematical analog is the probability density functions (pdf's). In fact, it's mathematically more straightforward to define the cumulative distribution functions (cdf's) and use that to define the pdf.</p> <p>The cdf of a random variable \\(X\\), canonically written \\(F_X\\), is a function that maps from real numbers to the numbers between zero and one. It is the probability of the event that \\(X\\) maps to any value equal to or less than the given input value:</p> \\[ \\begin{gathered} F_X : \\mathbb{R} \\to [0, 1] \\\\ F_X(x) \\equiv \\mathbb{P}[X \\leq x] \\end{gathered} \\] <p>Note that the cdf is a function of numbers \\(x\\), which I emphasize by using regular parentheses around \\(x\\) in \\(F_X(x)\\).</p> <p>It follows that, for a discrete random variable, which takes on specific values \\(x_i\\), the cdf is just the sum of the probabilities of those specific values smaller than the given \\(x\\):</p> \\[ F_X(x) = \\mathbb{P}[X \\leq x] = \\sum_{j \\,:\\, x_j \\leq x} \\mathbb{P}[X = x_j] \\] <p>A discrete random variable therefore has maximum and minimum values:</p> \\[ \\begin{gathered} \\text{if } x &lt; x_\\mathrm{min} \\text{, then } F_X(x) = 0 \\\\ \\text{if } x \\geq x_\\mathrm{max} \\text{, then } F_X(x) = 1 \\end{gathered} \\] <p>A continuous random variable can also have a minimum and maximum. However, there are continuous random variables that can take on any real number. In those cases, the cdf approaches zero and one as \\(x\\) goes out toward infinity:</p> \\[ \\begin{gathered} \\lim_{x \\to -\\infty} F_X(x) = 0 \\\\ \\lim_{x \\to \\infty} F_X(x) = 1 \\end{gathered} \\]"},{"location":"random_variables/#probability-density-functions","title":"Probability density functions","text":"<p>For a discrete random variable, it is straightforward to define its probability mass function or \"pmf\", canonically written \\(f_X\\), which is just the probability that it takes on each discrete value \\(x_i\\):</p> \\[ \\begin{gathered} f_X : \\mathbb{R} \\to [0, 1] \\\\ f_X(x_i) \\equiv \\mathbb{P}[X = x_i] \\end{gathered} \\] <p>The analog for continuous random variables is the probability density function, abbreviated \"pdf\":</p> \\[ \\begin{gathered} f_X(x) : \\mathbb{R} \\to [0, 1] \\\\ f_X(x) \\equiv \\frac{d}{dx} F_X(x) \\end{gathered} \\] <p>Note that the pdf of a continuous random variable \\(f_X(x)\\) is not \\(\\mathbb{P}[X = x]\\). This may seem like a pedantic diversion, but I actually think it's important to avoid confusion. For a continuous random variable, \\(\\mathbb{P}[X = x]\\) is basically zero for any value of \\(x\\). For example, say \\(X\\) takes on values between 0 and 1 uniformly. Then it follows that:</p> \\[ \\begin{aligned} \\mathbb{P}[0 \\leq X \\leq 1] &amp;= 1 \\\\ \\mathbb{P}[0.495 \\leq X \\leq 0.505] &amp;= 0.1 \\\\ \\mathbb{P}[0.4995 \\leq X \\leq 0.5005] &amp;= 0.01 \\end{aligned} \\] <p>and so on. We normally don't say that \\(\\mathbb{P}[X = 0.500\\ldots] = 0\\), since that makes it sound like it's impossible for \\(X\\) to take on the value \\(0.5\\). Nevertheless, it should be clear that, from any practical point of view, the probability of getting exactly \\(0.500\\ldots\\) from your experiment is essentially zero. On the other hand, It does make sense to write ask about the density of \\(X\\) at 0.5, i.e., \\(f_X(0.5)\\).</p> <p>The word \"density\" in probability density function emphasizes that the pdf, when integrated, gives a probability:</p> \\[ \\int_{x_0}^{x_1} f_X(x) \\,dx = F_X(x_1) - F_X(x_0) = \\mathbb{P}[x_0 &lt; X \\leq x_1] \\] <p>In other words, the little-\\(f\\) pdf \\(f_X\\) is the derivative of the big-\\(F\\) cdf \\(F_X\\).</p> <p>It follows from the definition of the cdf and some fundamental calculus that</p> \\[ \\int_{-\\infty}^\\infty f_X(x) \\,dx = 1 \\] <p>In other words, all probability density must be somewhere. This also requires that</p> \\[ \\lim_{x \\to \\pm \\infty} f_X(x) = 0 \\] <p>so that probability density can't be \"hiding\" infinitely far away from the origin.</p> <p>The definitions I've given are simple ones, and they need to be refined to deal with more complicated functions. For example, if the cdf has discontinuities, you need careful with the limits on the integrals.</p>"},{"location":"random_variables/#independent-identically-distributed-random-variables","title":"Independent, identically-distributed random variables","text":"<p>If a random variable \\(X\\) has the same cdf as another random variable \\(Y\\) (and therefore also the same pdf), we say that \\(X\\) is \"distributed like\" \\(Y\\):</p> \\[ \\text{if } F_X(x) = F_Y(x) \\text{ for all } x, \\text{ then } X \\sim Y. \\] <p>Many cdf's are named. For example, you might say that a random variable \\(X\\) is distributed like a normal random variable, with mean \\(\\mu\\) and variance \\(\\sigma^2\\):</p> \\[ X \\sim \\mathcal{N}(x; \\mu, \\sigma^2). \\] <p>This is shorthand for saying</p> \\[ f_X(x) = f_\\mathcal{N}(x; \\mu, \\sigma^2) \\equiv \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left\\{-\\frac{1}{2} \\frac{(x-\\mu)^2}{\\sigma^2} \\right\\} \\] <p>Critically, to say that \\(X \\sim Y\\) does not imply that \\(X\\) and \\(Y\\) are the same function. In fact, they might be defined in terms of completely different sample spaces. The random variable \\(X\\) might be looking at stock prices and \\(Y\\) might be looking at the time between solar flares. All that is required is that \\(X\\) and \\(Y\\) deliver the same numbers with the same probabilities.</p> <p>Because frequentist statistics is concerned with repeatable trials, we will often consider independent, identically-distributed, abbreviated \"iid\", random variables. Identically-distributed means that all the random variables in the collection have the same cdf. Independent means that the probabilities of \"and\" events multiply:</p> \\[ \\text{$X$ and $Y$ are independent} \\iff \\mathbb{P}[(X = x) \\cap (Y = y)] = \\mathbb{P}[X = x] \\mathbb{P}[Y = y] \\] <p>It follows that, if the events in the sample spaces that \\(X\\) and \\(Y\\) map from are independent, then \\(X\\) and \\(Y\\) will be independent. So if \\(X\\) is defined with respect to one iteration of an experiment, and \\(Y\\) is defined with respect to another, physically independent iteration of that experiment, then \\(X\\) and \\(Y\\) are independent.</p> <p>In the context of iid random variables, it is actually common to write \"\\(X\\)\" as referring to a whole set of random variables. Thus,</p> \\[ X \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2) \\] <p>actually means that there is some set of random variables \\(X_1\\), \\(X_2\\), \\(\\ldots\\); that \\(f_{X_i}(x) = f_\\mathcal{N}(x; \\mu, \\sigma^2)\\) for every \\(i\\); and that \\(X_i\\) and \\(X_j\\) are independent for all \\(i \\neq j\\).</p> <p>In day-to-day speech, we might say, \"I draw \\(n\\) values from this random variable \\(X\\)\". Mathematically, this means, \"Consider \\(n\\) iid random variables distributed like \\(X\\)\".</p>"},{"location":"random_variables/#sums-and-products-of-random-variables","title":"Sums and products of random variables","text":"<p>If you know the cdf and pdf for a random variable \\(X\\), and you also know these values for another random variable \\(Y\\), you might be interested in the behavior of these two variables together. Say, for example, that I draw two random numbers from a normal distribution using a random number generation. What is the probability that their sum is above or below some threshold?</p> <p>If \\(X\\) and \\(Y\\) are random variables, we define a new random variable \\(Z = X \\stackrel{\\mathrm{rv}}{+} Y\\), where again, I put that little \"rv\" in there to remind you that the plus sign in \\(X + Y\\), where \\(X\\) and \\(Y\\) are functions like random variables, cannot mean the same thing as the plus sign in \\(x + y\\), where \\(x\\) and \\(y\\) are just numbers.</p> <p>The meaning of \\(X + Y\\) feels intuitive, like in my example about drawing numbers from a random number generator. But how do we define \\(X + Y\\) mathematically? Imagine first that \\(X\\) and \\(Y\\) are independent, discrete random variables. Then it should be that \\(\\mathbb{P}[Z = z]\\) is the probability of \\(X = x_i\\) and \\(Y = y_j\\) for all the cases where \\(x_i + y_i = z\\). Note that this, given some \\(z\\), this constrains \\(y_j\\) to by \\(z - x_i\\):</p> \\[ \\begin{aligned} f_Z(z) &amp;= \\sum_i \\mathbb{P}[X=x_i] \\mathbb{P}[Y=z-x_i] \\\\ &amp;= \\sum_i f_X(x_i) f_Y(z - x_i). \\end{aligned} \\] <p>A similar definition holds for when \\(Z = X + Y\\) is continuous. Rather than summing over all the outcomes in which \\(X\\) takes on certain values, we integrate:</p> \\[ f_Z(z) = \\int f_X(x) f_Y(z - x) \\,dx \\] <p>For a product of random variables \\(Z = XY\\), we need to use \\(f_Y(z/x)\\), and for a ratio of random variables \\(Z = X/Y\\), we need to use \\(f_Y(zx)\\). Note that, in each of these cases, the notations like \\(X + Y\\), \\(XY\\), and \\(X/Y\\) refer to a new random variable and also give us a hint about how to compute its pdf.</p>"},{"location":"random_variables/#dont-expect-the-expected-value","title":"Don't expect the expected value","text":"<p>The expected value \\(\\mathbb{E}[X]\\) of a random variable \\(X\\) is the probability-weighted average of the values it takes on. For a discrete random variable, you simply sum. For a continuous random variable, you need to integrate:</p> \\[ \\mathbb{E}[X] \\equiv \\begin{cases} \\sum_i x_i f_X(x_i) &amp; \\text{ for discrete random variables} \\\\ \\int x f_X(x) \\,dx &amp; \\text{ for continuous random variables} \\end{cases} \\] <p>Thus, just like \"maximum\", \"expected value\" is a function that maps random variables to numbers:</p> \\[ \\mathbb{E} : \\text{random variables} \\to \\mathbb{R} \\] <p>I use square brackets with \\(\\mathbb{E}\\) to emphasize that this is a function that maps from something other than numbers.</p> <p>The name \"expected value\" is misleading. We previously noted that, for continuous random variables, the probability of getting any particular number is essentially zero, so there's no particular number you should \"expect\". For discrete random variables, where there is finite probability of getting each particular number, the expected value actually need not be any of the values that \\(X\\) maps to. In our coin flip example, the random variable \\(X\\) measuring the number of heads flipped has expectation value:</p> \\[ \\mathbb{E}[X] = 0 \\times \\mathbb{P}[T] + 1 \\times \\mathbb{P}[H] = \\tfrac{1}{2}. \\] <p>You certainly don't ever expect to flip \\(0.5\\) heads!</p> <p>A better way to think of the expected value is as a measure of central position. There are of course multiple ways to quantify central position beyond the expected value, which is analogous to the mean or average. You can certainly map random variables to their medians and modes, but those mappings are substantially more complex and not as useful for our purposes.</p>"},{"location":"random_variables/#variance","title":"Variance","text":"<p>If the expected value is a measure of central position, then variance \\(\\mathbb{V}\\) is the corresponding measurement of spread or scale. It is a function of random variables that returns a nonnegative number:</p> \\[ \\mathbb{V} : \\text{random variables} \\to [0, \\infty). \\] <p>For a random variable \\(X\\), the variance is the expected value of the square of the deviation of the random variable from its own expected value:</p> \\[ \\mathbb{V}[X] \\equiv \\mathbb{E}[(X - \\mathbb{E}[X])^2] \\] <p>Because all those nested brackets are confusing, people often replace the notation \\(\\mathbb{E}[X]\\) with the notation \\(\\mu\\) so that the definition of variance reads \\(\\mathbb{V}[X] = \\mathbb{E}[(X - \\mu)^2]\\).</p> <p>Although \\(\\mu\\) is just a number when we say \\(\\mu = \\mathbb{E}[X]\\), the same notation \"\\(\\mu\\)\" cannot refer to a number when we write \\(X - \\mu\\). We can subtract numbers from the values that a function maps to, but we cannot subtract numbers from raw functions, any more than I can subtract a number from the operation of multiplication. In other words, \\(f(x) - 2\\) makes sense, but \\(f - 2\\) does not. Thus, \"\\(\\mu\\)\" in \\(X - \\mu\\) is actually a trivial random variable that maps its whole sample space to \\(\\mu\\), so that \\(\\mathbb{P}[\\text{``}\\mu\\text{''} = \\mu] = 1\\). Similarly, \\((X - \\mu)^2 = (X - \\mu) \\stackrel{\\mathrm{rv}}{\\times} (X - \\mu)\\) is another new random variable. Given random variables \\(X\\) and \\(Y\\), we saw earlier how to compute the pdf of a new random variable \\(XY\\), so we also know how to find the pdf for a random variable \\(X^2 = X \\times X\\).</p>"},{"location":"random_variables/#covariance","title":"Covariance","text":"<p>Starting with the definition of the expected value, you can easily show that it is a linear function: \\(\\(\\mathbb{E}[aX + bY] = a \\,\\mathbb{E}[X] + b \\,\\mathbb{E}[Y]\\)\\) Note that, just as \\(\\mu\\) transformed from a number to a random variable when we wrote \\(X + \\mu\\), so the number \\(a\\) in this equation transforms into a random variable when we write \\(aX\\).</p> <p>Variance, however is not linear. You can easily show that, for some number \\(a\\),</p> \\[ \\mathbb{V}[aX] = \\mathbb{E}[(aX - \\mathbb{E}[aX])^2] = \\mathbb{E}[a^2(X - \\mathbb{E}[X])] = a^2 \\, \\mathbb{E}[X]. \\] <p>With some more algebra, you can show that</p> \\[ \\mathbb{V}[X + Y] = \\mathbb{V}[X] + \\mathbb{V}[Y] + 2 \\,\\mathrm{Cov}[X, Y], \\] <p>where \\(\\mathrm{Cov}[X, Y]\\), the covariance of \\(X\\) and \\(Y\\) is</p> \\[ \\mathrm{Cov}[X, Y] \\equiv \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E][Y])}. \\] <p>To avoid the nested brackets, this is sometimes written</p> \\[ \\mathrm{Cov}[X, Y] = \\mathbb{E}[(X - \\mu_X)(Y - \\mu_Y)]. \\] <p>In words, the covariance is the expected value of the product of the deviations between the values that \\(X\\) and \\(Y\\) map to and the expected values of those random variables. Note that covariance is a function of two random variables:</p> \\[ \\mathrm{Cov} : (\\text{random variables})^2 \\to \\mathbb{R}. \\] <p>If \\(X\\) and \\(Y\\) are independent, then their covariance is zero. (The reverse is not necessarily true.) Some algebra will also show that \\(\\(\\mathbb{E}[XY] = \\mathbb{E}[X] \\, \\mathbb{E}[Y] + \\mathrm{Cov}[X, Y].\\)\\) Thus, for independent random variables, which we will deal with a lot, the expected value of the produce of two random variables is simply the product of their expectation values. These properties of the expected value will come in very handy in future sections.</p> <p>You may be more familiar with correlation than covariance. The most common definition of correlation, Pearson's correlation coefficient, typically written \\(\\rho\\), is just a rescaling of the covariance to values between \\(-1\\) and \\(+1\\):</p> \\[ \\begin{gathered} \\rho : (\\text{random variables})^2 \\to [-1, 1] \\\\ \\rho[X, Y] \\equiv \\frac{\\mathrm{Cov}[X, Y]}{\\sqrt{\\mathbb{V}[X] \\mathbb{V}[Y]}}, \\end{gathered} \\] <p>It turns out that dividing by that factor rescales the covariance, which can be any number, to a single range, \\(-1\\) to \\(+1\\). Thus, the correlation between two random variables does not depend on their scaling:</p> \\[ \\begin{gathered} \\mathrm{Cov}[aX, bY] = ab \\,\\mathrm{Cov}[X, Y] \\\\ \\rho[aX, bY] = \\rho[X, Y] \\end{gathered} \\]"},{"location":"random_variables/#higher-moments","title":"Higher moments","text":"<p>The expected value and the variance are two in a series of properties of random variables called moments. Just as the variance has a square in it, skewness has a cube and kurtosis has a fourth power. The pdf's of random variables with positive skewness have an asymmetric tail, trailing out to the right; negative skewness means a tail to the left. Kurtosis measures the \"fatness\" or \"heaviness\" of the tails.</p>"},{"location":"regression/","title":"Regression","text":"<p>\"Regression\" refers to a whole suite of tools, all of which have to do with fitting a model to data, predicting outputs from inputs. In a sense, the humble linear regression is the most basic machine learning technique: it is \"trained\" on some input data and then can make predictions.</p>"},{"location":"regression/#the-history-and-etymology-of-regression","title":"The history and etymology of regression","text":"<p>Francis Galton, a Victorian English scientist, was an early eugenicist: he was interested in how to improve humanity through selective breeding. He first asked if intelligence and \"greatness\" were hereditary.[^1] This presented some technical difficulties, so he next turned to an easier-to-quantify value: height. Do tall children tend to have taller children?</p> <p>Having developed an early form of what we call linear regression, he was distressed to find that the regression coefficient, that is, the relationship between inputs and outputs, was less than 1. In other words, parents who were taller than average tended to have taller-than-average children (i.e., the coefficient was greater than zero) but the children tended to not be as tall as the parents (i.e., the coefficient was less than 1). The converse was true for shorter parents: they tended to have shorter children, but not quite as short as they were. Galton summarized this phenomenon as \"regression to the mean\": both tall and short people tended to move back toward the middle. As a eugenicist, he would have preferred a coefficient greater than 1, so that tall people would have extra-tall children.</p> <p>Regression toward the mean is still an important concept. For example, a naive experimental design might be to take the 10% of students in a class with the worst grades on a test, do some intervention to try to improve their skills, and then administer a test again. Even if the intervention does nothing, you expect those 10% of students to perform better on the second test than on the first, simply because a student is more likely to produce a middling score than an extremely bad score, regardless of their performance on the first test.</p> <p>For the most part, however, \"regression\" is now used to refer to this suite of tools with no reference to Galton's original idea of regression \"to the mean\".</p> <p>[^1]: This was the subject of his 1869 Hereditary Genius. He found that \"eminent\" people were more likely to have \"eminent\" siblings, parents, and children, and that this relationship appeared to drop off with distance. Of course, \"people\" here means \"men\".</p>"},{"location":"statistics/","title":"Statistics","text":""},{"location":"statistics/#a-statistic-is-a-function-of-data","title":"A statistic is a function of data","text":"<p>In common speech, we use the terms like expected value, variance, covariance, and correlation in reference to data, that is, to sets of fixed numbers, rather than to mathematical objects like random variables. In this chapter, we will bridge the gap between the probability theory we have considered so far and the practice of statistics.</p> <p>The term \"statistics\" has two meanings. As a singular noun, a statistic, is a function that maps data, that is, an ordered list of numbers, to a real number. We use \\(t\\) as a stand-in to mean \"any old statistic\":</p> \\[ t : \\mathbb{R}^n \\to \\mathbb{R} \\] <p>To study the properties of a statistic, we will define a random variable that is analogous to that statistic. A random variable that is analogous to a statistic is called an estimator, often written \\(T\\).</p> <p>For example, the arithmetic mean maps a list of numbers \\(x_i\\) to a single number, and so is a statistic, which I will write as \\(t\\), such that \\(t \\equiv (1/n) \\sum_i x_i\\). There is an analogous estimator \\(T \\equiv t(X) = (1/n) \\sum_i X_i\\). As I emphasized in the previous chapter, although the notation in these two equations is almost identical, just replacing some lowercase letters with uppercase ones, it is critical to remember that adding random variables is not the same thing as adding numbers. The function \\(t\\) maps numbers to a number, so \\(t(X)\\) is, strictly speaking, nonsense. But in the same way that we use the plus sign when adding numbers, like in \\(x + y\\), and for \"adding\" random variables, like in \\(X + Y\\), we use \\(t\\) to refer to an operation on numbers as well as to the analogous operation on random variables.</p> <p>The word \"statistics\" ending in s could either be the plural of statistic, that is, many functions of the data, or it can refer to the field of study of estimators. Thus it is syntactically and semantically correct to say \"statistics is fun\".</p> <p>Up to this point, we have discussed random variables in a deductive way: given the distribution of the random variable, you ask what its expected value or variance is. Estimators are the starting point of statistics (the field of study), which goes the opposite way: given some data points, what can we say about the distribution of the random variables that gave rise to that data?</p>"},{"location":"tests/","title":"Tests","text":"<p>Things to do:</p> <p>Things to do:</p> <ul> <li>If you cannot construct the entire universe of possible observations (events?), then you are actually sampling from the \\(p\\)-value distribution</li> <li>You observe a specific statistic, and then you sample another n.</li> <li>Under the null hypothesis, the observed value and the other n came from the same distribution.</li> <li>The p-value is the probability of getting a value as small as the observed one, given that the null is true.</li> <li>If you know the \"shape\" of the distribution, you can compute this exactly (e.g., as we did in our sign test).</li> <li>If you don't know the shape, then you need to do this empirically, with a parametric test.</li> <li>You say, I have n+1 items in a list. What's the chance that this particular item falls at rank m or below (or above)? That's \\((r+1)/(n+1)\\).</li> <li>It's a mistake to do \\(r/n\\), but not a terrible mistake, except that it gives you \\(p=0\\)!</li> <li>Overestimates small \\(p\\) (because you go up to 1/n+1) and underestimates large, but that's normally not a problem. The important thing is avoiding p=0 bias.</li> <li>Also, this provides an estimate of the \\(p\\)-value, since you're basically doing a binomial test: of \\(n+1\\) total coin flips, \\(r+1\\) come out heads. What's the probability of heads? This has a confidence interval around it.</li> </ul>"},{"location":"tests/#frequentist-inferential-statistics-statistical-tests-and-confidence-intervals","title":"Frequentist inferential statistics: statistical tests and confidence intervals","text":"<p>[1. Define p-value (1 and 2-sided). 2. Zea mays example data. 3. Binomial sign test. 4. Fisher's weird sum. 5. Paired test (wilcox)?. 6. Paired t-test.]{.mark}</p> <p>[Add a chapter for statistical power and sample size. Define Type I and Type II errors. Chapters on parametric vs. nonparametric. Permutation tests.]{.mark}</p> <p>\\(T=t(X)\\) is statistic. \\(t^\\star = t(x)\\) is obs. value. \\(p=\\mathbb{P}[T&gt;t^\\star]\\) or \\(\\mathbb{P}[T&lt;t^\\star]\\). 2-sided is \\(\\mathbb{P}[T&lt;t^\\star \\text{ or ] T &gt; t_0 + (t^\\star-t_0)}\\), where \\(t_0\\) is some central value?</p> <p>The previous chapter laid out the conceptual points of inference. In this chapter, I lay out the nuts and bolts of what most people think statistics is: tests.</p> <p>The mathematics of statistical tests follows directly from the previous mathematical chapters: we define some estimators, functions of the data that predict some property of the data, and determine their variance. We just give things different names and interpret them differently.</p> <p>But rather than begin with the mathematics of tests, I'll start with the concepts of how they work, and show how to implement them algorithmically and computationally. Once you understand what the test is doing, we can get back to unpacking some of the math you will hear bandied about around them.</p>"},{"location":"tests/#the-ingredients-in-a-statistical-test","title":"The ingredients in a statistical test","text":"<p>To run a statistical test, you need:</p> <ol> <li>Observed data.</li> <li>A listing of the universe of observations that could have been made, of which the actual observation is one instance.</li> <li>A null hypothesis that associates a probability with each possible observation.</li> <li>A test statistic that summarizes how interesting the observed data are in a single number.</li> </ol> <p>Every statistical test that you know of can be defined by how it selects its universe of possible observations, its null hypothesis, and its test statistic.</p> <p>The test will give you two outputs:</p> <ol> <li>A \\(p\\)-value, which is the probability of observing data as extreme (or \"interesting\") as what you did observe, given the null hypothesis.</li> <li>Confidence intervals on the test statistic.</li> </ol> <p>More concretely, the \\(p\\)-value is the proportion of the observations in the universe of observations, weighted by their probabilities, that have test statistics that are more extreme (i.e., either larger or smaller) than the test statistic for the observed data. The confidence interval says that, using the observed data as our best estimates of the underlying truth, what range of test statistics would we expect?</p>"},{"location":"tests/#interpreting-p-values-and-confidence-intervals","title":"Interpreting \\(p\\)-values and confidence intervals","text":"<p>The \\(p\\)-value was developed by Fisher, probably the one person who had the most influence on modern statistics. He used the \\(p\\)-value, a rigorously-derived number, to reason informally about data. If the \\(p\\)-value too big, by some standard to be determined ad hoc, then the data didn't show anything interesting. In different books and papers, for different data sets, Fisher used different standards for \"too big\", but \\(0.05\\) was a recurring choice.</p> <p>Later, Neyman and Pearson (the younger) reformulated Fisher's method as a kind of decision-making under uncertainty. If you were a decision-maker, and you had to make a decision, what would you do when faced with certain data? They suggested you pick some specific confidence level, written \\(\\alpha\\), and make your decision based on whether \\(p\\) was greater than or less than \\(\\alpha\\).</p> <p>Although Fisher furiously disagreed with Neyman and Pearson about how to interpret \\(p\\)-values, the canonical \\(\\alpha = 0.05\\) somehow made it into the scientific literature.</p> <p>As laid out in the previous chapter, \"what is the probability that this data is true, given the data?\" is a very tricky question. The critical point is that the \\(p\\)-value is, in the language of that chapter, \\(\\mathbb{P}[X | \\theta]\\), but we want to know about \\(\\mathbb{P}[\\theta | X]\\). The link between these is the mysterious prior probability \\(\\mathbb{P}[\\theta]\\).</p> <p>I think my only take-away is that you should trust your gut more than a \\(p\\)-value. First, Fisher developed statistics as a way to rigorously show that data weren't interesting, a mathematical counterbalance to humans' ability to find interesting patterns in data, especially their own. He meant \\(p\\)-values as a way to deflate puffed-up conclusions, not to puff them up. Second, Fisher, who invented \\(p\\)-values, couldn't really agree with other extremely intelligent people about exactly what they meant. So don't make yourself crazy trying to figure it out too precisely either! Just know that big \\(p\\)-values, where \"big\" depends on the intellectual context, mean the data aren't interesting, where \"interesting\" is defined by the null hypothesis.</p> <p>[Move all of the above to previous chapter where p-values are defined]{.mark}</p>"},{"location":"tests/#a-test-with-a-simple-explanation-but-difficult-math-the-binomial-proportion-test","title":"A test with a simple explanation but difficult math: the binomial proportion test","text":"<p>To understand how to go from a test's ingredients to its outputs, I will work through many examples of increasing complexity. The first and simplest is this: I flip a coin \\(n\\) times, seeing some sequence of heads and tails, and I want to infer whether the coin is fair.</p> <ul> <li>The observed data is the sequence of heads and tails I flipped.</li> <li>The universe of possible observations is all the sequences of heads and tails I could have flipped. So if \\(n=2\\), the universe is the four cases \\(HH, HT, TH, TT\\).</li> <li>The null hypothesis is that heads and tails are equally likely.</li> <li>The test statistic is the number of heads I flipped. This statistic, like all the other statistics discussed in chapter XX, is a function of the data: given some sequence of heads and tails, it just counts up the number of heads.</li> </ul>"},{"location":"tests/#one-and-two-sided-p-values","title":"One- and two-sided \\(p\\)-values","text":"<p>The \\(p\\)-value is the probability of data as extreme as what we observed, or more extreme, given the null hypothesis that \\(p = 0.5\\).[^1] The tricky part of this definition is understand what \"as extreme or more\" means.</p> <p>\"Extremeness\" is quantified by the test statistic, which summarizes each of the possible observations ---each length-\\(n\\) sequence of heads and tails--- into a single number, the number of heads in that sequence. \"As extreme or more\" can mean 3 things:</p> <ul> <li>as small as what was observed, or smaller,</li> <li>as large as what was observed, or larger, or</li> <li>as far away from the \"center\" as was observed, or further away.</li> </ul> <p>Exact tests: you can count every possibility for a binomial</p> <p>The first two cases call for one-sided \\(p\\)-values and can be used if you have an a priori hypothesis that the number of heads that you will observe will be much higher or lower than what you would expect if the coin were fair. So if I want to seek support for my a priori hypothesis that the coin is more likely to land tails than heads, then I would use that one-sided \\(p\\)-value. If I don't have an a priori expectation about whether the flips will be enriched for heads or tails ---if I just think that the coin is not fair, but I don't know which way--- then I need a more conservative, two-sided \\(p\\)-value.</p> <p>In this example, the null hypothesis of a fair coin means that all the possible sequences of heads and tails are equally likely, so to compute a one-sided \\(p\\)-value, I just need to count up the number of sequences that are more extreme than the observed one: \\((\\text{one-sided \\(p\\)-value} = \\frac{\\text{number of possible sequences more extreme than observed}}{\\text{total number of sequences}}\\)\\) You may know that the number of length-\\(n\\) heads-tails sequences is \\(2^n\\) and that the number of those sequences that has \\(x\\) heads is \\(\\binom{n}{x} = n! / \\left[ x! (n-x)! \\right]\\). Thus, for this example, if we have a \"low-sided\" expectation about the number of heads: \\((\\text{``low-sided'' one-sided \\(p\\)-value} = \\frac{\\sum_{y=0}^x \\binom{n}{x}}{2^n}\\)\\) A \"high-sided\" \\(p\\)-value would sum from \\(x\\) up to \\(n\\).</p> <p>For the two-sided \\(p\\)-value, we need to sum up the \"low-side\" and \"high-side\" \\(p\\)-values. Because the observed \\(x\\) falls to one side of the central expectation \\(\\mathbb{E}[x] = n/2\\), we need to symmetrize to find the other value. For example if \\(x\\) is low, we make another, mirrored \"high\" \\(x_\\mathrm{hi} = n-x\\): \\((\\text{two-sided \\(p\\)-value} = \\frac{\\sum_{y=0}^x \\binom{n}{x} + \\sum_{y=n-x}^n \\binom{n}{x}}{2^n}\\)\\) If \\(x &gt; n/2\\), then the sums would go from \\(0\\) to \\(n-x\\) and from \\(x\\) to \\(n\\).</p>"},{"location":"tests/#checking-for-a-specified-unfairness-of-a-coin","title":"Checking for a specified unfairness of a coin","text":"<p>In the first example, we used a null hypothesis that heads and tails were equal, which simplifed thinking about \\(p\\)-values because we could simply count up the equiprobable sequences of heads and tails.</p> <p>Now, we generalize slightly, allowing a null hypothesis that the probability \\(p\\) of flipping heads is not exactly \\(\\tfrac{1}{2}\\):</p> <ul> <li>The observed data is, again, the sequence of heads and tails I flipped.</li> <li>The universe of possible observations is, again, all the sequences of heads and tails I could have flipped.</li> <li>The null hypothesis is that the probability of flipping heads is \\(p\\).</li> <li>The test statistic is the number of heads I flipped.</li> </ul> <p>The mathematically savvy will know that the probability of \\(y\\) successful trials among \\(n\\) total trials, with each trial having a probability \\(p\\) of success, is the binomial distribution: \\(\\(\\mathrm{Binom}(y; n, p) = \\binom{n}{y} p^y (1-p)^{n-y}\\)\\) This is just like the above, where we counted the number of sequences, only now we need to weight them by the number of more (or less) probably heads (and tails): \\((\\text{``low-sided'' one-sided \\(p\\)-value} = \\sum_{y=0}^x \\mathrm{Binom}(y; n, p)\\)\\)</p>"},{"location":"tests/#computing-confidence-intervals","title":"Computing confidence intervals","text":"<p>Recall that a confidence interval is a method: it is a pair of statistics, functions of the data, that will, in a certain proportion \\(\\alpha\\) of replicate experiments, will contain the true value, regardless of what the true value is.</p> <p>In this example, the \"true value\" in question is the unknown probability \\(p_\\mathrm{true}\\) that the coin will flip heads, which would be anything between 0 and 1. So we need a method that, whether \\(p_\\mathrm{true} = 10^{-6}\\) or \\(p_\\mathrm{true} = 0.999\\), will produce an interval that contains the true \\(p_\\mathrm{true}\\) in a proportion of \\(\\alpha\\) of the data that would arise in replicates of that experiment.</p> <p>As mentioned earlier, there are multiple methods that can achieve this result. The one that doesn't rely on any kind of approximation, which we will address later, is the Clopper-Pearson interval. This says to find the largest value \\(p_+\\) for which \\(\\sum_{y=0}^x \\mathrm{Binom}(y; n, p_+) &gt; \\alpha/2\\) and the smallest value \\(p_-\\) for which \\(\\sum_{y=x}^n \\mathrm{Binom}(y; n, p_-) &gt; \\alpha/2\\). In other words, if \\(\\alpha=95\\%\\), pick an uppper limit on \\(p_\\mathrm{true}\\) so that, if that were the true value, in exactly \\(2.5\\%\\) of replicates of the experiment, you would get numbers of heads larger than what you observed. Similarly, pick a lower limit so that, if that were the true probability of flipping heads, in \\(2.5\\%\\) of replicates, you would get numbers of heads larger than what you observed.</p> <p>If this seems like a big, confusing headache, I hope you take it as a sign that confidence intervals are confusing, not that you're not understanding!</p> <p>[Now with computers, Clopper-Pearson is easy]{.mark}</p>"},{"location":"tests/#a-test-with-simple-math-but-a-difficult-explanation-the-z-test","title":"A test with simple math but a difficult explanation: the \\(z\\)-test","text":"<p>The elegance of the previous example is that it works by simply enumerating all the possible sets of observations. The inelegant part is that there can be many: \\(2^n\\) grows quickly with \\(n\\), making the sums in the \\(p\\)-value calculations onerous. Furthermore, the math of the Clopper-Pearson interval is a bit of a mess and hard to articulate.</p> <p>As a counterexample, consider the humble \\(z\\)-test, in which your experiment consists of drawing a single number from a pre-specified population, specifically, a normal distribution with known mean \\(\\mu\\) and variance \\(\\sigma^2\\):</p> <ul> <li>The observed data is the single number \\(x\\) you drew.</li> <li>The universe of possible observations is all numbers, from \\(-\\infty\\) to \\(+\\infty\\).</li> <li>The null hypothesis is that the number \\(x\\) was drawn from a normal population with mean \\(\\mu\\) and variance \\(\\sigma^2\\).</li> <li>The test statistic is just the number \\(x\\).</li> </ul> <p>In this example, rather than summing up over the discrete universe of possibilities, you need to integrate to find the \\(p\\)-value: \\((\\text{``low-sided'' \\(p\\)-value} = \\int_{y=-\\infty}^x f_\\mathcal{N}(y; \\mu, \\sigma^2) \\,\\mathrm{d}y = F_\\mathcal{N}(y; \\mu, \\sigma^2).\\)\\)</p>"},{"location":"tests/#cis","title":"cis","text":"<p>$$ \\begin{aligned} 1 - \\alpha &amp;= \\mathbb{P}[X_- \\leq \\mu \\leq X_+] \\ \\alpha / 2 &amp;= \\mathbb{P}[X_- \\geq \\mu] \\end{aligned}$$ Making the inspired guess that \\(\\(\\frac{X_- - \\mu}{\\sigma} = \\frac{X - \\mu}{\\sigma} - b\\)\\) that is, that \\(X_- = X - \\sigma b\\), we find that \\(\\(\\begin{aligned}   \\alpha/2 &amp;= \\mathbb{P}[X - \\sigma b \\geq \\mu] \\\\   &amp;= \\mathbb{P}[\\frac{X - \\mu]{\\sigma} &gt; b}, \\end{aligned}\\)\\) which you can find that, for \\(\\alpha = 0.05\\), you get \\(b = 1.96\\).</p>"},{"location":"tests/#approx","title":"approx","text":"<p>In fact, rather than enumerating all \\(2^n\\) possibilities, the typical approach is to approximate the binomial distribution with the normal distribution. It turns out that, when \\(n\\) is not too small, that: \\(\\(\\mathrm{Binom}(x; n, p) \\approx \\mathcal{N}\\left(x; \\; \\mu = np, \\; \\sigma^2 = \\frac{p (1-p)}{n}\\right),\\)\\) in other words, you can use a normal distribution with mean a mean and variance computed from \\(n\\) and \\(p\\) to approximate the binomial distribution.</p> <p>FIGURE</p> <p>The binomial counting approach was intellectually elegant but mathematically inelegant. The normal approximation approach is mathematically elegant but intellectually confusing, as it makes the ingredients in the test very artificial:</p> <ul> <li>The observed data is not the sequence of heads and tails but rather   simply \\(x\\), which was previously the observed test statistics, that   is, the number of heads flipped.</li> <li>The universe of possible observations is all values between \\(0\\) and   \\(n\\). Of course, this isn't really true, as you can only observe   integer \\(x\\). But for the math to work out, you need to imagine that   you could have observed \\(x = 1.02\\), for example.</li> <li>The null hypothesis, as above, is some assertion about the probability   \\(p\\), only now \\(p\\) is not the probability of any particular event, it's   just a parameter in the normal distribution's mean and variance.</li> <li>The test statistic is just \\(x\\), the observed number.</li> </ul> <p>The other intellectually weird thing is that now, because the universe of possible observations is infinite, you cannot count up the states but must integrate over them: \\((\\text{``low-sided'' one-sided \\(p\\)-value} = \\int_{y=0}^x \\mathcal{N}\\left(y; np, \\frac{p(1-p)}{n} \\right) \\,\\mathrm{d}y\\)\\) This is where the mathematical elegance comes in: the integral of the normal distribution, which is its cumulative distribution function, is oft-calculated and so easy to look up. So now rather than summing over something like \\(2^n\\) states, we just look up a single number.</p> <p>The confidence intervals become similarly simple: I look for a lower bound \\(p_-\\) such that the probability that I would observe a value smaller than what I did observe is \\(\\alpha/2\\): \\(\\(\\alpha/2 = \\int_{y=0}^x \\mathcal{N} \\left(y; np_-, \\frac{p_-(1-p_-)}{n} \\right) \\,\\mathrm{d}y\\)\\)</p> <p>There are a few other approximations to the binomial distribution that are more accurate, but they all come down to the same logic: they exchange the intellectual simplicity of counting up over a finite universe of states for the mathematical elegance of integrating over a well-known function.</p> <p>point out how this works with a z-test?</p>"},{"location":"tests/#the-data-with-different-null-hypotheses-and-statistics-paired-tests","title":"The data with different null hypotheses and statistics: paired tests","text":"<p>So far we've had null hypotheses and test statistics that are obvious matches for the data that we're looking at. In the real world, when you're making up your own statistical tests, you'll find that you actually have multiple choices for null hypotheses and test statistics. To show how this can work, I'll analyze the same kind of data in a few ways.</p> <p>Charles Darwin wanted to know if cross-fertilized corn (Zea mays) grew better than self-fertilized corn. He took 15 pairs of plants, one self- and one cross-fertilized, each having germinated on the same day, and planted them together in one of 4 pots. He measured the heights of the plants, to one-eight of an inch, after some time (Table 1.1{reference-type=\"ref\" reference=\"table:darwin\"}).</p> <p>::: {#table:darwin} +----------+-----+---------------------------------------+ |          |     | Height (inches)                       | +==========+=====+===================+===================+ | 3-4 Pair | Pot | Crossed           | Self-fert.        | +----------+-----+-------------------+-------------------+ | 1        | 1   | \\(23 \\tfrac{1}{2}\\) | \\(17 \\tfrac{3}{8}\\) | +----------+-----+-------------------+-------------------+ | 2        | 1   | \\(12\\)              | \\(20 \\tfrac{3}{8}\\) | +----------+-----+-------------------+-------------------+ | 3        | 1   | \\(21\\)              | \\(20\\)              | +----------+-----+-------------------+-------------------+ | 4        | 2   | \\(22\\)              | \\(20\\)              | +----------+-----+-------------------+-------------------+ | 5        | 2   | \\(19 \\tfrac{1}{8}\\) | \\(18 \\tfrac{3}{8}\\) | +----------+-----+-------------------+-------------------+ | 6        | 2   | \\(21 \\tfrac{1}{2}\\) | \\(18 \\tfrac{5}{8}\\) | +----------+-----+-------------------+-------------------+ | 7        | 3   | \\(22 \\tfrac{1}{8}\\) | \\(18 \\tfrac{5}{8}\\) | +----------+-----+-------------------+-------------------+ | 8        | 3   | \\(20 \\tfrac{3}{8}\\) | \\(15 \\tfrac{1}{4}\\) | +----------+-----+-------------------+-------------------+ | 9        | 3   | \\(18 \\tfrac{1}{4}\\) | \\(16 \\tfrac{1}{2}\\) | +----------+-----+-------------------+-------------------+ | 10       | 3   | \\(21 \\tfrac{5}{8}\\) | \\(18\\)              | +----------+-----+-------------------+-------------------+ | 11       | 3   | \\(23 \\tfrac{1}{4}\\) | \\(16 \\tfrac{1}{4}\\) | +----------+-----+-------------------+-------------------+ | 12       | 4   | \\(21\\)              | \\(18\\)              | +----------+-----+-------------------+-------------------+ | 13       | 4   | \\(22 \\tfrac{1}{8}\\) | \\(12 \\tfrac{3}{4}\\) | +----------+-----+-------------------+-------------------+ | 14       | 4   | \\(23\\)              | \\(15 \\tfrac{1}{2}\\) | +----------+-----+-------------------+-------------------+ | 15       | 4   | \\(12\\)              | \\(18\\)              | +----------+-----+-------------------+-------------------+</p> <p>: Darwin's Zea mays data :::</p> <p>The obvious question is: did the hybridized plants grow faster than the self-fertilized plants?</p>"},{"location":"tests/#a-sign-test","title":"A sign test","text":"<p>This just reduces down to the binomial test above. Null hypothesis is that the median is zero.</p>"},{"location":"tests/#wilcoxons-signed-rank-test","title":"Wilcoxon's signed rank test","text":"<p>Null hypothesis is zero median and symmetric distribution. Much more complex test statistic.</p>"},{"location":"tests/#fishers-sum-test","title":"Fisher's sum test","text":"<p>Null is zero median and symmetric distribution. Simple test statistic.</p>"},{"location":"tests/#paired-t-test","title":"Paired \\(t\\)-test","text":""},{"location":"tests/#scotts","title":"Scott's","text":"<p>Something about permuting within the pots? Bootstrapping?</p>"},{"location":"tests/#parametric-inference","title":"Parametric inference","text":"<p>- z-test - t-test - Wilk's theorem? and likelihood-ratio tests - F-tests in general, variance. Then go to ANOVA</p>"},{"location":"tests/#non-parametric-inference","title":"Non-parametric inference","text":"<p>- Kolmogorov-Smirnov</p>"},{"location":"tests/#t-test","title":"\\(t\\)-test","text":""},{"location":"tests/#equal-variance","title":"Equal variance","text":"<p>The (old school) t-test is two sample, assuming equal variances. We're interested in the difference in the means between the two populations.</p> <p>The null hypothesis is that we're drawing \\(n_1 + n_2\\) samples from a population that has this equal variance, and that the labels on the two \"populations\" are just fictitious.</p> <p>Our estimator \\(s_p^2\\) for the pooled variance is just the average of the variances of the two \"populations\", weighted by \\(n_i - 1\\) (which is a better estimator than weighting by just \\(n_i\\)): \\(\\(s_p^2 = \\frac{(n_1 - 1) s_1^2 + (n_2 - 1) s_2^2}{n_1 + n_2 - 2}.\\)\\)</p> <p>To see why this is, say that we're going to develop some pooled estimator \\(\\hat{\\mathbb{V}}_{X,p}\\), which is a constant times the sum of squares: \\(\\(\\begin{aligned} \\hat{\\mathbb{V}}_{X,p} &amp;= C \\left[ \\sum_i (X_{1i} - \\overline{X}_1)^2 + \\ldots \\right] \\\\   &amp;= C \\left[ \\sigma^2 \\sum_i (Z_{1i} - \\overline{Z}_1^2 ) + \\ldots \\right] \\\\   &amp;= C \\sigma^2 \\left[ \\sum_i Z_{1i}^2 - n \\overline{Z}_1^2 + \\ldots \\right] \\\\ \\mathbb{E}[\\hat{\\mathbb{V]}_{X,p}} &amp;= C \\sigma^2 \\left[ \\sum_i \\mathbb{E}[Z_{1i]^2} - n \\mathbb{E}[\\overline{Z]_1^2} + \\ldots \\right] \\\\   &amp;= C \\sigma^2 \\left( n_1 - 1 + n_2 - 1 \\right) \\\\ \\end{aligned}\\)\\) which implies that \\(\\(C = \\frac{1}{n_1 + n_2 - 2}.\\)\\)</p> <p>To see the last bit, use this identity: $$\\begin{aligned} \\sum_i (Z_i - \\overline{Z})^2 &amp;= \\sum_i (Z_i^2 - 2 Z_i \\overline{Z} + \\overline{Z}^2) \\   &amp;= \\sum_i Z_i^2 - 2 n \\overline{Z} \\overline{Z} + n \\overline{Z}^2 \\   &amp;= \\sum_i Z_i^2 - n \\overline{Z}^2. \\end{aligned} $$</p> <p>The thing we're observing is the difference between the mean of \\(n_1\\) samples from a (potentially ficitious) variable \\(X_1\\) and \\(n_2\\) from \\(X_2\\): \\(\\(\\overline{X}_1 - \\overline{X}_2 = \\frac{1}{n_1} \\sum_{i=1}^{n_1} X_{1i} - \\frac{1}{n_2} \\sum_{i=1}^{n_2} X_{2i}.\\)\\) It would be nice if our statistic was distributed like \\(\\mathcal{N}(0, 1)\\), so we compute the variance of this observation:</p> \\[ \\begin{aligned} \\mathrm{Var}\\left[ \\overline{X}_1 - \\overline{X}_2 \\right]   &amp;= \\frac{1}{n_1^2} \\sum_i \\mathrm{Var}[X_1] + \\frac{1}{n_2^2} \\sum_i \\mathrm{Var}[X_2] \\\\   &amp;= \\frac{1}{n_1^2} n_1 s_p^2 + \\frac{1}{n_2^2} n_2 s_p^2 \\\\   &amp;= \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right) s_p^2. \\end{aligned} \\] <p>So the statistic for this test is just the observation over its variance: \\(\\(t = \\frac{\\overline{X}_1 - \\overline{X}_2}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}.\\)\\)</p> <p>The confusing thing is that \\(\\overline{X}_1\\), \\(\\overline{X}_2\\), and \\(s_p\\) are all random variables. We know how to take the sum (or difference) of two random variables (i.e., how to figure out the distribution of the numerator), but it's not immediately obvious how to find the distribution of the whole thing, which has a different random variable in its denominator.</p>"},{"location":"tests/#computational-approach","title":"Computational approach","text":"<ul> <li>Compute the observed \\(t\\) statistic</li> </ul> <ul> <li>Compute the observed sizes, means, and standard deviations for the two sample populations</li> </ul> <ul> <li>Many times, generate two sets of random variates. One set of variates is drawn from a normal distribution with the first sample mean and variance.</li> </ul> <ul> <li>For each iteration, compute the simulated \\(t\\) statistic.</li> </ul> <ul> <li>The empirical \\(p\\)-value is the fraction (not true! need \\(r+1/n+1\\)) of simulated statistics that are greater than the observed statistic.</li> </ul>"},{"location":"tests/#unequal-variance-welchs","title":"Unequal variance (Welch's)","text":"<p>This is the Behrens-Fisher problem. It stumped Fisher! He came up with a weird statistic with a weird distribution (Behrens-Fisher), but it didn't really stick, since he couldn't calculate confidence intervals (?).</p> <p>Instead, people went for the Welch-Satterthwaite equation, which approximates the interesting distribution using a more handy one by matching the first and second moments. (Maybe worth discussing those? Or just say mean and variance?)</p>"},{"location":"tests/#anova","title":"anova","text":"<p>Say you have some (equally-sized) groups. Each group was drawn from a normal distribution (all with the same variance). Are the data consistent with the model in which all those groups have the same mean?</p> <p>The statistic is \\(F \\equiv \\frac{\\mathrm{MS}_B}{\\mathrm{MS}_W}\\), where \\(MS_B\\) is the mean of the squares of the residuals(?) between the group means and the grand mean (\"between\") and \\(MS_W\\) is the mean of the squares of the residuals between the data points and the group means (\"within\").</p> <p>Again, focus on what's the population we're sampling from. It's easy to think about a finite population, where you can just do all the possible combinations and compare their \\(F\\) statistics. Then move on to say that, if you believe that the particular variances and means you measured are the exact, true distribution that you're sampling from, ask what happens when you sample from that infinite population.</p>"},{"location":"tests/#z-test-example","title":"z-test example","text":"<p>What's the nonparametric equivalent of this? It's just saying what the empirical cdf is! Then say, if you really truly believe your mean and standard deviation, then you can do that.</p> <p>In other words, you say you are absolutely sure what population you are drawing from. The same is actually true of the t-test, except that the z-test is asking about a single value, distributed like \\(N(\\mu, \\sigma^2)\\), while the t-test is about the mean of the \\(n\\) points, which is distributed like \\(N(\\mu, \\sigma^2/n)\\).</p>"},{"location":"tests/#paired-differences","title":"Paired differences","text":""},{"location":"tests/#historical-example-and-motivation","title":"Historical example and motivation","text":"<p>Darwin's thing with the pots, as described in Fisher's Design of Experiments</p> <p>We'll make a tower of the kinds of assumptions made to test Darwin's hypothesis.</p>"},{"location":"tests/#sign-test","title":"Sign test","text":"<p>Assume that it's meaningful if a hybrid plant is taller than a self-fertilized plant, but don't assign any meaning beyond that. Then the data are effectively dichotomized: you get some number of cases in which one is taller and some number of cases in which the other is taller.</p> <p>Better to say that we're sampling from any distribution that has zero median. You could even say you're sampling from all distributions. That's confusing, mathematically, because there are infinitely many distributions, and it's not obvious how you should sample from that functional space, but it works out, because all those distributions will have the same distribution of pluses and minuses.</p> <p>This is now just a binomial test.</p>"},{"location":"tests/#rank-test-mann-whitney-u","title":"Rank test (Mann-Whitney \\(U\\))","text":"<p>Assume that the ranks of the differences are meaningful.</p> <p>Now you're sampling from any distribution that is symmetric about zero. That means it has zero median also.</p>"},{"location":"tests/#fishers-weird-sum-test","title":"Fisher's weird sum test","text":"<p>Not sure if there's any name for this. Assume that the actual values of the differences are meaningful.</p>"},{"location":"tests/#welchs-t-test","title":"Welch's \\(t\\)-test","text":"<p>Assume that the two populations are normally distributed and, and therefore that that the variances of the populations are meaningful. Then you can infer where this set of differences would stand in an infinite set of such differences.</p> <p>For early statisticians, this was really appealing, mostly from a computational point of view: you could actually compute the mean and standard deviation with pen and paper in a reasonable amount of time, but you definitely couldn't do all \\(2^n\\) different ways of taking sums. Fisher does it for one example in his book, and I'm sure it was pretty crazy. He makes it clear that he went to great lengths to do it, and his conclusion is that the results are basically the same, so you should probably be doing the easier thing and not worry about it. Nowadays it's gotten pretty easy to do the other thing!, so it's</p>"},{"location":"tests/#wilcox-test-and-mann-whitney-test","title":"Wilcox test and Mann-Whitney test","text":"<p>Walsh averages and confidence intervals, from here</p> <p>There a few different names for these things:</p> <ul> <li>One-sample test: is this distribution symmetric about zero (or whatever)?</li> </ul> <ul> <li>Two-sample unpaired (independent; Mann-Whitney): does one of these distributions \"stochastically dominate\" the other (i.e., is it that a random value drawn from population \\(A\\) is more than 50% probable to be greater than a random value from \\(B\\))?</li> </ul> <ul> <li>Two-sample paired (dependent): are the differences between paired data points symmetric about zero?</li> </ul>"},{"location":"tests/#wilcoxon","title":"Wilcoxon","text":"<ol> <li> <p>For each pair \\(i\\), compute the magnitude and sign \\(s_i\\) of the difference. Exclude tied pairs.</p> </li> <li> <p>Order the pairs by the magnitude of their difference: \\(i=1\\) is the pair with the smallest magnitude. Now \\(i\\) is the rank.</p> </li> <li> <p>Compute the \\(W = \\sum_i i s_i\\).</p> </li> </ol> <p>Thus, the bigger differences get more weight.</p> <p>(There might be a way to do a visualization of this: as you walk along the data points, you get a good bump for every rank that is in the \"high\" data set, and you get a bad hit for every rank that is not. Then it settles out pretty quickly, and you want to know the meaning of the final intercept.)</p> <p>For small \\(W\\), the distribution has to be computed for each integer \\(W\\). For larger values (\\(\\geq 50\\)), a normal approximation works.</p> <p>Compare the sign test, which does not use ranks, and which assumes the median is zero, but not that the distributions are symmetric. That's just a binomial test of the number of pluses or minuses you get. It's like setting the weights, which in \\(W\\) are the ranks, all equal.</p>"},{"location":"tests/#mann-whitney","title":"Mann-Whitney","text":"<ol> <li> <p>Assign ranks to every observation.</p> </li> <li> <p>Compute \\(R_1\\), the sum ranks that belong to points for sample 1. Note that \\(R_1 + R_2 = \\sum_{i=1}^N = N(N+1)/2\\).</p> </li> <li> <p>Compute \\(U_1 = R_1 - n_1(n_1+1)/2\\) and \\(U_2\\). Use the smaller of \\(U_1\\) or \\(U_2\\) when looking at a table.</p> </li> </ol> <p>At minimum \\(U_1 = 0\\), which means that sample 1 had ranks \\(1,2,\\ldots,n_1\\). Note that \\(U_1 + U_2 = n_1 n_2\\).</p> <p>For large \\(U\\), there is a normal distribution approximation.</p>"},{"location":"tests/#generation","title":"Generation","text":"<p>Say you have \\(N\\) total points and \\(n_1\\) in sample 1. Find all the ways to draw \\(n_1\\) numbers from the sequence \\(1, 2, \\ldots N\\). Compute \\(U\\) for each of those. Voila.</p> <p>Note that, if you fix \\(n_1\\), then you don't have to subtract the \\(n_1(n_1+1)/2\\) to get the right \\(p\\)-value.</p>"},{"location":"tests/#statistical-power-cochrane-armitage-test","title":"Statistical power: Cochrane-Armitage test","text":"<p>[Do I need this example, if I'm showing the Zea mays choices instead?]{.mark}</p> <p>We never want to run just any test: we want to use the test that is most capable of distinguishing between the scenarios we're interested in. Usually this is a matter of choosing the test that has the right assumptions: the one-sample t-test is more powerful than the Wilcoxon test if the data come from a truly normally-distributed population.</p> <p>In other cases, you might have more flexibility. There's a somewhat obscure test that is, I think, a great illustration of this.</p> <p>Imagine that you have some data with a dichotomous outcome for some categorical predictor value. One classic example is drug dosing: you think that, as the dosage of the drug goes up, you have more good outcomes than bad outcomes. Did a greater proportion of people on board the Titanic survive as you go up from crew to Third Class to Second to First? Did the proportion of some kind of event increase over years? Technically, this means you have a \\(2 \\times k\\) table of counts, with two outcomes and \\(k\\) predictor categories.</p> <p>Outcome Dose 1 Dose 2 Dose 3</p> <p>Good 1 5 9 Bad 9 4 1</p> <p>You could use a \\(\\chi^2\\) test with equal expected frequencies across the columns. In other words, there might be more \"good\" than \"bad\" outcomes, but you don't expect that proportion to differ meaningfully across categories. You would pool the data across categories, use the observed proportion of good outcomes as you best guess of the true proportion \\(f\\), and compare the actual data with you expectation that a fraction \\(f\\) of the counts in each column are \"good\".</p> <p>In our examples, we think the data have some particular kind of pattern. The \\(\\chi^2\\) test doesn't look for any particular pattern; it just looks for any deviation from the null. The test statistic for the \\(\\chi^2\\) distribution is based around the sum of the square deviations from the expected values, usually written \\(\\sum_i (O_i - E_i)^2\\), with some stuff in the denominator to make the distribution of the statistic easier to work with. If the sum of the squared deviations is too large, then we have evidence that the observed values are not \"sticking to\" the expected frequencies.</p> <p>The trick I'm going to show you is to keep the same null hypothesis---that outcome doesn't depend on dose---but adjust the test so that it's more sensitive to particular kinds of dependencies.</p> <p>This is a fair approach because we're still just trying to say, \"OK, say you (the nameless antagonist) were right, and there really was no pattern in the data. Then I'm free to make up any test statistic, so long as, if you're right, we can show that the observed data were likely to have arisen by chance.\"</p> <p>To start constructing the test, think about each flip as a weighted binomial trial. We'll use these weights to adjust the test statistic to be more sensitive to what we suspect the true pattern in the data is, but we'll need to derive the distribution of the test statistic so that we can satisfy the nameless antagonist.</p> <p>Say each flip \\(y_i\\), which is in some category \\(x_i\\), gets some associated weight \\(w_i\\). A really simple statistic would be \\(\\sum_i w_i y_i\\), the sum of the weights of the \"successful\" trials. It would be nice to have this be zero-centered: \\(\\(\\sum_i \\left\\{ w_i y_i - \\mathbb{E}\\left[ w_i y_i \\right] \\right\\} = \\sum_i w_i (y_i - \\overline{p}),\\)\\) where \\(\\overline{p} = (1/N)\\sum_i y_i\\).</p> <p>It would also be nice for this to have variance 1, so we can divide by the square root of \\(\\(\\mathrm{Var}\\left[ \\sum_i w_i (y_i - \\overline{p}) \\right] = \\overline{p} (1-\\overline{p}) \\sum_i w_i^2\\)\\) to produce the statistic \\(\\(T = \\frac{\\sum_i w_i (y_i - \\overline{p})}{\\sqrt{\\overline{p} (1-\\overline{p}) \\sum_i w_i^2}}.\\)\\)</p> <p>You could also conceive of this being a table with two rows and some number of columns. We bin the trials by their weights: all trials with the same weight are in the same column. Successes go in the top row; failures in the bottom. Now write \\(t_c\\) as the weight of the trials in the \\(c\\)-th column, \\(n_{1c}\\) is the number of successful trials with weight \\(t_c\\) (i.e., in column \\(c\\)), and \\(n_{2c}\\) is the number of failures. Then some math shows that you can rewrite \\(T\\) as \\(\\(T = \\frac{\\sum_c w_c (n_{1c} n_{2\\bullet} - n_{2c} n_{1\\bullet})}{\\sqrt{(n_{1\\bullet} n_{2\\bullet} / n_{\\bullet\\bullet}^2) \\sum_c n_{\\bullet c} w_c^2}}\\)\\) where \\(n_{r\\bullet}\\) are the row margins, \\(n_{\\bullet c}\\) are the column margins, and \\(n_{\\bullet\\bullet}\\) is the total number of trials.</p> <p>N.B.: The wiki page gives a different answer, but I don't trust it, since the variance formula doesn't assume independence of the \\(y_i\\). A fact sheet about the PASS software that shows the formula in terms of the \\(y_i\\) seems to make a mistake by using \\(i\\) as an index both for individual trials and for the weight categories.</p> <p>The confusing thing here is how to pick the weights. This test is mostly used to look for linear trends: imagine that each \\(y_i\\) is associated with some \\(x_i\\), so that the weights would be \\(x_i\\) or \\(x_i - \\overline{x}\\). Why you pick these exact weights has to do with the sensitivity of the test. There could, of course, be a nonlinear trend, like a U-shape, that would lead to a zero expectation for this statistic. The \\(\\chi^2\\) test can find that, but Cochrane-Armitage with these weights cannot.</p> <p>To see why you use those weights for a linear test, imagine that \\(p_i \\propto x_i\\), and zero-center the \\(x_i\\) such that \\(p_i = m x_i + \\overline{p}\\).</p> <p>Then the question is what \\(w_i\\) maximize \\(\\mathbb{E}[T]\\)? You can quickly see that this is equivalent to maximizing \\(\\sum_i w_i x_i / \\sqrt{\\sum_i w_i^2}\\), and taking a derivative with respect to \\(w_j\\) shows that, at the extremum, \\(x_j \\sum_i w_i^2 = w_j \\sum_i w_i x_i\\), which \\(w_i = x_i\\) for all \\(i\\) satisfies. So those weights are the best way to get a large statistic if you think that there actually is a linear test.</p> <p>[^1]: I apologize that \\(p\\) means the expected proportion of heads and \\(p\\)-value means something totally different.</p>"}]}