
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../inference/">
      
      
        <link rel="next" href="../regression/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.3">
    
    
      
        <title>Tests - A short introduction to advanced statistics for scientists</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.d7758b05.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#frequentist-inferential-statistics-statistical-tests-and-confidence-intervals" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="A short introduction to advanced statistics for scientists" class="md-header__button md-logo" aria-label="A short introduction to advanced statistics for scientists" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            A short introduction to advanced statistics for scientists
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Tests
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/swo/stats-book" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="A short introduction to advanced statistics for scientists" class="md-nav__button md-logo" aria-label="A short introduction to advanced statistics for scientists" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    A short introduction to advanced statistics for scientists
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/swo/stats-book" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Probability
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Probability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../functions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../probability/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Probability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../random_variables/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Random variables
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Descriptive statistics
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Descriptive statistics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../statistics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Statistics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../estimators/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Estimators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../confidence_intervals/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Confidence intervals
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Inferential statistics
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Inferential statistics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../inference/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Statistical inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Tests
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Tests
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#the-ingredients-in-a-statistical-test" class="md-nav__link">
    <span class="md-ellipsis">
      The ingredients in a statistical test
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interpreting-p-values-and-confidence-intervals" class="md-nav__link">
    <span class="md-ellipsis">
      Interpreting \(p\)-values and confidence intervals
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a-test-with-a-simple-explanation-but-difficult-math-the-binomial-proportion-test" class="md-nav__link">
    <span class="md-ellipsis">
      A test with a simple explanation but difficult math: the binomial proportion test
    </span>
  </a>
  
    <nav class="md-nav" aria-label="A test with a simple explanation but difficult math: the binomial proportion test">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#one-and-two-sided-p-values" class="md-nav__link">
    <span class="md-ellipsis">
      One- and two-sided \(p\)-values
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#checking-for-a-specified-unfairness-of-a-coin" class="md-nav__link">
    <span class="md-ellipsis">
      Checking for a specified unfairness of a coin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Checking for a specified unfairness of a coin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#computing-confidence-intervals" class="md-nav__link">
    <span class="md-ellipsis">
      Computing confidence intervals
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a-test-with-simple-math-but-a-difficult-explanation-the-z-test" class="md-nav__link">
    <span class="md-ellipsis">
      A test with simple math but a difficult explanation: the \(z\)-test
    </span>
  </a>
  
    <nav class="md-nav" aria-label="A test with simple math but a difficult explanation: the \(z\)-test">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cis" class="md-nav__link">
    <span class="md-ellipsis">
      cis
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#approx" class="md-nav__link">
    <span class="md-ellipsis">
      approx
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-data-with-different-null-hypotheses-and-statistics-paired-tests" class="md-nav__link">
    <span class="md-ellipsis">
      The data with different null hypotheses and statistics: paired tests
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The data with different null hypotheses and statistics: paired tests">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-sign-test" class="md-nav__link">
    <span class="md-ellipsis">
      A sign test
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wilcoxons-signed-rank-test" class="md-nav__link">
    <span class="md-ellipsis">
      Wilcoxon's signed rank test
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fishers-sum-test" class="md-nav__link">
    <span class="md-ellipsis">
      Fisher's sum test
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paired-t-test" class="md-nav__link">
    <span class="md-ellipsis">
      Paired \(t\)-test
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scotts" class="md-nav__link">
    <span class="md-ellipsis">
      Scott's
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#parametric-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Parametric inference
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-parametric-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Non-parametric inference
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#t-test" class="md-nav__link">
    <span class="md-ellipsis">
      \(t\)-test
    </span>
  </a>
  
    <nav class="md-nav" aria-label="\(t\)-test">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#equal-variance" class="md-nav__link">
    <span class="md-ellipsis">
      Equal variance
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Equal variance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#computational-approach" class="md-nav__link">
    <span class="md-ellipsis">
      Computational approach
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unequal-variance-welchs" class="md-nav__link">
    <span class="md-ellipsis">
      Unequal variance (Welch's)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#anova" class="md-nav__link">
    <span class="md-ellipsis">
      anova
    </span>
  </a>
  
    <nav class="md-nav" aria-label="anova">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#z-test-example" class="md-nav__link">
    <span class="md-ellipsis">
      z-test example
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#paired-differences" class="md-nav__link">
    <span class="md-ellipsis">
      Paired differences
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Paired differences">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#historical-example-and-motivation" class="md-nav__link">
    <span class="md-ellipsis">
      Historical example and motivation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sign-test" class="md-nav__link">
    <span class="md-ellipsis">
      Sign test
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rank-test-mann-whitney-u" class="md-nav__link">
    <span class="md-ellipsis">
      Rank test (Mann-Whitney \(U\))
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fishers-weird-sum-test" class="md-nav__link">
    <span class="md-ellipsis">
      Fisher's weird sum test
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#welchs-t-test" class="md-nav__link">
    <span class="md-ellipsis">
      Welch's \(t\)-test
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#wilcox-test-and-mann-whitney-test" class="md-nav__link">
    <span class="md-ellipsis">
      Wilcox test and Mann-Whitney test
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Wilcox test and Mann-Whitney test">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#wilcoxon" class="md-nav__link">
    <span class="md-ellipsis">
      Wilcoxon
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mann-whitney" class="md-nav__link">
    <span class="md-ellipsis">
      Mann-Whitney
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mann-Whitney">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generation" class="md-nav__link">
    <span class="md-ellipsis">
      Generation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#statistical-power-cochrane-armitage-test" class="md-nav__link">
    <span class="md-ellipsis">
      Statistical power: Cochrane-Armitage test
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Regression
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../generating_functions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Generating functions
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../appendix/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Appendix
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#the-ingredients-in-a-statistical-test" class="md-nav__link">
    <span class="md-ellipsis">
      The ingredients in a statistical test
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interpreting-p-values-and-confidence-intervals" class="md-nav__link">
    <span class="md-ellipsis">
      Interpreting \(p\)-values and confidence intervals
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a-test-with-a-simple-explanation-but-difficult-math-the-binomial-proportion-test" class="md-nav__link">
    <span class="md-ellipsis">
      A test with a simple explanation but difficult math: the binomial proportion test
    </span>
  </a>
  
    <nav class="md-nav" aria-label="A test with a simple explanation but difficult math: the binomial proportion test">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#one-and-two-sided-p-values" class="md-nav__link">
    <span class="md-ellipsis">
      One- and two-sided \(p\)-values
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#checking-for-a-specified-unfairness-of-a-coin" class="md-nav__link">
    <span class="md-ellipsis">
      Checking for a specified unfairness of a coin
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Checking for a specified unfairness of a coin">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#computing-confidence-intervals" class="md-nav__link">
    <span class="md-ellipsis">
      Computing confidence intervals
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a-test-with-simple-math-but-a-difficult-explanation-the-z-test" class="md-nav__link">
    <span class="md-ellipsis">
      A test with simple math but a difficult explanation: the \(z\)-test
    </span>
  </a>
  
    <nav class="md-nav" aria-label="A test with simple math but a difficult explanation: the \(z\)-test">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cis" class="md-nav__link">
    <span class="md-ellipsis">
      cis
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#approx" class="md-nav__link">
    <span class="md-ellipsis">
      approx
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-data-with-different-null-hypotheses-and-statistics-paired-tests" class="md-nav__link">
    <span class="md-ellipsis">
      The data with different null hypotheses and statistics: paired tests
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The data with different null hypotheses and statistics: paired tests">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-sign-test" class="md-nav__link">
    <span class="md-ellipsis">
      A sign test
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wilcoxons-signed-rank-test" class="md-nav__link">
    <span class="md-ellipsis">
      Wilcoxon's signed rank test
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fishers-sum-test" class="md-nav__link">
    <span class="md-ellipsis">
      Fisher's sum test
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paired-t-test" class="md-nav__link">
    <span class="md-ellipsis">
      Paired \(t\)-test
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scotts" class="md-nav__link">
    <span class="md-ellipsis">
      Scott's
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#parametric-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Parametric inference
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-parametric-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Non-parametric inference
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#t-test" class="md-nav__link">
    <span class="md-ellipsis">
      \(t\)-test
    </span>
  </a>
  
    <nav class="md-nav" aria-label="\(t\)-test">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#equal-variance" class="md-nav__link">
    <span class="md-ellipsis">
      Equal variance
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Equal variance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#computational-approach" class="md-nav__link">
    <span class="md-ellipsis">
      Computational approach
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#unequal-variance-welchs" class="md-nav__link">
    <span class="md-ellipsis">
      Unequal variance (Welch's)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#anova" class="md-nav__link">
    <span class="md-ellipsis">
      anova
    </span>
  </a>
  
    <nav class="md-nav" aria-label="anova">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#z-test-example" class="md-nav__link">
    <span class="md-ellipsis">
      z-test example
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#paired-differences" class="md-nav__link">
    <span class="md-ellipsis">
      Paired differences
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Paired differences">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#historical-example-and-motivation" class="md-nav__link">
    <span class="md-ellipsis">
      Historical example and motivation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sign-test" class="md-nav__link">
    <span class="md-ellipsis">
      Sign test
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rank-test-mann-whitney-u" class="md-nav__link">
    <span class="md-ellipsis">
      Rank test (Mann-Whitney \(U\))
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fishers-weird-sum-test" class="md-nav__link">
    <span class="md-ellipsis">
      Fisher's weird sum test
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#welchs-t-test" class="md-nav__link">
    <span class="md-ellipsis">
      Welch's \(t\)-test
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#wilcox-test-and-mann-whitney-test" class="md-nav__link">
    <span class="md-ellipsis">
      Wilcox test and Mann-Whitney test
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Wilcox test and Mann-Whitney test">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#wilcoxon" class="md-nav__link">
    <span class="md-ellipsis">
      Wilcoxon
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mann-whitney" class="md-nav__link">
    <span class="md-ellipsis">
      Mann-Whitney
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mann-Whitney">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generation" class="md-nav__link">
    <span class="md-ellipsis">
      Generation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#statistical-power-cochrane-armitage-test" class="md-nav__link">
    <span class="md-ellipsis">
      Statistical power: Cochrane-Armitage test
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<p>Things to do:</p>
<p>Things to do:</p>
<ul>
<li>If you cannot construct the entire universe of possible observations (events?), then you are actually sampling from the <span class="arithmatex">\(p\)</span>-value distribution</li>
<li>You observe a specific statistic, and then you sample another <em>n</em>.</li>
<li>Under the null hypothesis, the observed value and the other <em>n</em> came from the same distribution.</li>
<li>The <em>p</em>-value is the probability of getting a value as small as the observed one, given that the null is true.</li>
<li>If you know the "shape" of the distribution, you can compute this exactly (e.g., as we did in our sign test).</li>
<li>If you <em>don't</em> know the shape, then you need to do this empirically, with a parametric test.</li>
<li>You say, I have <em>n+1</em> items in a list. What's the chance that this particular item falls at rank <em>m</em> or below (or above)? That's <span class="arithmatex">\((r+1)/(n+1)\)</span>.</li>
<li>It's a mistake to do <span class="arithmatex">\(r/n\)</span>, but not a terrible mistake, except that it gives you <span class="arithmatex">\(p=0\)</span>!</li>
<li>Overestimates small <span class="arithmatex">\(p\)</span> (because you go up to 1/n+1) and underestimates large, but that's normally not a problem. The important thing is avoiding p=0 bias.</li>
<li>Also, this provides an <em>estimate</em> of the <span class="arithmatex">\(p\)</span>-value, since you're basically doing a binomial test: of <span class="arithmatex">\(n+1\)</span> total coin flips, <span class="arithmatex">\(r+1\)</span> come out heads. What's the probability of heads? This has a confidence interval around it.</li>
</ul>
<h1 id="frequentist-inferential-statistics-statistical-tests-and-confidence-intervals">Frequentist inferential statistics: statistical tests and confidence intervals<a class="headerlink" href="#frequentist-inferential-statistics-statistical-tests-and-confidence-intervals" title="Permanent link">&para;</a></h1>
<p>[1. Define p-value (1 and 2-sided). 2. Zea mays example data. 3. Binomial sign test. 4. Fisher's weird sum. 5. Paired test (wilcox)?. 6. Paired t-test.]{.mark}</p>
<p>[Add a chapter for statistical power and sample size. Define Type I and Type II errors. Chapters on parametric vs. nonparametric. Permutation tests.]{.mark}</p>
<p><span class="arithmatex">\(T=t(X)\)</span> is statistic. <span class="arithmatex">\(t^\star = t(x)\)</span> is obs. value. <span class="arithmatex">\(p=\mathbb{P}[T&gt;t^\star]\)</span> or <span class="arithmatex">\(\mathbb{P}[T&lt;t^\star]\)</span>. 2-sided is <span class="arithmatex">\(\mathbb{P}[T&lt;t^\star \text{ or ] T &gt; t_0 + (t^\star-t_0)}\)</span>, where <span class="arithmatex">\(t_0\)</span> is some central value?</p>
<p>The previous chapter laid out the conceptual points of inference. In this chapter, I lay out the nuts and bolts of what most people think statistics is: tests.</p>
<p>The mathematics of statistical tests follows directly from the previous mathematical chapters: we define some <em>estimators</em>, functions of the data that predict some property of the data, and determine their variance. We just give things different names and interpret them differently.</p>
<p>But rather than begin with the mathematics of tests, I'll start with the concepts of how they work, and show how to implement them algorithmically and computationally. Once you understand what the test is doing, we can get back to unpacking some of the math you will hear bandied about around them.</p>
<h2 id="the-ingredients-in-a-statistical-test">The ingredients in a statistical test<a class="headerlink" href="#the-ingredients-in-a-statistical-test" title="Permanent link">&para;</a></h2>
<p>To run a statistical test, you need:</p>
<ol>
<li>Observed data.</li>
<li>A listing of the universe of observations that could have been made, of which the actual observation is one instance.</li>
<li>A <em>null hypothesis</em> that associates a probability with each possible observation.</li>
<li>A <em>test statistic</em> that summarizes how interesting the observed data are in a single number.</li>
</ol>
<p>Every statistical test that you know of can be defined by how it selects its universe of possible observations, its null hypothesis, and its test statistic.</p>
<p>The test will give you two outputs:</p>
<ol>
<li>A <span class="arithmatex">\(p\)</span>-value, which is the probability of observing data as extreme (or "interesting") as what you did observe, given the null hypothesis.</li>
<li>Confidence intervals on the test statistic.</li>
</ol>
<p>More concretely, the <span class="arithmatex">\(p\)</span>-value is the proportion of the observations in the universe of observations, weighted by their probabilities, that have test statistics that are more extreme (i.e., either larger or smaller) than the test statistic for the observed data. The confidence interval says that, using the observed data as our best estimates of the underlying truth, what range of test statistics would we expect?</p>
<h2 id="interpreting-p-values-and-confidence-intervals">Interpreting <span class="arithmatex">\(p\)</span>-values and confidence intervals<a class="headerlink" href="#interpreting-p-values-and-confidence-intervals" title="Permanent link">&para;</a></h2>
<p>The <span class="arithmatex">\(p\)</span>-value was developed by Fisher, probably the one person who had the most influence on modern statistics. He used the <span class="arithmatex">\(p\)</span>-value, a rigorously-derived number, to reason informally about data. If the <span class="arithmatex">\(p\)</span>-value too big, by some standard to be determined <em>ad hoc</em>, then the data didn't show anything interesting. In different books and papers, for different data sets, Fisher used different standards for "too big", but <span class="arithmatex">\(0.05\)</span> was a recurring choice.</p>
<p>Later, Neyman and Pearson (the younger) reformulated Fisher's method as a kind of decision-making under uncertainty. If you were a decision-maker, and you had to make a decision, what would you do when faced with certain data? They suggested you pick some specific <em>confidence level</em>, written <span class="arithmatex">\(\alpha\)</span>, and make your decision based on whether <span class="arithmatex">\(p\)</span> was greater than or less than <span class="arithmatex">\(\alpha\)</span>.</p>
<p>Although Fisher furiously disagreed with Neyman and Pearson about how to interpret <span class="arithmatex">\(p\)</span>-values, the canonical <span class="arithmatex">\(\alpha = 0.05\)</span> somehow made it into the scientific literature.</p>
<p>As laid out in the previous chapter, "what is the probability that this data is true, given the data?" is a very tricky question. The critical point is that the <span class="arithmatex">\(p\)</span>-value is, in the language of that chapter, <span class="arithmatex">\(\mathbb{P}[X | \theta]\)</span>, but we want to know about <span class="arithmatex">\(\mathbb{P}[\theta | X]\)</span>. The link between these is the mysterious prior probability <span class="arithmatex">\(\mathbb{P}[\theta]\)</span>.</p>
<p>I think my only take-away is that you should trust your gut more than a <span class="arithmatex">\(p\)</span>-value. First, Fisher developed statistics as a way to rigorously show that data <em>weren't</em> interesting, a mathematical counterbalance to humans' ability to find interesting patterns in data, especially their own. He meant <span class="arithmatex">\(p\)</span>-values as a way to deflate puffed-up conclusions, not to puff them up. Second, Fisher, who invented <span class="arithmatex">\(p\)</span>-values, couldn't really agree with other extremely intelligent people about exactly what they meant. So don't make yourself crazy trying to figure it out too precisely either! Just know that big <span class="arithmatex">\(p\)</span>-values, where "big" depends on the intellectual context, mean the data aren't interesting, where "interesting" is defined by the null hypothesis.</p>
<p>[Move all of the above to previous chapter where p-values are defined]{.mark}</p>
<h2 id="a-test-with-a-simple-explanation-but-difficult-math-the-binomial-proportion-test">A test with a simple explanation but difficult math: the binomial proportion test<a class="headerlink" href="#a-test-with-a-simple-explanation-but-difficult-math-the-binomial-proportion-test" title="Permanent link">&para;</a></h2>
<p>To understand how to go from a test's ingredients to its outputs, I will work through many examples of increasing complexity. The first and simplest is this: I flip a coin <span class="arithmatex">\(n\)</span> times, seeing some sequence of heads and tails, and I want to infer whether the coin is fair.</p>
<ul>
<li>The <em>observed data</em> is the sequence of heads and tails I flipped.</li>
<li>The universe of possible observations is all the sequences of heads and tails I could have flipped. So if <span class="arithmatex">\(n=2\)</span>, the universe is the four cases <span class="arithmatex">\(HH, HT, TH, TT\)</span>.</li>
<li>The <em>null hypothesis</em> is that heads and tails are equally likely.</li>
<li>The <em>test statistic</em> is the number of heads I flipped. This statistic, like all the other statistics discussed in chapter XX, is a function of the data: given some sequence of heads and tails, it just counts up the number of heads.</li>
</ul>
<h3 id="one-and-two-sided-p-values">One- and two-sided <span class="arithmatex">\(p\)</span>-values<a class="headerlink" href="#one-and-two-sided-p-values" title="Permanent link">&para;</a></h3>
<p>The <span class="arithmatex">\(p\)</span>-value is the probability of data as extreme as what we observed, or more extreme, given the null hypothesis that <span class="arithmatex">\(p = 0.5\)</span>.[^1] The tricky part of this definition is understand what "as extreme or more" means.</p>
<p>"Extremeness" is quantified by the test statistic, which summarizes each of the possible observations ---each length-<span class="arithmatex">\(n\)</span> sequence of heads and tails--- into a single number, the number of heads in that sequence. "As extreme or more" can mean 3 things:</p>
<ul>
<li>as small as what was observed, or smaller,</li>
<li>as large as what was observed, or larger, or</li>
<li>as far away from the "center" as was observed, or further away.</li>
</ul>
<p><strong><em>Exact tests: you can count every possibility for a binomial</em></strong></p>
<p>The first two cases call for <em>one-sided</em> <span class="arithmatex">\(p\)</span>-values and can be used if you have an <em>a priori</em> hypothesis that the number of heads that you will observe will be much higher or lower than what you would expect if the coin were fair. So if I want to seek support for my <em>a priori</em> hypothesis that the coin is more likely to land tails than heads, then I would use that one-sided <span class="arithmatex">\(p\)</span>-value. If I don't have an <em>a priori</em> expectation about whether the flips will be enriched for heads or tails ---if I just think that the coin is not fair, but I don't know which way--- then I need a more conservative, <em>two-sided</em> <span class="arithmatex">\(p\)</span>-value.</p>
<p>In this example, the null hypothesis of a fair coin means that all the possible sequences of heads and tails are equally likely, so to compute a one-sided <span class="arithmatex">\(p\)</span>-value, I just need to count up the number of sequences that are more extreme than the observed one: <span class="arithmatex">\(<span class="arithmatex">(\text{one-sided <span class="arithmatex">\(p\)</span>-value} = \frac{\text{number of possible sequences more extreme than observed}}{\text{total number of sequences}}\)</span>\)</span> You may know that the number of length-<span class="arithmatex">\(n\)</span> heads-tails sequences is <span class="arithmatex">\(2^n\)</span> and that the number of those sequences that has <span class="arithmatex">\(x\)</span> heads is <span class="arithmatex">\(\binom{n}{x} = n! /
\left[ x! (n-x)! \right]\)</span>. Thus, for this example, if we have a "low-sided" expectation about the number of heads: <span class="arithmatex">\(<span class="arithmatex">(\text{``low-sided'' one-sided <span class="arithmatex">\(p\)</span>-value} = \frac{\sum_{y=0}^x \binom{n}{x}}{2^n}\)</span>\)</span> A "high-sided" <span class="arithmatex">\(p\)</span>-value would sum from <span class="arithmatex">\(x\)</span> up to <span class="arithmatex">\(n\)</span>.</p>
<p>For the two-sided <span class="arithmatex">\(p\)</span>-value, we need to sum up the "low-side" and "high-side" <span class="arithmatex">\(p\)</span>-values. Because the observed <span class="arithmatex">\(x\)</span> falls to one side of the central expectation <span class="arithmatex">\(\mathbb{E}[x] = n/2\)</span>, we need to symmetrize to find the other value. For example if <span class="arithmatex">\(x\)</span> is low, we make another, mirrored "high" <span class="arithmatex">\(x_\mathrm{hi} = n-x\)</span>: <span class="arithmatex">\(<span class="arithmatex">(\text{two-sided <span class="arithmatex">\(p\)</span>-value} = \frac{\sum_{y=0}^x \binom{n}{x} + \sum_{y=n-x}^n \binom{n}{x}}{2^n}\)</span>\)</span> If <span class="arithmatex">\(x &gt; n/2\)</span>, then the sums would go from <span class="arithmatex">\(0\)</span> to <span class="arithmatex">\(n-x\)</span> and from <span class="arithmatex">\(x\)</span> to <span class="arithmatex">\(n\)</span>.</p>
<h2 id="checking-for-a-specified-unfairness-of-a-coin">Checking for a specified unfairness of a coin<a class="headerlink" href="#checking-for-a-specified-unfairness-of-a-coin" title="Permanent link">&para;</a></h2>
<p>In the first example, we used a null hypothesis that heads and tails were equal, which simplifed thinking about <span class="arithmatex">\(p\)</span>-values because we could simply count up the equiprobable sequences of heads and tails.</p>
<p>Now, we generalize slightly, allowing a null hypothesis that the probability <span class="arithmatex">\(p\)</span> of flipping heads is not exactly <span class="arithmatex">\(\tfrac{1}{2}\)</span>:</p>
<ul>
<li>The <em>observed data</em> is, again, the sequence of heads and tails I flipped.</li>
<li>The universe of possible observations is, again, all the sequences of heads and tails I could have flipped.</li>
<li>The <em>null hypothesis</em> is that the probability of flipping heads is <span class="arithmatex">\(p\)</span>.</li>
<li>The <em>test statistic</em> is the number of heads I flipped.</li>
</ul>
<p>The mathematically savvy will know that the probability of <span class="arithmatex">\(y\)</span> successful trials among <span class="arithmatex">\(n\)</span> total trials, with each trial having a probability <span class="arithmatex">\(p\)</span> of success, is the <em>binomial distribution</em>: <span class="arithmatex">\(<span class="arithmatex">\(\mathrm{Binom}(y; n, p) = \binom{n}{y} p^y (1-p)^{n-y}\)</span>\)</span> This is just like the above, where we counted the number of sequences, only now we need to weight them by the number of more (or less) probably heads (and tails): <span class="arithmatex">\(<span class="arithmatex">(\text{``low-sided'' one-sided <span class="arithmatex">\(p\)</span>-value} = \sum_{y=0}^x \mathrm{Binom}(y; n, p)\)</span>\)</span></p>
<h3 id="computing-confidence-intervals">Computing confidence intervals<a class="headerlink" href="#computing-confidence-intervals" title="Permanent link">&para;</a></h3>
<p>Recall that a <em>confidence interval</em> is a method: it is a pair of statistics, functions of the data, that will, in a certain proportion <span class="arithmatex">\(\alpha\)</span> of replicate experiments, will contain the true value, regardless of what the true value is.</p>
<p>In this example, the "true value" in question is the unknown probability <span class="arithmatex">\(p_\mathrm{true}\)</span> that the coin will flip heads, which would be anything between 0 and 1. So we need a method that, whether <span class="arithmatex">\(p_\mathrm{true} = 10^{-6}\)</span> or <span class="arithmatex">\(p_\mathrm{true} = 0.999\)</span>, will produce an interval that contains the true <span class="arithmatex">\(p_\mathrm{true}\)</span> in a proportion of <span class="arithmatex">\(\alpha\)</span> of the data that would arise in replicates of that experiment.</p>
<p>As mentioned earlier, there are multiple methods that can achieve this result. The one that doesn't rely on any kind of approximation, which we will address later, is the <em>Clopper-Pearson</em> interval. This says to find the largest value <span class="arithmatex">\(p_+\)</span> for which <span class="arithmatex">\(\sum_{y=0}^x \mathrm{Binom}(y; n, p_+) &gt; \alpha/2\)</span> and the smallest value <span class="arithmatex">\(p_-\)</span> for which <span class="arithmatex">\(\sum_{y=x}^n \mathrm{Binom}(y; n, p_-) &gt;
\alpha/2\)</span>. In other words, if <span class="arithmatex">\(\alpha=95\%\)</span>, pick an uppper limit on <span class="arithmatex">\(p_\mathrm{true}\)</span> so that, if that were the true value, in exactly <span class="arithmatex">\(2.5\%\)</span> of replicates of the experiment, you would get numbers of heads larger than what you observed. Similarly, pick a lower limit so that, if that were the true probability of flipping heads, in <span class="arithmatex">\(2.5\%\)</span> of replicates, you would get numbers of heads larger than what you observed.</p>
<p>If this seems like a big, confusing headache, I hope you take it as a sign that confidence intervals are confusing, not that you're not understanding!</p>
<p>[Now with computers, Clopper-Pearson is easy]{.mark}</p>
<h2 id="a-test-with-simple-math-but-a-difficult-explanation-the-z-test">A test with simple math but a difficult explanation: the <span class="arithmatex">\(z\)</span>-test<a class="headerlink" href="#a-test-with-simple-math-but-a-difficult-explanation-the-z-test" title="Permanent link">&para;</a></h2>
<p>The elegance of the previous example is that it works by simply enumerating all the possible sets of observations. The inelegant part is that there can be many: <span class="arithmatex">\(2^n\)</span> grows quickly with <span class="arithmatex">\(n\)</span>, making the sums in the <span class="arithmatex">\(p\)</span>-value calculations onerous. Furthermore, the math of the Clopper-Pearson interval is a bit of a mess and hard to articulate.</p>
<p>As a counterexample, consider the humble <span class="arithmatex">\(z\)</span>-test, in which your experiment consists of drawing a single number from a pre-specified population, specifically, a normal distribution with known mean <span class="arithmatex">\(\mu\)</span> and variance <span class="arithmatex">\(\sigma^2\)</span>:</p>
<ul>
<li>The observed data is the single number <span class="arithmatex">\(x\)</span> you drew.</li>
<li>The universe of possible observations is all numbers, from <span class="arithmatex">\(-\infty\)</span> to <span class="arithmatex">\(+\infty\)</span>.</li>
<li>The null hypothesis is that the number <span class="arithmatex">\(x\)</span> was drawn from a normal population with mean <span class="arithmatex">\(\mu\)</span> and variance <span class="arithmatex">\(\sigma^2\)</span>.</li>
<li>The test statistic is just the number <span class="arithmatex">\(x\)</span>.</li>
</ul>
<p>In this example, rather than summing up over the <em>discrete</em> universe of possibilities, you need to integrate to find the <span class="arithmatex">\(p\)</span>-value: <span class="arithmatex">\(<span class="arithmatex">(\text{``low-sided'' <span class="arithmatex">\(p\)</span>-value} = \int_{y=-\infty}^x f_\mathcal{N}(y; \mu, \sigma^2) \,\mathrm{d}y = F_\mathcal{N}(y; \mu, \sigma^2).\)</span>\)</span></p>
<h3 id="cis">cis<a class="headerlink" href="#cis" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
1 - \alpha &amp;= \mathbb{P}[X_- \leq \mu \leq X_+] \
\alpha / 2 &amp;= \mathbb{P}[X_- \geq \mu]
\end{aligned}$$ Making the inspired guess that
<span class="arithmatex">\(<span class="arithmatex">\(\frac{X_- - \mu}{\sigma} = \frac{X - \mu}{\sigma} - b\)</span>\)</span> that is, that
<span class="arithmatex">\(X_- = X - \sigma b\)</span>, we find that <span class="arithmatex">\(<span class="arithmatex">\(\begin{aligned}
  \alpha/2 &amp;= \mathbb{P}[X - \sigma b \geq \mu] \\
  &amp;= \mathbb{P}[\frac{X - \mu]{\sigma} &gt; b},
\end{aligned}\)</span>\)</span> which you can find that, for <span class="arithmatex">\(\alpha = 0.05\)</span>, you get
<span class="arithmatex">\(b = 1.96\)</span>.</p>
<h2 id="approx">approx<a class="headerlink" href="#approx" title="Permanent link">&para;</a></h2>
<p>In fact, rather than enumerating all <span class="arithmatex">\(2^n\)</span> possibilities, the typical
approach is to approximate the binomial distribution with the normal
distribution. It turns out that, when <span class="arithmatex">\(n\)</span> is not too small, that:
<span class="arithmatex">\(<span class="arithmatex">\(\mathrm{Binom}(x; n, p) \approx \mathcal{N}\left(x; \; \mu = np, \; \sigma^2 = \frac{p (1-p)}{n}\right),\)</span>\)</span>
in other words, you can use a normal distribution with mean a mean and
variance computed from <span class="arithmatex">\(n\)</span> and <span class="arithmatex">\(p\)</span> to approximate the binomial
distribution.</p>
<p><strong>FIGURE</strong></p>
<p>The binomial counting approach was intellectually elegant but
mathematically inelegant. The normal approximation approach is
mathematically elegant but intellectually confusing, as it makes the
ingredients in the test very artificial:</p>
<ul>
<li>The observed data is not the sequence of heads and tails but rather
  simply <span class="arithmatex">\(x\)</span>, which was previously the observed test statistics, that
  is, the number of heads flipped.</li>
<li>The universe of possible observations is all values between <span class="arithmatex">\(0\)</span> and
  <span class="arithmatex">\(n\)</span>. Of course, this isn't really true, as you can only observe
  integer <span class="arithmatex">\(x\)</span>. But for the math to work out, you need to imagine that
  you could have observed <span class="arithmatex">\(x = 1.02\)</span>, for example.</li>
<li>The null hypothesis, as above, is some assertion about the probability
  <span class="arithmatex">\(p\)</span>, only now <span class="arithmatex">\(p\)</span> is not the probability of any particular event, it's
  just a parameter in the normal distribution's mean and variance.</li>
<li>The test statistic is just <span class="arithmatex">\(x\)</span>, the observed number.</li>
</ul>
<p>The other intellectually weird thing is that now, because the universe
of possible observations is infinite, you cannot count up the states but
must integrate over them:
<span class="arithmatex">\(<span class="arithmatex">(\text{``low-sided'' one-sided <span class="arithmatex">\(p\)</span>-value} = \int_{y=0}^x \mathcal{N}\left(y; np, \frac{p(1-p)}{n} \right) \,\mathrm{d}y\)</span>\)</span>
This is where the mathematical elegance comes in: the integral of the
normal distribution, which is its cumulative distribution function, is
oft-calculated and so easy to look up. So now rather than summing over
something like <span class="arithmatex">\(2^n\)</span> states, we just look up a single number.</p>
<p>The confidence intervals become similarly simple: I look for a lower
bound <span class="arithmatex">\(p_-\)</span> such that the probability that I would observe a value
smaller than what I did observe is <span class="arithmatex">\(\alpha/2\)</span>:
<span class="arithmatex">\(<span class="arithmatex">\(\alpha/2 = \int_{y=0}^x \mathcal{N} \left(y; np_-, \frac{p_-(1-p_-)}{n} \right) \,\mathrm{d}y\)</span>\)</span></p>
<p>There are a few other approximations to the binomial distribution that
are more accurate, but they all come down to the same logic: they
exchange the intellectual simplicity of counting up over a finite
universe of states for the mathematical elegance of integrating over a
well-known function.</p>
<p><strong>point out how this works with a z-test?</strong></p>
<h2 id="the-data-with-different-null-hypotheses-and-statistics-paired-tests">The data with different null hypotheses and statistics: paired tests<a class="headerlink" href="#the-data-with-different-null-hypotheses-and-statistics-paired-tests" title="Permanent link">&para;</a></h2>
<p>So far we've had null hypotheses and test statistics that are obvious
matches for the data that we're looking at. In the real world, when
you're making up your own statistical tests, you'll find that you
actually have multiple choices for null hypotheses and test statistics.
To show how this can work, I'll analyze the same kind of data in a few
ways.</p>
<p>Charles Darwin wanted to know if cross-fertilized corn (<em>Zea mays</em>) grew
better than self-fertilized corn. He took 15 pairs of plants, one self-
and one cross-fertilized, each having germinated on the same day, and
planted them together in one of 4 pots. He measured the heights of the
plants, to one-eight of an inch, after some time (Table
<a href="#table:darwin">1.1</a>{reference-type="ref" reference="table:darwin"}).</p>
<p>::: {#table:darwin}
+----------+-----+---------------------------------------+
|          |     | Height (inches)                       |
+==========+=====+===================+===================+
| 3-4 Pair | Pot | Crossed           | Self-fert.        |
+----------+-----+-------------------+-------------------+
| 1        | 1   | <span class="arithmatex">\(23 \tfrac{1}{2}\)</span> | <span class="arithmatex">\(17 \tfrac{3}{8}\)</span> |
+----------+-----+-------------------+-------------------+
| 2        | 1   | <span class="arithmatex">\(12\)</span>              | <span class="arithmatex">\(20 \tfrac{3}{8}\)</span> |
+----------+-----+-------------------+-------------------+
| 3        | 1   | <span class="arithmatex">\(21\)</span>              | <span class="arithmatex">\(20\)</span>              |
+----------+-----+-------------------+-------------------+
| 4        | 2   | <span class="arithmatex">\(22\)</span>              | <span class="arithmatex">\(20\)</span>              |
+----------+-----+-------------------+-------------------+
| 5        | 2   | <span class="arithmatex">\(19 \tfrac{1}{8}\)</span> | <span class="arithmatex">\(18 \tfrac{3}{8}\)</span> |
+----------+-----+-------------------+-------------------+
| 6        | 2   | <span class="arithmatex">\(21 \tfrac{1}{2}\)</span> | <span class="arithmatex">\(18 \tfrac{5}{8}\)</span> |
+----------+-----+-------------------+-------------------+
| 7        | 3   | <span class="arithmatex">\(22 \tfrac{1}{8}\)</span> | <span class="arithmatex">\(18 \tfrac{5}{8}\)</span> |
+----------+-----+-------------------+-------------------+
| 8        | 3   | <span class="arithmatex">\(20 \tfrac{3}{8}\)</span> | <span class="arithmatex">\(15 \tfrac{1}{4}\)</span> |
+----------+-----+-------------------+-------------------+
| 9        | 3   | <span class="arithmatex">\(18 \tfrac{1}{4}\)</span> | <span class="arithmatex">\(16 \tfrac{1}{2}\)</span> |
+----------+-----+-------------------+-------------------+
| 10       | 3   | <span class="arithmatex">\(21 \tfrac{5}{8}\)</span> | <span class="arithmatex">\(18\)</span>              |
+----------+-----+-------------------+-------------------+
| 11       | 3   | <span class="arithmatex">\(23 \tfrac{1}{4}\)</span> | <span class="arithmatex">\(16 \tfrac{1}{4}\)</span> |
+----------+-----+-------------------+-------------------+
| 12       | 4   | <span class="arithmatex">\(21\)</span>              | <span class="arithmatex">\(18\)</span>              |
+----------+-----+-------------------+-------------------+
| 13       | 4   | <span class="arithmatex">\(22 \tfrac{1}{8}\)</span> | <span class="arithmatex">\(12 \tfrac{3}{4}\)</span> |
+----------+-----+-------------------+-------------------+
| 14       | 4   | <span class="arithmatex">\(23\)</span>              | <span class="arithmatex">\(15 \tfrac{1}{2}\)</span> |
+----------+-----+-------------------+-------------------+
| 15       | 4   | <span class="arithmatex">\(12\)</span>              | <span class="arithmatex">\(18\)</span>              |
+----------+-----+-------------------+-------------------+</p>
<p>: Darwin's <em>Zea mays</em> data
:::</p>
<p>The obvious question is: did the hybridized plants grow faster than the
self-fertilized plants?</p>
<h3 id="a-sign-test">A sign test<a class="headerlink" href="#a-sign-test" title="Permanent link">&para;</a></h3>
<p>This just reduces down to the binomial test above. Null hypothesis is
that the median is zero.</p>
<h3 id="wilcoxons-signed-rank-test">Wilcoxon's signed rank test<a class="headerlink" href="#wilcoxons-signed-rank-test" title="Permanent link">&para;</a></h3>
<p>Null hypothesis is zero median and symmetric distribution. Much more
complex test statistic.</p>
<h3 id="fishers-sum-test">Fisher's sum test<a class="headerlink" href="#fishers-sum-test" title="Permanent link">&para;</a></h3>
<p>Null is zero median and symmetric distribution. Simple test statistic.</p>
<h3 id="paired-t-test">Paired <span class="arithmatex">\(t\)</span>-test<a class="headerlink" href="#paired-t-test" title="Permanent link">&para;</a></h3>
<h3 id="scotts">Scott's<a class="headerlink" href="#scotts" title="Permanent link">&para;</a></h3>
<p>Something about permuting within the pots? Bootstrapping?</p>
<h2 id="parametric-inference">Parametric inference<a class="headerlink" href="#parametric-inference" title="Permanent link">&para;</a></h2>
<p>- z-test - t-test - Wilk's theorem? and likelihood-ratio tests -
F-tests in general, variance. Then go to ANOVA</p>
<h2 id="non-parametric-inference">Non-parametric inference<a class="headerlink" href="#non-parametric-inference" title="Permanent link">&para;</a></h2>
<p>- Kolmogorov-Smirnov</p>
<h2 id="t-test"><span class="arithmatex">\(t\)</span>-test<a class="headerlink" href="#t-test" title="Permanent link">&para;</a></h2>
<h3 id="equal-variance">Equal variance<a class="headerlink" href="#equal-variance" title="Permanent link">&para;</a></h3>
<p>The (old school) <em>t</em>-test is two sample, assuming equal variances. We're
interested in the difference in the means between the two populations.</p>
<p>The null hypothesis is that we're drawing <span class="arithmatex">\(n_1 + n_2\)</span> samples from a
population that has this equal variance, and that the labels on the two
"populations" are just fictitious.</p>
<p>Our estimator <span class="arithmatex">\(s_p^2\)</span> for the pooled variance is just the average of the
variances of the two "populations", weighted by <span class="arithmatex">\(n_i - 1\)</span> (which is a
better estimator than weighting by just <span class="arithmatex">\(n_i\)</span>):
<span class="arithmatex">\(<span class="arithmatex">\(s_p^2 = \frac{(n_1 - 1) s_1^2 + (n_2 - 1) s_2^2}{n_1 + n_2 - 2}.\)</span>\)</span></p>
<p>To see why this is, say that we're going to develop some pooled
estimator <span class="arithmatex">\(\hat{\mathbb{V}}_{X,p}\)</span>, which is a constant times the sum of
squares: <span class="arithmatex">\(<span class="arithmatex">\(\begin{aligned}
\hat{\mathbb{V}}_{X,p} &amp;= C \left[ \sum_i (X_{1i} - \overline{X}_1)^2 + \ldots \right] \\
  &amp;= C \left[ \sigma^2 \sum_i (Z_{1i} - \overline{Z}_1^2 ) + \ldots \right] \\
  &amp;= C \sigma^2 \left[ \sum_i Z_{1i}^2 - n \overline{Z}_1^2 + \ldots \right] \\
\mathbb{E}[\hat{\mathbb{V]}_{X,p}} &amp;= C \sigma^2 \left[ \sum_i \mathbb{E}[Z_{1i]^2} - n \mathbb{E}[\overline{Z]_1^2} + \ldots \right] \\
  &amp;= C \sigma^2 \left( n_1 - 1 + n_2 - 1 \right) \\
\end{aligned}\)</span>\)</span> which implies that <span class="arithmatex">\(<span class="arithmatex">\(C = \frac{1}{n_1 + n_2 - 2}.\)</span>\)</span></p>
<p>To see the last bit, use this identity: $$\begin{aligned}
\sum_i (Z_i - \overline{Z})^2 &amp;= \sum_i (Z_i^2 - 2 Z_i \overline{Z} + \overline{Z}^2) \
  &amp;= \sum_i Z_i^2 - 2 n \overline{Z} \overline{Z} + n \overline{Z}^2 \
  &amp;= \sum_i Z_i^2 - n \overline{Z}^2.
\end{aligned}
$$</p>
<p>The thing we're observing is the difference between the mean of <span class="arithmatex">\(n_1\)</span> samples from a (potentially ficitious) variable <span class="arithmatex">\(X_1\)</span> and <span class="arithmatex">\(n_2\)</span> from <span class="arithmatex">\(X_2\)</span>: <span class="arithmatex">\(<span class="arithmatex">\(\overline{X}_1 - \overline{X}_2 = \frac{1}{n_1} \sum_{i=1}^{n_1} X_{1i} - \frac{1}{n_2} \sum_{i=1}^{n_2} X_{2i}.\)</span>\)</span> It would be nice if our statistic was distributed like <span class="arithmatex">\(\mathcal{N}(0, 1)\)</span>, so we compute the variance of this observation:</p>
<div class="arithmatex">\[
\begin{aligned}
\mathrm{Var}\left[ \overline{X}_1 - \overline{X}_2 \right]
  &amp;= \frac{1}{n_1^2} \sum_i \mathrm{Var}[X_1] + \frac{1}{n_2^2} \sum_i \mathrm{Var}[X_2] \\
  &amp;= \frac{1}{n_1^2} n_1 s_p^2 + \frac{1}{n_2^2} n_2 s_p^2 \\
  &amp;= \left( \frac{1}{n_1} + \frac{1}{n_2} \right) s_p^2.
\end{aligned}
\]</div>
<p>So the statistic for this test is just the observation over its variance: <span class="arithmatex">\(<span class="arithmatex">\(t = \frac{\overline{X}_1 - \overline{X}_2}{s_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}.\)</span>\)</span></p>
<p>The confusing thing is that <span class="arithmatex">\(\overline{X}_1\)</span>, <span class="arithmatex">\(\overline{X}_2\)</span>, and <span class="arithmatex">\(s_p\)</span> are all random variables. We know how to take the sum (or difference) of two random variables (i.e., how to figure out the distribution of the numerator), but it's not immediately obvious how to find the distribution of the whole thing, which has a different random variable in its denominator.</p>
<h4 id="computational-approach">Computational approach<a class="headerlink" href="#computational-approach" title="Permanent link">&para;</a></h4>
<ul>
<li>Compute the observed <span class="arithmatex">\(t\)</span> statistic</li>
</ul>
<ul>
<li>Compute the observed sizes, means, and standard deviations for the two sample populations</li>
</ul>
<ul>
<li>Many times, generate two sets of random variates. One set of variates is drawn from a normal distribution with the first sample mean and variance.</li>
</ul>
<ul>
<li>For each iteration, compute the simulated <span class="arithmatex">\(t\)</span> statistic.</li>
</ul>
<ul>
<li>The empirical <span class="arithmatex">\(p\)</span>-value is the fraction (not true! need <span class="arithmatex">\(r+1/n+1\)</span>) of simulated statistics that are greater than the observed statistic.</li>
</ul>
<h3 id="unequal-variance-welchs">Unequal variance (Welch's)<a class="headerlink" href="#unequal-variance-welchs" title="Permanent link">&para;</a></h3>
<p>This is the Behrens-Fisher problem. It stumped Fisher! He came up with a weird statistic with a weird distribution (Behrens-Fisher), but it didn't really stick, since he couldn't calculate confidence intervals (?).</p>
<p>Instead, people went for the Welch-Satterthwaite equation, which approximates the interesting distribution using a more handy one by matching the first and second moments. (Maybe worth discussing those? Or just say mean and variance?)</p>
<h2 id="anova">anova<a class="headerlink" href="#anova" title="Permanent link">&para;</a></h2>
<p>Say you have some (equally-sized) groups. Each group was drawn from a normal distribution (all with the same variance). Are the data consistent with the model in which all those groups have the same mean?</p>
<p>The statistic is <span class="arithmatex">\(F \equiv \frac{\mathrm{MS}_B}{\mathrm{MS}_W}\)</span>, where <span class="arithmatex">\(MS_B\)</span> is the mean of the squares of the residuals(?) between the group means and the grand mean ("between") and <span class="arithmatex">\(MS_W\)</span> is the mean of the squares of the residuals between the data points and the group means ("within").</p>
<p>Again, focus on what's the population we're sampling from. It's easy to think about a finite population, where you can just do all the possible combinations and compare their <span class="arithmatex">\(F\)</span> statistics. Then move on to say that, if you believe that the particular variances and means you measured are the exact, true distribution that you're sampling from, ask what happens when you sample from that infinite population.</p>
<h3 id="z-test-example">z-test example<a class="headerlink" href="#z-test-example" title="Permanent link">&para;</a></h3>
<p>What's the nonparametric equivalent of this? It's just saying what the empirical cdf is! Then say, if you really truly believe your mean and standard deviation, then you can do that.</p>
<p>In other words, you say you are absolutely sure what population you are drawing from. The same is actually true of the <em>t</em>-test, except that the <em>z</em>-test is asking about a single value, distributed like <span class="arithmatex">\(N(\mu, \sigma^2)\)</span>, while the <em>t</em>-test is about the mean of the <span class="arithmatex">\(n\)</span> points, which is distributed like <span class="arithmatex">\(N(\mu, \sigma^2/n)\)</span>.</p>
<h2 id="paired-differences">Paired differences<a class="headerlink" href="#paired-differences" title="Permanent link">&para;</a></h2>
<h3 id="historical-example-and-motivation">Historical example and motivation<a class="headerlink" href="#historical-example-and-motivation" title="Permanent link">&para;</a></h3>
<p>Darwin's thing with the pots, as described in Fisher's <em>Design of Experiments</em></p>
<p>We'll make a tower of the kinds of assumptions made to test Darwin's hypothesis.</p>
<h3 id="sign-test">Sign test<a class="headerlink" href="#sign-test" title="Permanent link">&para;</a></h3>
<p>Assume that it's meaningful if a hybrid plant is taller than a self-fertilized plant, but don't assign any meaning beyond that. Then the data are effectively dichotomized: you get some number of cases in which one is taller and some number of cases in which the other is taller.</p>
<p>Better to say that we're sampling from any distribution that has zero median. You could even say you're sampling from <em>all</em> distributions. That's confusing, mathematically, because there are infinitely many distributions, and it's not obvious how you should sample from that functional space, but it works out, because all those distributions will have the same distribution of pluses and minuses.</p>
<p>This is now just a binomial test.</p>
<h3 id="rank-test-mann-whitney-u">Rank test (Mann-Whitney <span class="arithmatex">\(U\)</span>)<a class="headerlink" href="#rank-test-mann-whitney-u" title="Permanent link">&para;</a></h3>
<p>Assume that the <em>ranks</em> of the differences are meaningful.</p>
<p>Now you're sampling from any distribution that is symmetric about zero. That means it has zero median also.</p>
<h3 id="fishers-weird-sum-test">Fisher's weird sum test<a class="headerlink" href="#fishers-weird-sum-test" title="Permanent link">&para;</a></h3>
<p>Not sure if there's any name for this. Assume that the actual values of the differences are meaningful.</p>
<h3 id="welchs-t-test">Welch's <span class="arithmatex">\(t\)</span>-test<a class="headerlink" href="#welchs-t-test" title="Permanent link">&para;</a></h3>
<p>Assume that the two populations are normally distributed and, and therefore that that the variances of the populations are meaningful. Then you can infer where this set of differences would stand in an infinite set of such differences.</p>
<p>For early statisticians, this was really appealing, mostly from a computational point of view: you could actually compute the mean and standard deviation with pen and paper in a reasonable amount of time, but you definitely couldn't do all <span class="arithmatex">\(2^n\)</span> different ways of taking sums. Fisher does it for one example in his book, and I'm sure it was pretty crazy. He makes it clear that he went to great lengths to do it, and his conclusion is that the results are basically the same, so you should probably be doing the easier thing and not worry about it. Nowadays it's gotten pretty easy to do the other thing!, so it's</p>
<h2 id="wilcox-test-and-mann-whitney-test">Wilcox test and Mann-Whitney test<a class="headerlink" href="#wilcox-test-and-mann-whitney-test" title="Permanent link">&para;</a></h2>
<p><strong>Walsh averages and confidence intervals</strong>, from <a href="http://www.stat.umn.edu/geyer/old03/5102/notes/rank.pdf">here</a></p>
<p>There a few different names for these things:</p>
<ul>
<li>One-sample test: is this distribution symmetric about zero (or whatever)?</li>
</ul>
<ul>
<li>Two-sample unpaired (independent; Mann-Whitney): does one of these distributions "stochastically dominate" the other (i.e., is it that a random value drawn from population <span class="arithmatex">\(A\)</span> is more than 50% probable to be greater than a random value from <span class="arithmatex">\(B\)</span>)?</li>
</ul>
<ul>
<li>Two-sample paired (dependent): are the differences between paired data points symmetric about zero?</li>
</ul>
<h3 id="wilcoxon">Wilcoxon<a class="headerlink" href="#wilcoxon" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p>For each pair <span class="arithmatex">\(i\)</span>, compute the magnitude and sign <span class="arithmatex">\(s_i\)</span> of the difference. Exclude tied pairs.</p>
</li>
<li>
<p>Order the pairs by the magnitude of their difference: <span class="arithmatex">\(i=1\)</span> is the pair with the smallest magnitude. Now <span class="arithmatex">\(i\)</span> is the rank.</p>
</li>
<li>
<p>Compute the <span class="arithmatex">\(W = \sum_i i s_i\)</span>.</p>
</li>
</ol>
<p>Thus, the bigger differences get more weight.</p>
<p>(There might be a way to do a visualization of this: as you walk along the data points, you get a good bump for every rank that is in the "high" data set, and you get a bad hit for every rank that is not. Then it settles out pretty quickly, and you want to know the meaning of the final intercept.)</p>
<p>For small <span class="arithmatex">\(W\)</span>, the distribution has to be computed for each integer <span class="arithmatex">\(W\)</span>. For larger values (<span class="arithmatex">\(\geq 50\)</span>), a normal approximation works.</p>
<p>Compare the sign test, which does not use ranks, and which assumes the median is zero, but not that the distributions are symmetric. That's just a binomial test of the number of pluses or minuses you get. It's like setting the weights, which in <span class="arithmatex">\(W\)</span> are the ranks, all equal.</p>
<h3 id="mann-whitney">Mann-Whitney<a class="headerlink" href="#mann-whitney" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p>Assign ranks to every observation.</p>
</li>
<li>
<p>Compute <span class="arithmatex">\(R_1\)</span>, the sum ranks that belong to points for sample 1. Note that <span class="arithmatex">\(R_1 + R_2 = \sum_{i=1}^N = N(N+1)/2\)</span>.</p>
</li>
<li>
<p>Compute <span class="arithmatex">\(U_1 = R_1 - n_1(n_1+1)/2\)</span> and <span class="arithmatex">\(U_2\)</span>. Use the smaller of <span class="arithmatex">\(U_1\)</span> or <span class="arithmatex">\(U_2\)</span> when looking at a table.</p>
</li>
</ol>
<p>At minimum <span class="arithmatex">\(U_1 = 0\)</span>, which means that sample 1 had ranks <span class="arithmatex">\(1,2,\ldots,n_1\)</span>. Note that <span class="arithmatex">\(U_1 + U_2 = n_1 n_2\)</span>.</p>
<p>For large <span class="arithmatex">\(U\)</span>, there is a normal distribution approximation.</p>
<h4 id="generation">Generation<a class="headerlink" href="#generation" title="Permanent link">&para;</a></h4>
<p>Say you have <span class="arithmatex">\(N\)</span> total points and <span class="arithmatex">\(n_1\)</span> in sample 1. Find all the ways to draw <span class="arithmatex">\(n_1\)</span> numbers from the sequence <span class="arithmatex">\(1, 2, \ldots N\)</span>. Compute <span class="arithmatex">\(U\)</span> for each of those. Voila.</p>
<p>Note that, if you fix <span class="arithmatex">\(n_1\)</span>, then you don't have to subtract the <span class="arithmatex">\(n_1(n_1+1)/2\)</span> to get the right <span class="arithmatex">\(p\)</span>-value.</p>
<h2 id="statistical-power-cochrane-armitage-test">Statistical power: Cochrane-Armitage test<a class="headerlink" href="#statistical-power-cochrane-armitage-test" title="Permanent link">&para;</a></h2>
<p>[Do I need this example, if I'm showing the Zea mays choices instead?]{.mark}</p>
<p>We never want to run just any test: we want to use the test that is most capable of distinguishing between the scenarios we're interested in. Usually this is a matter of choosing the test that has the right assumptions: the one-sample <em>t</em>-test is more powerful than the Wilcoxon test if the data come from a truly normally-distributed population.</p>
<p>In other cases, you might have more flexibility. There's a somewhat obscure test that is, I think, a great illustration of this.</p>
<p>Imagine that you have some data with a dichotomous outcome for some categorical predictor value. One classic example is drug dosing: you think that, as the dosage of the drug goes up, you have more good outcomes than bad outcomes. Did a greater proportion of people on board the Titanic survive as you go up from crew to Third Class to Second to First? Did the proportion of some kind of event increase over years? Technically, this means you have a <span class="arithmatex">\(2 \times k\)</span> table of counts, with two outcomes and <span class="arithmatex">\(k\)</span> predictor categories.</p>
<p>Outcome Dose 1 Dose 2 Dose 3</p>
<hr />
<p>Good 1 5 9 Bad 9 4 1</p>
<p>You could use a <span class="arithmatex">\(\chi^2\)</span> test with equal expected frequencies across the columns. In other words, there might be more "good" than "bad" outcomes, but you don't expect that proportion to differ meaningfully across categories. You would pool the data across categories, use the observed proportion of good outcomes as you best guess of the true proportion <span class="arithmatex">\(f\)</span>, and compare the actual data with you expectation that a fraction <span class="arithmatex">\(f\)</span> of the counts in each column are "good".</p>
<p>In our examples, we think the data have some <em>particular</em> kind of pattern. The <span class="arithmatex">\(\chi^2\)</span> test doesn't look for any particular pattern; it just looks for any deviation from the null. The test statistic for the <span class="arithmatex">\(\chi^2\)</span> distribution is based around the sum of the square deviations from the expected values, usually written <span class="arithmatex">\(\sum_i (O_i - E_i)^2\)</span>, with some stuff in the denominator to make the distribution of the statistic easier to work with. If the sum of the squared deviations is too large, then we have evidence that the observed values are not "sticking to" the expected frequencies.</p>
<p>The trick I'm going to show you is to keep the same null hypothesis---that outcome doesn't depend on dose---but adjust the test so that it's more sensitive to particular kinds of dependencies.</p>
<p>This is a fair approach because we're still just trying to say, "OK, say you (the nameless antagonist) were right, and there really was no pattern in the data. Then I'm free to make up any test statistic, so long as, if you're right, we can show that the observed data were likely to have arisen by chance."</p>
<p>To start constructing the test, think about each flip as a weighted binomial trial. We'll use these weights to adjust the test statistic to be more sensitive to what we suspect the true pattern in the data is, but we'll need to derive the distribution of the test statistic so that we can satisfy the nameless antagonist.</p>
<p>Say each flip <span class="arithmatex">\(y_i\)</span>, which is in some category <span class="arithmatex">\(x_i\)</span>, gets some associated weight <span class="arithmatex">\(w_i\)</span>. A really simple statistic would be <span class="arithmatex">\(\sum_i w_i y_i\)</span>, the sum of the weights of the "successful" trials. It would be nice to have this be zero-centered: <span class="arithmatex">\(<span class="arithmatex">\(\sum_i \left\{ w_i y_i - \mathbb{E}\left[ w_i y_i \right] \right\} = \sum_i w_i (y_i - \overline{p}),\)</span>\)</span> where <span class="arithmatex">\(\overline{p} = (1/N)\sum_i y_i\)</span>.</p>
<p>It would also be nice for this to have variance 1, so we can divide by the square root of <span class="arithmatex">\(<span class="arithmatex">\(\mathrm{Var}\left[ \sum_i w_i (y_i - \overline{p}) \right] = \overline{p} (1-\overline{p}) \sum_i w_i^2\)</span>\)</span> to produce the statistic <span class="arithmatex">\(<span class="arithmatex">\(T = \frac{\sum_i w_i (y_i - \overline{p})}{\sqrt{\overline{p} (1-\overline{p}) \sum_i w_i^2}}.\)</span>\)</span></p>
<p>You could also conceive of this being a table with two rows and some number of columns. We bin the trials by their weights: all trials with the same weight are in the same column. Successes go in the top row; failures in the bottom. Now write <span class="arithmatex">\(t_c\)</span> as the weight of the trials in the <span class="arithmatex">\(c\)</span>-th column, <span class="arithmatex">\(n_{1c}\)</span> is the number of successful trials with weight <span class="arithmatex">\(t_c\)</span> (i.e., in column <span class="arithmatex">\(c\)</span>), and <span class="arithmatex">\(n_{2c}\)</span> is the number of failures. Then some math shows that you can rewrite <span class="arithmatex">\(T\)</span> as <span class="arithmatex">\(<span class="arithmatex">\(T = \frac{\sum_c w_c (n_{1c} n_{2\bullet} - n_{2c} n_{1\bullet})}{\sqrt{(n_{1\bullet} n_{2\bullet} / n_{\bullet\bullet}^2) \sum_c n_{\bullet c} w_c^2}}\)</span>\)</span> where <span class="arithmatex">\(n_{r\bullet}\)</span> are the row margins, <span class="arithmatex">\(n_{\bullet c}\)</span> are the column margins, and <span class="arithmatex">\(n_{\bullet\bullet}\)</span> is the total number of trials.</p>
<p><em>N.B.</em>: The wiki page gives a different answer, but I don't trust it, since the variance formula doesn't assume independence of the <span class="arithmatex">\(y_i\)</span>. A fact sheet about the PASS software that shows the formula in terms of the <span class="arithmatex">\(y_i\)</span> seems to make a mistake by using <span class="arithmatex">\(i\)</span> as an index both for individual trials and for the weight categories.</p>
<p>The confusing thing here is how to pick the weights. This test is mostly used to look for linear trends: imagine that each <span class="arithmatex">\(y_i\)</span> is associated with some <span class="arithmatex">\(x_i\)</span>, so that the weights would be <span class="arithmatex">\(x_i\)</span> or <span class="arithmatex">\(x_i - \overline{x}\)</span>. Why you pick these exact weights has to do with the <em>sensitivity</em> of the test. There could, of course, be a nonlinear trend, like a U-shape, that would lead to a zero expectation for this statistic. The <span class="arithmatex">\(\chi^2\)</span> test can find that, but Cochrane-Armitage with these weights cannot.</p>
<p>To see why you use those weights for a linear test, imagine that <span class="arithmatex">\(p_i \propto x_i\)</span>, and zero-center the <span class="arithmatex">\(x_i\)</span> such that <span class="arithmatex">\(p_i = m x_i + \overline{p}\)</span>.</p>
<p>Then the question is what <span class="arithmatex">\(w_i\)</span> maximize <span class="arithmatex">\(\mathbb{E}[T]\)</span>? You can quickly see that this is equivalent to maximizing <span class="arithmatex">\(\sum_i w_i x_i / \sqrt{\sum_i w_i^2}\)</span>, and taking a derivative with respect to <span class="arithmatex">\(w_j\)</span> shows that, at the extremum, <span class="arithmatex">\(x_j \sum_i w_i^2 = w_j \sum_i w_i x_i\)</span>, which <span class="arithmatex">\(w_i = x_i\)</span> for all <span class="arithmatex">\(i\)</span> satisfies. So those weights are the best way to get a large statistic if you think that there actually is a linear test.</p>
<p>[^1]: I apologize that <span class="arithmatex">\(p\)</span> means the expected proportion of heads and <span class="arithmatex">\(p\)</span>-value means something totally different.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.sections"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="../javascript/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
    
  </body>
</html>